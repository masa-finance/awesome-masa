[
    {
        "speaker": "A",
        "text": "Hey, bankless nation. I'm very excited about the episode today. David is out, and this episode gets technical at times. So I have ETH researcher Justin Drake, whom I'm sure many of you know, he's co hosting with me today. Some context as we get into this episode. So we did a previous episode a few weeks ago called the Sci-Fi Roadmap to Ethereum. And in that episode, Justin Drake described the end game for Ethereum. And he said this. We snarkify everything. In today's episode, we explore exactly what that means and how we do it. How do we snarkify everything? Our guest today is Brian Redford. He is the co founder of what might be the world's first type one Zkevm. And if you don't know what that means, that's fine. Neither did I. As we're getting into this episode, and it turns out that building a type one ZK evm is an important part of delivering what Justin Drake called an enshrined rollup inside of Ethereum. More on that in just a minute. But before we get in, just wanna mention something quick from our friends and sponsors over at so why are we having this conversation and why now? The compute era scaled with Moore's law, but the blockchain era scales with something differently. It scales with cryptography, specifically, cryptographic breakthroughs like ZK snarks. And all of this ZK snark stuff, the snarkifying of the EVM, it's all happening a lot faster than any of us previously thought. It's happening so fast that a project called Risk Zero just came on our radar last week. And they've already produced a working version of of the world's first ZK snarkified type one EVM. What kinds of things could this unlock in the future? Why is this important? Well, what if we could convert an optimistic roll up to a ZK rollup? What if we could upgrade Ethereum's layer one from a single threaded EVM model to a multi threaded evms so that compute was virtually limitless and free? What if ETH validators themselves had the ability to run from something as small as a smartwatch? All of these are possible unlocks with this technology. This is crazy cool. Deep stuff down the crypto rabbit hole we go. And it gets technical at times, but it's absolutely worth holding on for the ride. This is crazy cool stuff, and we're going deep down the rabbit hole today. And this gets technical at times, but I think it's absolutely worth it to hold on for the ride because this is key to understanding how blockchains actually work and how they scale. And in so understanding, I think this type of thing can help you avoid bad investments and dead ends, and there are a lot of those out in the space as well. We're going to get right to the episode with Brian and justin, but before we do, I want to thank the sponsors that made this possible, including our number one recommended crypto exchange for 2023 Kraken. Go check them out. Bankless nation. I am super excited to introduce you to Brian Redford. He's the co founder of a ZkevM project that we're going to find out a bit more about on today's episode called risk Zero. Brian, welcome to bankless."
    },
    {
        "speaker": "B",
        "text": "Thanks. Glad to be here."
    },
    {
        "speaker": "A",
        "text": "Also excited to be joined yet again on Bankless by Justin Drake. He's an Ethereum researcher and repeat bankless guest. Justin, how are you doing to."
    },
    {
        "speaker": "C",
        "text": "I'm doing great. Thanks for having me again. But I guess this time maybe as a host asking some technical questions. Yeah."
    },
    {
        "speaker": "A",
        "text": "How's it feel? The tables have turned. So, Justin, I'm going to tap you in as my co host today. So David is out and we're going to talk about the next generation zkevms. So I think we're talking to Brian about the world's first maybe ZK evM. That's a type one ZK evM. And I'm not even sure the words that I'm saying or what they mean. So we'll absolutely need to define that. David's out right now, so Justin, you're going to tap in and help me with this. I feel like this is a continuation, though, of a conversation that you had with him on etHcc. And I think bankless listeners may have listened to an episode entitled Ethereum's Sci-Fi Roadmap, or the Sci-Fi roadmap to Ethereum, in which there was this really interesting part. And I love that episode, by the way, where you were describing the ability of us in the future of Ethereum to snarkify the EVM on Mainnet on the base layer. And that sounded really interesting to me. And I think that ties into the conversation that we're about to have. So tee this up for us, if you will, Justin, as a continuation on that conversation, I know this, we're talking about Sci-Fi Ethereum futures stuff, but what does it mean to snarkify the EVM and how does that tie into the conversation? We're about to have with Brian today."
    },
    {
        "speaker": "C",
        "text": "Right? So, big picture, we're actually going to snockify all of Ethereum. And there's two big components that need to be snarkified. One is the EVM, which is this virtual machine which processes Ethereum transactions. And then the other part is the beacon chain. Now, once we've snugified these two things, we'll be in a position where compute won't ever be a bottleneck for Ethereum. So it means that, for example, as a validator, you won't have to really have beefy cpu's, so you'll be able to be a validator on your smartwatch. If you're building bridges between l one's, you'll be able to have another l one verify the state of Ethereum without having to redo all the computations themselves. It also has implications for, like, clients, for what we call enshrined rollups, which are super high security roll ups. And when the words type one evm come to mind, type one zkvm. It really kind of gets me excited as a researcher because it was kind of a piece of Sci-Fi engineering that was thought to be five to ten years into the future. But it looks like, well, there's several teams working on them. There's, for example, Tyco, and there's risk zero. Now it looks like the engineering will just be ready so much sooner. So we're starting to see the light at the end of the tunnel. And that has some big implications in terms of applications for ethereum, both at layer two, but also what I'm excited about, which is the layer one, what."
    },
    {
        "speaker": "A",
        "text": "You just described here, Justin, is the Holy Grail. And I still want to spend some more time with you right here, just fleshing this out and making sure that we understand this going to the episode, because it sets the context for the rest of the conversation with, with Brian and risk zero here. So the light at the end of the tunnel, or what I just referred to as, as the holy Grail, snarkifying all of Ethereum. What this means is we used get to use the spooky math, you know, the crypto magic ZK math that you've, you described so eloquently, many times on the bankless episode. And what we get, the prize that's held out to us, is the ability, first of all, you said to validate, be a validator on something as small as, like, a yemenite smartphone or an Apple Watch. So. Okay, is that really what we're talking about? So of course, one of the end goals and the end goal, the entire purpose of Ethereum is for it to maintain, for it to remain decentralized. And that means, ideally, anybody with a basic consumer level computer can validate transactions on the Ethereum mainnet. And right now the requirements for doing that are somewhat higher than just a smartphone or a smartwatch. But this will decrease the requirements, the hardware profile, in order to validate transactions on the Ethereum mainnet and also to stake. Is that what you're saying?"
    },
    {
        "speaker": "C",
        "text": "That's correct. So anytime you have a human that ends up wanting to interact with Ethereum, they need to interface through a full node. And there's some complications here, because running a full node is not something that you can necessarily easily do on your phone, and it's not something that the average individual will want to do, and it's something that every single validator has to do if they want to become a validator. So there's this barrier to entry, and more often than not today, for the vast majority of users, we end up patching this technical barrier to entry with some level of centralization. So, for example, if you're using metamask, you're going to be connecting to infuria nodes that are running the full nodes on your behalf. There's some amount of trust that you as a user are placing into metamask. So once we have a type one ZkvM, once we snockify the VM and all of Ethereum, we're going to be in a position where I, the user, will be able to interact with Ethereum with much, much, much less hardware like a phone or a smartwatch will be able to get best in class access with best in class security, best in class latency, all with very, very little hardware."
    },
    {
        "speaker": "A",
        "text": "Does this imply anything for a bandwidth as well? Will this decrease the bandwidth requirements, or will bandwidth become the constraint here now?"
    },
    {
        "speaker": "C",
        "text": "Right, great question. So, consensus kind of solves two problems. One of them is execution, and the other one is data availability. Snarks is kind of this magic technology that removes computation as a bottleneck within the context of consensus. And it turns out we have another magic technology for data availability called data availability sampling. Now, neither of these are really in production right now, but once we have both in production, you won't have to pay the cost of computation, and you will have to pay a very, very minimal cost from the bandwidth perspective. So you won't even need to download the Ethereum blocks. All you have to do is make these small queries for chunks of blocks and that's going to be enough to guarantee that you're on the canonical Ethereum chain."
    },
    {
        "speaker": "A",
        "text": "So data availability sampling as well, that's a core upgrade of what I think people are calling Dank Sharding as well, not proto dank sharding as I recall, but dank sharding, which could occur later in the future."
    },
    {
        "speaker": "C",
        "text": "Exactly right. So we're looking at technologies two, three, maybe four, five years into the future, which in a way will transform accessibility of Ethereum for users. And in the end game, accessing Ethereum will be just as easy as accessing any other website. And you'll have guarantees, just like on the website today, you have this cap lock, sorry, this lock in HTTPs, you'll have a similar lock saying you're really connected to the real Ethereum, and you will have to have done almost no work and you will have to download almost no data."
    },
    {
        "speaker": "B",
        "text": "Trust nobody. Also, unlike the current lock, where you actually have to trust the signature verifiers."
    },
    {
        "speaker": "A",
        "text": "For those signature issues, ZK technology is the thing that makes this all possible. What's very interesting is I know there are people who talk about compute scaling in blockchains via Moore's law, and that's true. But where we really get the massive scalability is more with cryptography breakthroughs. That is something I've learned as part of exploring the roadmaps and being in this industry for many years now, is these cryptographic breakthroughs are the key sort of step function breakthroughs that allow us to actually scale this technology. One other quick question for you, Justin, before we get to Brian to kind of describe what he's actually working on, what we're doing here, what does this imply for maybe scalability? On Ethereum mainnet, we talked about lowering the compute requirements to be a validator, which is fantastic. That is a further decentralization unlock. How about transactions per second on the Ethereum layer one main net? Does this have any impact on that as well?"
    },
    {
        "speaker": "C",
        "text": "Fantastic question. So once we've snarkified the evm, we'll be in a position where we can greatly increase the gas limit and potentially even remove the gas limit for computation specifically. And the reason is that the gas limit is an anti denial of service vector, whereby when you receive a block, you want to be able to fully verify that the block is valid on the order of 1 second. And so if there's too many transactions in that block, then it's going to take more than 1 second to validate. But the magical thing about snarks is that the verification time of a snark is on the order of one millisecond. And so you can take a block that's arbitrarily large, with arbitrarily many transactions and arbitrary much transaction execution, and know that the execution is correct within a constant amount of time, which is only one millisecond."
    },
    {
        "speaker": "A",
        "text": "So what does that imply then? So if Ethereum right now supports like 16 transactions per second, and we're scaling that out via rollups, and last time I checked on l two beat, we're about, if you count all of the kind of roll ups combined, in that we're about five x 16 transactions per second, something like that. And that's the entirety of Ethereum right now. We're not handling a visa level throughput at this point in time, it's safe to say. But what you just said about your finality or confirmation times the millisecond level validation verification here, what does that imply for mainnet throughput? Justin?"
    },
    {
        "speaker": "C",
        "text": "Right. So what it means is that there's going to be a partial comeback of the layer one. So I think the spotlight is going to shift to l two s for the next few years, and there's going to be a lot of experimentation, a lot of innovation, and the layer one is really going to be lagging because we're extremely conservative and we're frankly kind of slow for good reasons. But once we are able to catch up from a technological standpoint with the l two s, well, the l one s will also have some of the similar powers that the l two s provide. And so the l one will, to an extent, be able to scale out one of the superpowers, will mean that we can increase the gas limit. And there's something that's kind of explicitly put in Vitalik's roadmap diagram. But another thing that we can do, and I think this is something that Vitalik will add in, maybe the next iteration of the diagram, is that we can have an opcode within the EVM, which allows you to verify validity, proofs, snarks of EVM blocks itself. So the EVM is aware of itself and knows when another EVM block is valid. And what this allows us to do is have multiple instances of the EVM, because ultimately you can think of the EVM as being this single threaded cpu or virtual machine, so it can only run on one core, just to simplify. And so there's this inherently sequential computation that's going on, which is a bottleneck for scalability. The EVM will never be able, most likely to do a million transactions per second, just because we have this inherent bottleneck. And so the way that we scale out once we've reached all the gains by increasing the gas limit, is by having multiple instances of the EVM. So this is going to be EvM zero, and then Evm one, Evm two. And the cool thing is that once we have this opcode, anyone can programmatically create a new instance of the EVM."
    },
    {
        "speaker": "A",
        "text": "Wow. So this would become kind of ethereum Mainnet, the layer one kind of our multicore moment. Then what's kind of the analog? Is it from 486 to Pentium? I don't know what we're doing here."
    },
    {
        "speaker": "C",
        "text": "Yeah, I think that's a good analogy. We're going multicore. So for a very, very long time we've had these cpu's which were just one core. And then what we did is we ramped up the frequency of the cpu's, so it was like hundreds of megahertz and then 1, then 1.52. And then nowadays we have 3 GHz cpu's and you can't do like 30. Reason is that the transistors just don't turn on enough fast enough. So there's this sequential bottleneck. And so the way that you scale is by scaling out vertically, sorry, horizontally, by having multiple cores working in parallel. And this is exactly what we can do once we have this opcode. And the term that I like to use is enshrined rollup. So once we've snarkified the canonical instance of the EVM that we have today, we're going to have one enshrined roll up. Then once we add this opcode, we'll allow anyone to create as many enshrined world apps as they want."
    },
    {
        "speaker": "A",
        "text": "Okay, well, Brian, you've been sitting here waiting very patiently, as Justin eloquently described the light at the end of the tunnel, this kind of holy grail. And I think that you are working on that now. You're not working on that within the bounds of the Ethereum foundation and applying that to Ethereum Mainnet, but you've got this project called Risk Zero, that is actually pursuing the technology that is required to get us to the promised land and everything that Justin just described. So Brian, I'm wondering if you could tell us maybe first I would love to get your reaction to what Justin just said and anything that that maybe triggers in your mind. And then we'll talk about what you're building out at risk zero and this platform that you're calling Zeth right now but first of all, any reaction to what Justin just said?"
    },
    {
        "speaker": "B",
        "text": "Yeah, I mean, it's pretty clear that with all of the advances that we're making in cryptography, as you said, that the capabilities of Ethereum are just going to continue to compound and compound and probably much faster than we would have seen or thought even possible. So I think the future is very bright, given all of the advances that are being made across the entire space. And this is just one key in a very large puzzle. So it's very exciting."
    },
    {
        "speaker": "A",
        "text": "Okay, well, so tell us what you're building then, out on the frontier, which again, looks a little Sci-Fi from our perspective, but seems to be at the same time also happening faster than many would have imagined, at least many years ago. So your company is called risk zero, and you've got, I believe, this platform called Zeth. And we describe this as a type one zke evm. And this goes beyond what I even know I'm describing. So, like, what is Zeph and what is a type one zkevM?"
    },
    {
        "speaker": "B",
        "text": "Yeah, so Zeth is an EVM implementation that's actually based on RET, which is like Geth, an implementation of the EVM. However, it's one that's built using rust and a type one Za EVM is simply one that can actually process the full nature of Ethereum and an entire block and prove effectively snarkify, an actual Ethereum block, as opposed to some of the other l two s that you have out there that have made various compromises in order to create a more provable EVM. Type one EVM makes no compromises and sticks to the original Ethereum specification, but still produces this snark that succinctly verifies that the EVM computation was run correctly."
    },
    {
        "speaker": "A",
        "text": "So the other Zk evms that we've talked about many times before, the ones that polygon are building, the ones that matter lab is building, the one that scroll is building. You'd differentiate that and you'd say, that's not a type one EVM because it's a little bit different in some way. Is that correct? And how is it different exactly?"
    },
    {
        "speaker": "B",
        "text": "They're all different in different ways. And they often change exactly how things get merkelized, how states stored, and they generally tend to implement all the opcodes. I don't know, Justin, do you want to expound upon that?"
    },
    {
        "speaker": "C",
        "text": "Yeah. So generally speaking, what will happen is that they will want a solidity developer and solidity code to be reused. That's kind of one of the main goals. And so they're going to re implement every single opcode, but be under the hood, under the bonnet. They're going to be taking some shortcuts to optimize things. And one of the prime shortcuts is to change the way that the storage is merkelized. Today we have what's called a Patricia Merkel tree, which is using this catch hack hash function. There's all sorts of technical terms just to say that the way that we handle storage is very much non snock friendly. What these teams have done is they've just taken a completely different approach to authenticate and merkelize the state. Another possible difference is changing the gas schedule, because DVM gas was designed from the perspective of CPU's, if a certain operation takes 100 nanoseconds and another one takes a ten nanoseconds, then the first one should be roughly ten times more expensive from a gas perspective. But the gas schedules don't translate very well to Snarkland, so you could have a very very cheap instruction on the CPU, for example, doing a hash like Ketchak that's extremely fast on the CPU. But if you were to do it in Snockland is extremely expensive. In order to protect themselves from these denial of service attacks, where someone can craft a block with lots and lots of catch hack and basically mount a denial of service attack on a specific roll up, they've adjusted the gas schedule, so it's not exactly compatible with the EVM that we have today on mainnet."
    },
    {
        "speaker": "A",
        "text": "At layer one, because of those changes, because of those, I guess, optimizations or differences, these types, type two ZK evms, are not candidates to become an enshrined roll up, at least in their existing form. Is that correct? And this a type one ZKEVM, is closer to a candidate to becoming an enshrined roll up. Am I making that connection correctly?"
    },
    {
        "speaker": "C",
        "text": "Yeah, that's exactly right. But what I'll say is that oftentimes like these are journeys, right? They start somewhere and then they incrementally become more and more compliant with the EVM. I mean, this even happened with optimistic roll ups, right? At first they had these small modifications relative to the EVM opcodes, and then they said, no, we want to be exactly equivalent with the EVM opcodes. And this journey is going to happen for the ZKE EVM roll up implementations, I believe. And part of the reason is that you get to benefit from a lot of tooling, from a lot of standards, from a lot of network effects. But the trade off here is that it's much, much harder from a technical standpoint. What's happening, and it's magical to see in front of our eyes, is that the technology is improving at an extremely fast pace. We have an equivalent of Moore's law for snark improvements. And I don't know what it is, but it's something like, I don't know, improving by four x every single year. So give it a few more years."
    },
    {
        "speaker": "A",
        "text": "Can we make a Drake saw here, please? I want a law."
    },
    {
        "speaker": "B",
        "text": "It's a Moore's law with another Moore's law on top of it."
    },
    {
        "speaker": "C",
        "text": "Yeah."
    },
    {
        "speaker": "A",
        "text": "Okay. Okay. So this is about the limit of my technical proficiency here, and I'm wondering, Justin, if you could sort of. What questions do you have for Brian here actually, about how this works? So we're talking about this type one ZK evM. We've, we've fleshed out the rough contours of what this actually is, but I think this has an impact on a lot of things. On, on bridges. We talked about enshrined roll ups, multi provers, you know, there's the performance conversation, security, licensing type of conversation. Why don't you take some of the technical details here and maybe, you know, I'll come back and ask the dumb questions as they arise in my mind."
    },
    {
        "speaker": "C",
        "text": "Perfect. Sounds great. But I guess I do have one non technical question, which is a little bit about context setting, which is that it seems like you guys were in stealth mode for a relatively long amount of time, maybe a year or two. And a few days ago when you made the announcement, Vitalik was messaging me and it's like, who are these risk zero people? Are they doing good work? Can they be trusted? Et cetera, et cetera. And if Vitalik is not aware of you guys, maybe the listener is also not aware of you guys. I guess my question is, what prompted you in the first place to build a ZKVM? Normally when you build ZKVM, is this because you're aiming towards a roll up? But my understanding is that you're not aiming towards a roll up. So what is the background here?"
    },
    {
        "speaker": "B",
        "text": "Yeah, I mean, so Risc Zero got started with this idea of building out general purpose ZK capabilities. So the ability to actually prove any computation. So not just the EVM, like an existing game. You could prove doom, you could prove Linux. People are actually using this to prove the execution of Linux. So ECC, not this past one, but two years ago or two eccs ago, I was talking about the fact that in my mind, the best way to build a CkeVM was to actually just take the code that people have already written and then run it in this sort of general purpose context, because you don't actually need to then engineer all these hundreds or circuits. So basically reduces the amount of capital required to actually run an EVM and produces a world where the proofs that you're creating are very much in line with the clients that created them. So, yeah, I would say we've been thinking about doing this for a long time. It's just getting the sort of technical requirement, getting the technical capabilities over the line to the point where we could do this. We really just got there like two months ago. So then as soon as that happened, we're like, okay, now we have to actually try to make the EVM real, the type one EVM. And it turns out, like, it was fairly straightforward once we got extra year of engineering done to get the continuations and long running proof feature to work."
    },
    {
        "speaker": "A",
        "text": "And Brian, this is maybe the flash of lightning that Justin is referring to, because Raciro just came across my radar last week as well. And David was like, hey, I'm going to be out. He's that burning man, actually. So I'm going to be out next week. Ryan, you should go talk to these people and see what's going on. And this is the tweet introducing Zeth, a fully open source type zero ZK EVM. Built on the Risc Zero ZK EVM and Banzai Zeath is a performant, upgradeable, and scalable way for developers to ZK prove any Ethereum block, ushering in the next generation of ZK and EVM. So pretty big tweet thread to splash in the world. And yeah, that's part of the context for this conversation. All plus one. Justin is like, we want to find out what you guys are doing, what's going on here."
    },
    {
        "speaker": "B",
        "text": "Yeah, so the type zero thing is definitely a joke that might have brought some people the wrong way, but the idea, like, it's a type one evm, but you didn't have to do zero work to make it work because we just utilized all the hard work of everyone else in the space to create this platform. It's not entirely true. We had to change some of the ways. Like, the Merkle petition sheet works to make sure it's more snark friendly or stark friendly. And then we also had to modify a bit of the database back end. So definitely required some brilliant work by some amazing engineers, but like a month and three people to get this over the line. Now there's tons of room to increase the performance of the system and all kinds of things like that. But we really have gotten to this kind of base level of now we can actually ZK proof Ethereum exactly as is."
    },
    {
        "speaker": "C",
        "text": "So if I were to summarize, it sounds like you guys started off very much as a technology company, focused squarely on snarks and snarkifying the world. And you have this really interesting approach, which I guess we should dig into. Very interesting technical approach. But in our pre call yesterday, one of the things you mentioned was that there was some sort of partnership, maybe with another roll up projects, maybe the optimism project. And that little piece of nugget was kind of interesting to me because it what created the bridge between the technology company and more so like the crypto or the Ethereum company?"
    },
    {
        "speaker": "B",
        "text": "Yeah, I mean, we've been talking to op on and off for a while, because this idea that you can take ZK and sort of create fraud proofs, as soon as you can ZK prove something, you can also ZK prove that something is different than what other people said it is. So you have this kind of ability to automatically create a fraud proof if you, if you have a ZK provable system. So we've been chatting with optimism for a while. They eventually decided they were going to send out this like RP or mission for people to actually zkify. The op stack and us and Mina and a couple other teams all applied, and our solution was very much based on the preliminary work we'd done on Zeph. We realized rather than taking this very complex fraud proving system, which is an amazing work of engineering, but sidestepped it and said, let's just take the op ref in development libraries and just create a way to ck prove those, which effectively provides a fraud proof mechanism. Because now you can prove, well, once it's done, which will be in nearish future, you'll be able to prove that an optimism block is or is not a different from what the chain actually agreed on. So we're going to actually see, I think, in the near future, through this partnership with optimism, the ability to get liquidity much faster than seven days, or move your assets around and access your money. So, yeah, and this is gets to sort of what you were saying about the magic of starks and being able to, in the distant future, have Ethereum itself be fractally hyperscale, however you want to say that."
    },
    {
        "speaker": "C",
        "text": "Right? So I think what is partially going on is that optimism as a project, which is an optimistic roll up, is thinking down the line of upgrading to a ZK rollup. And they submitted this request for proposal where they're saying, okay, anyone in the world, if you can help us snarkify optimism, we're going to give you money. Now I had a look at the, the grant kind of the request for proposal and it was 250,000 op tokens, which at current price is about $375,000. Now the reason I mentioned this is because if you had asked us two years ago to come up with a prototype, a proof of concept for type one ZKVM with open source code running in the. On the cluster of GPU's, that would have cost millions, if not tens of millions of dollars. And for now, for half of a million dollars, we have this new team coming forward and providing technology. I think one of the key tools used here is abstraction. There's this massive shortcut that was taken. There's various pieces of the puzzle. And the way that I think about it is that there's three steps. You start with an existing FM client, in this case ref, and then there's this new intermediate step, which is risk five, which I guess it would be good for you to describe what exactly is risk five. Then there's this final step towards getting a snark. And it seems like actually that there isn't much work going from the client to risk five, or almost no work. And then same thing for going from risk five to the snock. So can you describe these three steps and the work involved in getting someone."
    },
    {
        "speaker": "A",
        "text": "And really quick risk five? What is risk five? We know what risk Zero is a company, right?"
    },
    {
        "speaker": "B",
        "text": "So risk zero's name comes from risk five. And risk five is. It's an open source instruction set architecture for actual microprocessors. So people familiar with like the Apple M two or ARM or X 86 or mips, you know, these are actually instruction sets. So similar to the EVM, they have opcodes that tend to be, you know, they're opcodes that can actually be reified in hardware in a reasonable manner. So X 86 has hundreds, thousands of them. So it's a complex instruction set architecture. But then you've seen the shift towards arm and mips is very old, but RISC V is kind of the spiritual successor to mips in a way. So it's a very small set. Well, has a couple different dialects, but you can boil it down to about, I think, only 40 something opcodes at its core. So what we've done is created a Zkvm. So it's not an EVM same idea, except for what it does is process these much lower level instructions. So there's not really precompiles or anything like that. There are escape patches that you can use, but so there's this core very minimal set of computing instructions that RISC five sort of publishes. And people can actually take Sci-Fi the company that invented it. You can just get a spec from them and you can put RISC five cores into whatever project you're doing. So almost every computer that's shipping now does have some number of RISC V cores somewhere in the sort of bigger chip. So most chips, any. Anything people think of as a cpu or anything like that, is really a system on a chip that probably has 20 different cores in it. So anyway, by doing all of this, we've taken the ability to ZK prove something and said, we're going to be able to ZK prove anything you could run on a normal processor. So the reason that sort of going. So going from RiSC V to ZK proving a risk five was something we surprised the ZK world with at least about a year and a half ago when we released this. I think the time Elie had said that Ellie Song from Starkware thought that general purpose ZKVM was at least five years out. So this is another example of ZK kind of just happening much faster than people would otherwise expect. So by focusing on this minimal set of instructions, we were able to create a very performant ZKVM. And most of the work to translate from anything else into RISC V has already been done by the RISC V development community and by the LLVM compiler community. So we're really just leveraging the network effects of open source software to take, as Justin said, a massive shortcut to get to ZK proof of Ethereum."
    },
    {
        "speaker": "C",
        "text": "So, basically, there's two translation steps, or kind of compilation steps that need to happen. And it turns out that the rust programming language, by default today already allows you to compile to various cpu's. So when you have a Rust program, you can compile it to X 86, which is a lot of intel machines run on this. You could compile it to Arm, which is a lot of the new Macs and a lot of phones run on Arm, but you can also compile it to this more arcane but still popular enough to be supported instruction set, which is called RISC five. So there's basically all the work has been done to go from Rust, which is, for example, ref, which is written in Rust, to RISC five, and then there's this one time step that needs to be done to go from RISC five to a snock. And this is the heavy lifting that Brian and his team have done. But it's a one time thing. And so now we can kind of take any rust program that we want and kind of reap the benefits of abstraction. So now what? The normal, the current, I guess the traditional paradigm for snarkifying things is to work very, very close to the metal, very, very low level. Right? You have a program that you want to snarkify and you're going to kind of jump through lots of hoops and kind of work with these polynomials and very low level programming languages, partly because there isn't much tooling, but also partly because you need the performance of these really low level optimizations. But one of the things that's happening is that we're getting more and more powerful abstractions, which means that as a developer, you can work with higher and higher level programming languages. And rust is an extremely high level and friendly programming language within the blockchain space specifically, it's extremely popular. And in combination we're finding all sorts of optimizations, both at the software, the hardware level, to make this palatable, maybe we should move to performance, which is that back then we could have said, yes, you can go ahead and do it, but it will take days to produce a proof for an Ethereum block. What kind of performance do you guys have and how did you get there?"
    },
    {
        "speaker": "B",
        "text": "Yeah, so the performance varies based on which kind of hardware, actual hardware target you're running the ZKVM on. So we support CPU's, we support the M two GPU, and we support Nvidia GPU's as well. So getting to this level of performance has been a multistage journey. And honestly, there's a lot of room for us to get even more performance out of it. But one of the early choices, the reason we use this particular subset of RIS 532 bit instructions, is because it lets us operate in the sort of smaller prime field, which is much more amenable to being accelerated on GPU's. And specifically, I think, interesting to the sort of Ethereum space and blockchain in general, this smaller field means you don't need these massive crazy ZK proving rigs anymore. You can actually do ZK proofs using a 16 gigabyte desktop GPU. So that actually we built a proof system that could run in these really small kind of consumer grade cards. However, then that had some other kind of downsides. Like, you couldn't run giant programs. So we built this system called continuations, which uses just like, folding, you've mentioned a bunch. So it's a way to take proof and split it up into a bunch of small proofs and then let a bunch of different parties effectively prove bits of them and roll them back up into, into one single proof. So getting to this level of performance, we had to optimize the recursion circuits because, you know, taking 1024 proofs and rolling it down into one proof takes ten vertical steps of recursion. And then beyond that, we have this ability to actually run the proving computation itself. Yeah, as I mentioned, on the GPU rather than a cpu, and we see pretty significant gains for that. But there's, we're just really kind of scratching the surface because we haven't. We focused on enabling everything, which is kind of our core thesis. Let's do the general purpose thing, let people actually prove something, and then we can focus on the performance where it matters. So, yeah, it's been a combination on that performance."
    },
    {
        "speaker": "A",
        "text": "So I'm just in kind of like the intro as we're exploring this throughout the possibility that someday we could run an Ethereum, a snarkified Ethereum validator from our smartwatch. It sounds like there are a lot of performance steps necessary to get there. And I'm curious, like, how close are we? So is what you're saying, Brian, that right now we could run what you've developed, which is this ZK type one evm on a home consumer laptop. It's a pretty beefy laptop. And then how many steps away are we from getting that to, like, a smartwatch?"
    },
    {
        "speaker": "B",
        "text": "Well, so proving, I don't think you're going to. When you end up in this sort of enshrined roll up world, you're not going to have the proving be done on the smartwatch. The proving will be done by these machines off in the cloud or in this decentralized network. And then the verification, like, because your snark or stark, like, they're really small, the snark especially. And, you know, it takes literally fractions of a millisecond, so the computing power to verify the snark is there. And then once you have data availability sampling, you really just don't need that much information to actually participate fully in the network. So you can have a very light line."
    },
    {
        "speaker": "A",
        "text": "So then why does performance matter so much?"
    },
    {
        "speaker": "B",
        "text": "Oh, performance of the proving system. Yeah, I mean, effectively, it gets down to how quickly you can make these sort of EVM blocks. Right. Like right now it takes us, if we use 64 of these off the shelf machines, it takes about 50 minutes. And I think there's probably an easy ten x there. But if you use even more machines, we can get it down to twelve minutes. But realistically, if you want this sort of enshrined roll up, you need to. What did you say, Justin?"
    },
    {
        "speaker": "C",
        "text": "2 seconds. If the use case specifically is being a validator, then when a new block comes in, you want to know that it's valid a second later. Basically, the latency, the proof latency, the time it takes to generate the proof should be on the order of 1 second. And today we're not there yet. We're maybe 100 to 1000 x. So let's say 2.5 orders of magnitude away from getting there. And so, performance matters for two key reasons. One is this proof latency that we, for some use cases, we don't really need the low latency. But for other use cases, we do need the low latency. The other reason is just diminishing the size of the prover. So nowadays, if you want to be a prover, more likely than not, you're going to rent out a rig of GPU's on AWS. Brian was talking about 64 GPU's, I believe, on AWs. That's not super friendly to a decentralized proving network of people at home. What we partially want to do as well is shrink all these 64 GPU's into a small box. And the way to get there, in my opinion. I'd be curious what your opinion is, Brian, is to have snark acceleration. So we went from CPU's to GPU's, and then the end game is to go from GPU's to Asics."
    },
    {
        "speaker": "B",
        "text": "Yeah, I mean, I still think even when you get to the ASIC level, you're still going to end up like. And you have a huge decentralized network of provers. Maybe these Asics are even in people's phones, you're still going to end up splitting, proving an entire ETH block, probably up over hundreds or thousands of nodes. So I think parallelism is critically important no matter what. But the ability, now that we've sort of shrunk the requirements of the prover down to where it is, I think the ability of hardware acceleration to really make a difference is actually there. I was pretty bearish on hardware for the first year of the company's existence, because I didn't think. I didn't think that people were going to be able to do better than Nvidia. It's really hard to do better than Nvidia with their GPU performance. But the stuff I've seen coming out of several hardware teams recently is really, I think there's going to be the ability to get to gigahertz level GPU. Sorry, gigahertz level ZK proving through asics in the five year timeframe, let's say three to five years."
    },
    {
        "speaker": "C",
        "text": "Okay, so if I were to try and summarize, where does this performance come from? Which is just to recap, it's like a ten minute, roughly speaking, ten minutes to 1 hour. Proof latency comes from three different types of tricks. One is on the proof system itself, where you move to this different type of so called finite field, which is 32 bits, as opposed to something larger. You've leveraged the GPU's, which, as you said, Nvidia does a great job with their GPU's, which are used for AI, but can also be used for snarks, which are also extremely compute intensive. And then there's this final really beautiful trick, which is basically recursion, where you take a big chunk of computation, you cut it up into much smaller chunks, and then you do the proving for each small chunk in parallel, and then you reassemble all the pieces of the puzzle and all of that can be parallelized and distributed."
    },
    {
        "speaker": "B",
        "text": "Very well said."
    },
    {
        "speaker": "C",
        "text": "Now, I guess the next big topic in my mind as an Ethereum researcher and thinking of type one zka vms, is security. We have traditionally a lot of complexity going on here, and the likelihood for bugs is very, very high. And I have this saying, which is maybe, I guess a little arrogant, but I believe that every single ZKVM has multiple critical vulnerabilities today. And so we need to be prepared as a community to either have, like, mitigations to these bugs, and there's a lot of very good ideas. And we also, unfortunately, need to be prepared to roll ups getting hacked. So just like we've had a bunch of fairly large roll up hacks, sorry, bridge hacks, on the order of hundreds of millions of dollars, maybe close to a billion dollars, we could have multi billion dollar hacks in the ZK roll up space. I'm curious, how do you think of security, and how do you think of removing every single bug from the system?"
    },
    {
        "speaker": "B",
        "text": "Yeah, obviously a huge topic. Me and my co founders actually know each other. We met 23 years ago in the Seattle, like, infosec scene. So we were all like hackers back in the day. So we have a pretty deep set of sort of experience and knowledge in this space. And that's part of the reason we chose RISC five. Also, it actually has like, there's a full formal specification for it. You actually can prove that certain systems formally prove that certain systems implement RISC V. We haven't gotten to that level of sort of formal verification with what we're doing yet, but you can imagine getting to the place where you have very strong guarantees that the ZK system itself is proving risk five and only risk five, and that, and that the sort of conjectured amount of security, the number of bits of security, is actually, you know, is actually what we think it is. So there's a lot of work on the mathematical side to sort of prove that the crypto system itself is like the proving system is actually doing what it's supposed to separately. You then need to audit, as you mentioned, the actual ZK circuits. And I think that's an area where this approach really shines, is that because the RISC V instruction set itself is small, it means there's much less surface area to audit. Although we do have these sort of acceleration circuits that one can add on to the system, it still doesn't increase the sort of audit surface of the ZK part to. To the same degree that doing ZkVM from scratch would. Now, you're also, I think, as you pointed out in the pre call, by doing this, you are potentially onboarding a few more security considerations. For instance, you're trusting the rust compiler and you're trusting LLVM. Now, these things, and there are often bugs in LLVM. I think we just found one the other day. So compilers, especially for new architectures, aren't perfect. But this is one of the reasons, again, why we chose an existing architecture. I don't think arm or something older or more mature would have really fit in a ZK circuit. But by choosing an existing architecture, we get to leverage all of the billions of dollars of investment that's gone into the security of this existing ecosystem."
    },
    {
        "speaker": "C",
        "text": "Gotcha. If I were to summarize this kind of. This final step, where we go from RISC V to a snark, which is actually fairly digestible, because RISC five is relatively simple. Actually, this reminds me of Cairo from stockware. They have an even more reduced instruction set, which is super simple. What they've been able to do is apply some formal verification tools to prove that things are working properly there. The hope is that this one time investment, we can really drill down with powerful tools for more verification and prove that it's correct. And then we kind of have the rest of the can of worms, which is kind of this fairly complex compiler to go from rust to RISC five. And it's possible that there's bugs generally speaking in the rust compiler, but it's also possible that there's bugs specific to compiling to Riscfire, because RISC five is one of the more niche instruction sets that you can compile to. But what might happen is that we're going to start building these roll ups which are securing billions and billions of dollars, and then Lindy starts to kick in and we might have bug bounty programs. And it's interesting where in a way the blockchain space might make a huge contribution to compiler security. There'll be way more eyes. We recently had this bug in the Viper compiler and that caused a bunch of bugs. It would be great if we could apply similar tools like formal verification to compilers like Rust compilers, which today sounds very grandiose, but maybe the blockchain use case is so security critical that we're going to try and move forward partially in that direction."
    },
    {
        "speaker": "B",
        "text": "Yeah, I mean, people have done this for C, and if you're going to make. If you're going to do formal verification for C, then Rust is probably also within the bounds of what's possible. But these efforts take decades or a really long time. But blockchain accelerates everything to an obscene degree. ZK would still be like a niche academic pursuit, I think, if it weren't for blockchain needing it so badly. Yeah."
    },
    {
        "speaker": "C",
        "text": "Okay, great. So I guess the final kind of semi technical topic that I have is around licensing. So if we are going to be using a piece of code at layer one, really, for it to be palatable, socially palatable, I guess the licensing needs to be good. And I guess the favorite types of licenses that we have might be Apache 2.0 and the MIT license. Can you discuss what have you open sourced and under which licenses?"
    },
    {
        "speaker": "B",
        "text": "Yeah, so the sort of core Risc zero ZkvM. So the RISC V proving engine that's licensed under the Apache two license right now, always has been. And then Z itself will probably be Apache two slash MIT licensed. We might also end up dual licensing everything because that's kind of the standard in the rust community. And op like is a fan of mitzvah. So that's for the system that produces, takes the EVM program and chunks it up into a bunch of little proofs and then proves all of those. The parts we haven't yet open sourced or released is the part that actually takes all those proofs and recurses them down into a single proof. And then also this thing that converts the stark that we use into a snark. So there are two kind of aspects of this that we haven't launched and effectively we're waiting for these things to get through security audits because with the current system as is, you can't really, the proof is much too large to put on chain, so it's kind of hard to get wrecked because you can't actually use stuff on chain as readily. So right now, if people want to use the system, we have to get an API key from us, but that's definitely not the direction we're headed. We're very much committed to fully open sourcing the entire system, but we want to make sure we have a high confidence that people are not going to soon because of the ZK system."
    },
    {
        "speaker": "C",
        "text": "Okay, understood. So you've open sourced several key components on the very attractive license Apache 2.0 and you're thinking of dual licensing it maybe with MIT. So you can choose which license you want when you start using the code. And part of the prover is already open source, but maybe some of the final things involving the recursion and kind of the wrapping it into a tiny, tiny proof so that it can be consumed on chain that's not yet open source."
    },
    {
        "speaker": "B",
        "text": "Yeah, exactly."
    },
    {
        "speaker": "A",
        "text": "So Brian, risk zero seems to have come out of nowhere and it's super incredible how fast all this is coming to bear here. And open source working, it sounds like first to snarkify, our favorite l two s out there, like working with optimism and others. So I imagine that's going to be a bulk of the work at first. What are you guys planning to do here? What's the business model for risk zero? It almost sounds like what you're producing is a public good. And here Justin and I and the rest of the bankless community are kind of cheering you on. But I'm sure you have investors, I'm sure you have VC's here that have put some money in and they're going to expect some kind of return. Yet you're not building a layer two yourself as of yet, it sounds like. Or maybe you will tell us what risk Zero is put on earth to do. What are you planning to do in this space?"
    },
    {
        "speaker": "B",
        "text": "Yeah, in this space we're really focused on this Bonsai ZK application development platform. This is something we've been working on for a while. Um, because you can use ZK for all kinds of things. I don't know how if you've talked to like many of the ZK coprocessing teams, but you can use banzai effectively, the ZK coprocessor, which lets you run a bunch of complex logic off chain and then just attest to it on chain. So we like Denver, I talked about a, like a l one claw effectively running an order book on Ethereum directly at uniswap. You can achieve roughly uniswap level pricing by doing all of the order book matching off chain. So you have your orders, the orders get placed on chain, and then Bonsai sort of just reads those orders, does all the matching in ZK just on one machine, any random corner of the Internet or AWS, wherever you want it to be, and then says, okay, here's proof that these are the orders that got matched at this price."
    },
    {
        "speaker": "A",
        "text": "So this theory, Banzai is kind of a platform for ZK snarkifying apps, maybe."
    },
    {
        "speaker": "B",
        "text": "Let's call it apps and roll ups, anything really. So we expect right now it's a centralized SaaS offering, and we think there's long term value in sort of providing an enterprise sort of open core model there. But we will definitely be building a decentralized network around that as well. Exactly what that looks like, who knows? It's going to be very focused on the sort of core accounting for proving tasks. So kind of like a proof marketplace, but we expect that it will add interesting features for application developers over time because there's ability with continuations to do sort of ZK Docker. You can prove something up to a certain point. You can suspend the thing and then you can just keep going on later. So you can kind of imagine having a ready to go EVM image where people can just resume it and they have full access to the Ethereum state, things like that."
    },
    {
        "speaker": "A",
        "text": "Is this like sort of an AWS for ZK proofs kind of thing? It's just like a marketplace here, and you're looking to try to make it as decentralized as possible. That could be a future here, definitely."
    },
    {
        "speaker": "B",
        "text": "I mean, this is like when we got into, I mean, Jeremy or scientist is just always into AI and ZK and all of these things, but when we started really thinking about what we're going to do with it, I think that really got me excited. Washington was the potential of this technology to kind of let people who are building infrastructure and applications not need to rely on Facebook, Amazon, Microsoft, Google for everything. So this idea that we could actually fully decentralize the sort of infrastructure that goes into many of the applications we use has always been really appealing. Part of what this technology is capable of. So I think Bonsai is going to be a platform that helps people do that."
    },
    {
        "speaker": "A",
        "text": "So, Brian, what do you think happens next in the roadmap? All of this seems to be happening faster than we all thought it would, which is so incredibly exciting. The level of investment in the space and the level of talent in brains now being focused on crypto is just absolutely astounding. We almost ended the episode with one of the last parts, which I think is, of course, the public good. That is Ethereum mainnet kind of that will upgrade to a fully snarkified, enshrined ZK EVM probably last. We're going to want this fully proved out in all sorts of ways across crypto before we get to that stage. So I'm wondering, what do you think will happen in the interim over the next six months, over the next one to two years, how do you think the tech that you're building will start to impact the crypto landscape? Will we just see ZK snarkified layer two? Should we expect to see this technology applied mainly in roll ups? Are there apps that you see this being applied, or will it take a few years?"
    },
    {
        "speaker": "B",
        "text": "Yeah, all of the above. We're definitely working with l two s, l three s roll out frameworks. However you want to think about all of that space. And also we're working with people on DeFi projects, and eventually gaming is going to be a big part of this. The way I see this playing out is just like friend tech kind of surprised everybody with how much better the crypto onboarding experience has become. And sure, there's still a lot of room to go. I think the headway that people have been making and making crypto applications easier for people to use is going to then also increase demand for the capabilities of these systems to be. To be able to do more and more interesting things. So I think we'll see ZK playing a critical part in all of this by enabling people to do whatever computations they don't want off chain and readily attest to them on chain. So this co processing architecture is going to be a huge unlock for applications built for web three applications."
    },
    {
        "speaker": "A",
        "text": "Well, this has been great. Justin, do you have any other questions for Brian, or should we start to close this out here?"
    },
    {
        "speaker": "C",
        "text": "I think I have one final question, which is around your alignment with Ethereum. I think we had, during the pre call, it sounded like as an individual, as a person, you were in this space for quite a long time, and you have a certain set of beliefs. I'm curious what those are, but also how this translates into the culture of the company."
    },
    {
        "speaker": "B",
        "text": "Yeah, I mean, our sort of core, three core values are like integrity, transparency, and agency. And I think if those aren't, like, Ethereum aligned, I'm not really sure what that sort of even means. So coming out of the hacker culture that the founders came from, it just seems like a very natural sort of fit to the sort of ethos of Ethereum, I think, value wise. Yeah, we're really the vision of the sort of hyper structure world of the future is very much something we all very deeply resonates with all of us."
    },
    {
        "speaker": "A",
        "text": "Brian, what first brought you down the crypto rabbit hole?"
    },
    {
        "speaker": "B",
        "text": "You know, buying supplies for Burning man?"
    },
    {
        "speaker": "A",
        "text": "It's funny, as my co host is literally at Burning man right now. Fantastic."
    },
    {
        "speaker": "B",
        "text": "No, but it's been a crazy journey, and so it's really fun to get more and more into this space."
    },
    {
        "speaker": "A",
        "text": "Yeah, well, very good. And maybe my last question is kind of the high level to Justin. So this idea of hyperscaling Ethereum using kind of like fractal crypto proof, ZK proofs on top of ZK proofs, I mean, has this always been part of the plan, or is this just happenstance? And I guess when you think of the term hyperscaling, how do you envision looking? How do you envision Ethereum looking five years from now? Is all of this stuff just kind of working? And what's the total transactions per second? I don't know. What's the Sci-Fi Ethereum with this tech applied? How does that look, Justin?"
    },
    {
        "speaker": "C",
        "text": "Right. So, I mean, if you want to think in terms of endgames and fundamentals, you go back to these fundamental resources, computation, that's just not going to be a problem of consensus. And the way that I think about it is that consensus is this very flexible tool that can solve all sorts of problems. And then cryptography, what it does, it gives you a few superpowers that allows you to reduce the scope of consensus and basically have more and more crypto and less and less economics, if you will. And historically, actually, one of the big breakthroughs for consensus was simple message authentication and signatures. That kind of changed the model where you had these messages that could be intercepted and modified, but that didn't really matter because they were signed and authenticated. And so the model was, what can you do with consensus given signatures? And now we kind of have this new tool, which is much more powerful from a cryptographic standpoint, which is, what can you do with consensus with snocks? And it turns out that the things that needs to solve are data availability and finality. It turns out that data availability is something that we can solve with database sampling, as I mentioned. Then there's this other thing, finality, which I only discovered recently. You can also solve with cryptography, with these really, really sophisticated pieces of cryptography called one shot signatures, which actually marry quantum mechanics and cryptography. Then you can ask yourselves, okay, what is consensus useful if cryptography solves all these things? Well, it turns out that the last thing that's still not solved is this concept of liveness. Like, how do you make sure that the chain just keeps on going, even if validators just don't show up? For example, if there's world war three and 90% of the population has gone, this is kind of cool because we're reducing and reducing and reducing the scope of consensus, and we're kind of hardening the rest with pure cryptography and physics and mathematics. It's a very long journey to get the one shot signatures because we need these quantum computers. But in the meantime, we're going to enjoy the spoils of snarks, which are extremely significant."
    },
    {
        "speaker": "A",
        "text": "Well, it seems like we have entered the snark era for sure, and there's going to be a lot of that applied to crypto in the future. Brian, Justin, thank you so much for guiding us on the tour today. It's been much appreciated."
    },
    {
        "speaker": "B",
        "text": "Thank you."
    },
    {
        "speaker": "C",
        "text": "Thank you."
    },
    {
        "speaker": "A",
        "text": "Bankless nation risk and disclaimers. Gotta let you know, of course, none of this has been financial advice. I don't even think we talked Price in this whole episode, so obviously not crypto is risky, so our compilers, so our new layer twos, you could lose what you put in. But we are headed west. This is the frontier. It's not for everyone. But we're glad you're with us on the bankless journey. Thanks a lot."
    }
]