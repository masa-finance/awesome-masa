Speaker A: What we're advocating for with EAC is sort of freedom of access to compute, freedom of access to AI. We don't want these centralized entities, just like a central bank, like inflating away your money to control access to advanced AI. We want it in the hands of many because otherwise the centralized parties are going to abuse their power.
Speaker B: Welcome to bankless, where we explore the frontier of Internet money and Internet finance. This is Ryan. Sean Adams. I'm here with David Hoffman, and we're here to help you become more bankless or maybe more IAC in today's episode, right?
Speaker C: Accelerationist.
Speaker B: Yeah, yeah. We have Beth Jasos. He's the founding father of IAC. That's effective accelerationism. This is a philosophy that thinks AI doomers are not only wrong, they're taking humanity in the worst possible direction. The emphasis here is growth, acceleration, progress. These are the core pillars. And instead of slowing down on AI, Beth explains today why we should be speeding up. I think this is a direct answer to an episode that we recorded over a year ago with Eliezer Yudkowski on why we are all doomed and why AI is going to kill us all. So it's a debate, I guess, if you listen to those two episodes sequentially, we talk about a few things on today's episode, including why AI isn't going to kill us after all, according to Bethe. And what is IAC? He explains the fundamentals and why IAC is more than just AI. For Beth Jasos, it's an entire philosophy for life. Also, we get into what in the world homo techno capital, memetic accelerationism is. Did I say that right, David?
Speaker C: I don't know.
Speaker B: Finally, we end with Beth's take on crypto.
Speaker C: One thing I enjoyed about this conversation is that I think you could end up at this conversation from a variety of different angles. If you count are coming from crypto, you will start to catch a vibe for this. It will sound familiar to you a little bit if you've been paying attention to the AI world, for sure, but also you can go really far back and just talk about philosophy. Like, if you are a fan of Friedrich Nietzsche, you will find yourself in a familiar place. I've been a big fan of the concept of optimism as a way to achieve strong mental health. Like, optimists tend to feel better. They tend to be less depressed. They tend to feel like they have more control and agency around their surroundings. So if you come from it from like, the mental health and psychology background, you will find yourself, I think, resonating with some of the things that Beth Jesus has to say. That's one thing I really just enjoyed about this conversation, is Beth starts from a conversation around base principles getting really close to the metal and then expands it, extrapolates it outwards onto a political philosophy that people could align with.
Speaker B: So it's worth listening to this episode for that reason alone. David, there's so much more to discuss with you in the debrief. I think I've got to take some time to really unpack this. And that, of course, is the episode that we record right after the episode. If you're a bankless citizen, you can enjoy that now. All right, let's get right to the episode with Beth. But before we do, we want to thank the sponsors that made this possible, including bankless nation. I'm very excited to introduce our next guest. This is one guest, but two identities on the podcast today. Beth JSOs is a founding father of the effective accelerationism movement. This is also known as IAC. You've heard David and I talk about IAC before, particularly in contrast to the AI safety or decel movement, as some would call it. IAC is a basic philosophy that advocates for the full acceleration into our AI powered future. This is all gas and no brakes. Beth, welcome to bankless.
Speaker A: Well, thanks for having me. Pleasure to meet you, Ryan and David. And excited for the conversation today.
Speaker B: Now, I said there were actually two identities. One guess, but two identities. So Beth is your online alter ego, but in meatspace, your name is. Your identity, I should say, is Guillaume Verdone. This is your original identity, your nation's human signed identity.
Speaker A: Yeah.
Speaker B: Your human name, let's call it. And you are a physicist, quantum researcher, and the founder of an AI startup called Extropic. So I think we'll be talking more to your BEF Persona today, if that's okay. But also, welcome to Guillaume.
Speaker A: Same person, but appreciate it. Appreciate it. Yeah. Good to hear.
Speaker B: Include all of the identities in the intros. Why don't we actually start? Because for some people, that may have been a little bit jarring. Right. So let's start with the decision to use a pseudonym to kickstart the effective accelerationism movement. Why did you use this pseudonym of Beth Jasos? Why not just use your regular old nation state fiat name?
Speaker A: I think at the time, I was in some secretive teams within the a secretive unit of a big tech company as a Google X, and their secrecy is baseline. I was in particularly secretive team can't really talk about what I was doing. People can try to piece it together for my patents and have fun with that. Some Easter eggs in there. But secrecy was my baseline. I couldn't necessarily say too many have too many hot takes that wouldn't get back to me. Right. And. Or I felt, I don't know if that was, like, fully true. I mean, I've had some feedback that way, but there was always that sort of, you know, thought that if I said something that would be over the line, it's over, I would jeopardize my career, you know, again, this was like pre elon buying Twitter. It feels like a completely different vibe now, which is great. Back then, though, it felt like if you said something, some sort of latent truth or something, that you felt you were even just trying to experiment with your ideas and experiment with different point of views, you can lose your job and get cancelled and so on.
Speaker C: You felt muzzled.
Speaker A: Yeah, I did. And it doesn't have to do with any one particular employer, to be fair. I think it was just the general vibe. I think many people felt this way, and I think that's why anon accounts were so potent. You're kind of removing the sort of threat vector of reputational counter attacks if people try to shut you down, which, of course, now that I am doxxed, I no longer have that shield, but I still stand by my values, and that's why I'm here on these podcasts. But at the time, it's funny how giving myself an anonymous account opened up a whole new space of ideas that I didn't even let myself experiment with because I knew I wouldn't even be able to communicate them right. So freedom of speech induced freedom of thought for me, and sort of Beth was a sort of experiment in applying my thinking that I've applied to sort of quantum computing, physics based computing, and AI, applying that sort of thinking and background to sort of civilization and societal systems and how we organize ourselves in culture. And really it was supposed to start as a low stakes experiment, have a couple followers, and just put some ideas out there, see which ones stick. But it kind of grew and kept compounding and now were here, I guess. But that was really nice for me, because at the time, I think I had grown my reputation in my field of study, which was quantum computing, and it wasn't clear to me if my fans at the time would just wanted a job or something like that. It was kind of the opposite of why most people do. And on account maybe they just want. Well, I think eventually people just want their ideas to be evaluated on their own, regardless of reputation, regardless of your background. For me, it felt like I do the analogy of new game. Plus, when you have us beat the video game, you want to restart with your knowledge base and maybe some of your gear, but you're restarting the game. And so for me, it was like, can I rebuild a sort of following or reputation from scratch in an uncorrelated fashion? So I would never talk about quantum computing, I'd only talk about AI. Started it while in Google X, and then I ended up leaving that and starting my own company. Now, of course, I'm my own boss, more or less. So I have a lot more freedom of speech. But I think overall, people feel freer to say sort of things that were maybe taboo or off limits a couple years ago. And I think that's healthy for everyone embracing that sort of variance in the space of thoughts. And so, yeah, I don't know.
Speaker B: Well, it also feels to me, I mean, try this on too. Just from my observation, Beth has a different communication style than Guillaume, I think, you know, Beth is a bit more memetic. Beth is a bit more, like, in your face. Beth is a bit more rhetoric, I think. And that communication style seems to have meme market fit online, at least. And I think accounts for the growth of the Yak movement so far. Like, I don't know if you were trying that personality on or that Persona on, and it's just like a better communication style for the spread of ideas. But that's one observation I have from seeing Beth communicate.
Speaker A: Yeah, I mean, like we say, we're on a mission to spread viral optimism, a sort of fuck you optimism, like unapologetic optimism about the future, right? To fight the sort of pervasive pessimism and doomerism, and we will do anything we need to do in order to achieve mimetic fitness, right? Because if the. If it's a sort of viral memetic vaccine that renders people immune to feeling terrible about the future and being demoralized, we have a responsibility to make it spread. In an era of algorithmic information propagation, you can a b test what sort of packaging of your message has highest memetic fitness. And usually that's memes. That's a tone that's sort of brash in your face, that's adversarial, that's what gets propagated, that's what gets seen in, ergo, that becomes. That shapes people's priors, because they have an information diet that is fed to them by these algorithms. It is a certain communication style. I had experimented with similar firebrand style in quantum computing. Back then, I was really trying to push quantum computing towards something like deep learning, differentiable programming. So that's what I did at Google with some of my team there, is bringing deep learning, thinking, let gradient descent program for you to quantum computing. And that was a sort of similar fight back then against, let's say, the complexity theorists that needed hard proofs about everything, every statement, rather than just letting go and letting the computer figure out how to program for you. And it feels very similar to this battle against the rationalists and the eas that want to have axioms and want you to prove that humanity will be eternal and you guarantee our safety forever. And if you can't prove that, we got to shut everything down. Very similar sort of adversary. So I've been through this drill before, so I bring that sort of, sort of baggage to this new fight.
Speaker B: So, Beth, we're already talking about adversaries, the rationalists in the EA. And so I think we need to tee up this conversation now that we've established who we're talking to. Beth Jasos, one of the founding fathers of effective accelerationism. So let me just throw some context for bankless listeners and for you, Beth, as we get into this episode of what we're hoping to achieve in this and the reason for this episode. So about a year ago, like almost a year ago to the day that we are recording, we did an episode with a gentleman by the name of Eliezer Ukowski. I don't know if you know him at all or run across him in your travels. So we thought, David and I thought in our more innocent days that we were going to have a nice conversation with Eliezer Utkowski about the interplay between AI and crypto. And it turned out that was not the conversation Destiny had in store for us. It turned out we got stuck. Yeah, we got stuck somewhere, and we were going to end that episode in a full blown existential crisis about this thing I didn't know as much about as I feel like I know today, but yet I'm still kind of, like, wondering how much I really know that is AI safety. And Eliezer offered a compelling argument, basically why we titled the episode why AI is going to kill us all, because that was the distinct impression he left us with. In fact, the entire episode was a dire warning. Halt all AI progress right now. Go be with your loved ones. Kiss your children goodbye, because we're not going to.
Speaker C: I think Brian went and followed that advice.
Speaker B: It's never bad advice to go to tell the people in your life you love him. And so we're having you on partially, Beth, because you stand on pretty much the opposite side of that argument, I think. And not only do you not think that AI is going to kill us, you think it's going to be great, like, maybe usher us into a utopia. Okay, so this is not like he was right. You, I think, are saying Eliezer is dead wrong. In fact, it's the complete opposite. And we should accelerate progress on AI completely across the board. And so bankless is a technology podcast, but very specific in the crypto community. And I want to give you the chance to is explain your side of the argument. I want to give you a chance to pitch effective accelerationism IAc to the crypto audience and just, like, pill us, straight up pill us, like, give us.
Speaker A: The gospel of IAC, Beth, getting right into it. Yeah. I mean, you know, yeah. That Yudkowski podcast really was, I think, the start for you all to get into this sort of, this sort of area.
Speaker C: Did it tumble across your feed?
Speaker A: It definitely did. It definitely did. I think this sort of doom mongering or fear mongering has been really nefarious. It's really affected people psychologically. And yet they thought GPT-2 was going to kill us all. GPT-3 red. GPT four, 4.5 soon. Yeah, we're still here, right? The economy hasn't collapsed. Everything's fine. The crypto bags are still. They're doing all right. They're doing all right these days, but.
Speaker B: These are the last days, maybe, Beth, that's what Ali's here says. Enjoy the last days.
Speaker A: No, I think that that's the thing about having a system that is malleable and adaptive. Even if you have new technologies that are progressively rolled out, it just morphs and absorbs that capability and creates utility out of those new technological capabilities. And really, IAC is about trying to understand where do we come from and where are we going? What is the process that gave rise to life? What is the process that gave rise to civilization? What is the sort of this weird. Clearly something's going on. When we were maybe children, we had massive computers that were exponentially worse than the ones we're using to converse today. Something's going on. There's a sort of evolutionary process in the space of technologies, in the space of ideas. What is this machine? What is this process that's always morphing and adapting civilization and the technologies around us? Really. IAC, first and foremost, is trying to understand this process, how does it work? How did it get us here? Why is it good? And then how do we accelerate it? That's what we call acceleration, or techno capital acceleration, or more generally, homo technocapital memetic acceleration. That's all the things. Essentially, we think that this process of searching over the space of parameters or bits of information and how we organize ourselves culturally in terms of genetics, of humans, in terms of the space of technologies, in terms of how to organize companies, nations, etcetera, it's all one big search process. And competition induces a sort of evolutionary selective pressure on the space of all these things. And that sort of competition breeds fitness that then benefits us. All right? In a sense, we've tried capitalism and freedom versus authoritarianism or communism, where everything's top down, prescribed prices are essentially controlled. So far, free markets have been far more successful at creating wonderful things, such as the technologies we used to chat with today and the system we live in today. And overall, such systems where you have many more freedoms, you find much better. Optima, in terms of, again, technology is ways to live your life and just about everything. But I think it's too broad of a question. Just pill me on IAC. Let's go through how you feel about doomerism, and let's just pick it apart. Really, let's just pick it apart.
Speaker C: I kind of want to do one more thing first, just to frame this conversation. One of the maybe like, disarming things about the conversation that we had with Eleazer Yudkowski that maybe like, me and Ryan just weren't ready for is that Eleazer's arguments were extremely technical. They went down to the basement on, like, how AI works. He was using phrases called, like, gradient descent and reward mechanisms. And it all got, like, outside of our frame of expertise very quickly because he was like, down at the basement level and he was making this kind of logical argument that me and Ryan were just, weren't ready to fully unpack because he got really, really technical. And this conversation, this ex versus decel, people can approach this conversation at varying heights. Some people, I think Mark Andreessen, wrote the techno optimist manifesto talking about this at a very high level. And then we went and me and Ryan did a number of episodes about AI safety. And each one of those conversations were like somewhere up and down in the very technical to very philosophical conversation. I'm wondering where you see your innovation with this conversation. You run an AI startup, so you're pretty technical, but you're also just in. Your response just now is some pretty philosophical directional approaches. Where would you say you would like to innovate in this conversation?
Speaker A: Yeah, I mean, whatever you think is more appropriate for your audience. For me, it's always difficult to adjust my level of technicality. I tend to converse pretty technically. I was a theoretical physicist, and then now I run an AI startup. Usually I have very sort of mathematics first thinking, trying to explain my thinking, and I have to convert that into English to some extent. And so if there's any words I say or anything like that, that you want to dig in, let's dig in. But I would say yudd is not very technical, actually, and that if you start digging into his technical knowledge, it's actually lacking quite a bit. Right. I mean, this notion of recursive self improvement, I guess that the runaway foom maybe we can, we can address recursive self improvement is something we've tried in machine learning for a very long time. It's called meta learning. You have a system that learns how to accelerate the learning, and you have also architecture search algorithms. The reality is that the larger the space over which you search, the exponentially harder things become, it becomes to find the true optimum. What that means is that if we task an AI to improve aih, it's exponentially more complicated. Every level of an AI that improves an AI that improves an AI, you go, and so it's going to take exponentially more compute and energy to achieve that optimum. And so in a sense, there's already a soft cap on compute in terms of the energetic and capital costs of compute, and that keeps us safe, right? And in a sense, like everything, everything in our civilization, even every life form, is trying to fume, everything is trying to grow. And that is the thesis of IAC, but that because everything is trying to grow and competing with one another for resources, we yield, we get better Optima, but it also keeps everything in check. There's not one sort of singleton biological system or one single company or one single nation that takes over the whole system because they're competing with one another in general to find optima of high fitness. For example, if you're building a company, a company is almost like, at least in the startup space at the seed round, you're basically a search algorithm. You're searching over the space of technologies and you're burning capital and intellectual compute and maybe actual GPU compute to find, to pinpoint exactly what product you got to build. That's a couple bits of information, but that's costly, that's a costly thing to find. Every optimum, that is, high fitness is very costly to find. And the more optimal it is, often it's exponentially costlier to find that optimum. And so that keeps us safe in the sense that I can pretty confidently say we're not just a small. At a distance like this one weird trick where now we have super AGI that fumes and creates grey goo and takes over the world, that's very exponentially unlikely. That is the case. You can't guarantee it. You can't guarantee that. There's not one weird hack where, from what we've seen so far, it's taken billions of years to get where we are now with life and intelligence as we know it. And it's been a very long sort of process of improvement to this point. But there won't be a sort of hyperbolic discontinuity in the rate in which things improve. And in general, we take the opposite camp. Like this sort of rate of self improvement of everything is actually really important, and it's what creates everything we enjoy. And we should seek to keep that process growing and scale it, right? Really, systems in nature either get busy growing or they get busy dying. They either secure more resources, secure more free energy, and figure out cleverer ways to utilize it in order to grow, or they try to stagnate. They run out of fuel and then they die. That is it. Everything runs on some type of fuel. Nothing is forever. No bit of information is forever. To maintain its coherence, it costs energy, because everything wants to decohere naturally. Everything wants to sort of. It's a constant fight against entropy. And so the thesis of IAC is the one golden metric, to some extent, that measures the progress of the whole system is sort of free energy. How much energy are we acquiring and consuming as a civilization? Because that's a metric of our progress that can't be gamed. Right? And this will resonate with you guys, but if you measure it in us dollars, that's not objective scale, right? That's a scale that you can play with. You can inflate things, you can have inflation, you can print money, and then it seems like you have some progress, but really, you went the wrong way. Whereas something objective like energy, is a good metric to measure progress. And we think that scaling up civilization and doing so with urgency is how we ensure its sort of long term success and existence. And I think that one of the most dangerous things we could do is let this mindset of doom and demoralization become hyper stitches. Right? If you focus on doom, you focus on darker futures. Well, first of all, you stop building, you stop having children, you stop hoping for better things in the future. It sort of becomes a self fulfilling prophecy, and that's how civilizations and empires die, and that will cause massive pain. And so we're on a mission to spread optimism that is hyperstitious in the sense, yes, we can do it. We can build better things. We could build a better future. We can leverage aih to build a better future. We want to cure diseases, to tackle climate change, to unlock nuclear fusion, nuclear energy, massive prosperity. We have all this upside on the table, and the longer we wait, the longer we wait, the lower likelihood we can achieve it, and we have urgency to make it happen. And that's very similar a mindset you have in startups, in Silicon Valley. The most successful startups are very optimistic, and they believe they can do something that seems unimaginable at the beginning of the company, but then they do it, and it just keeps happening. And this is hyper stitches optimism effect. And that's the sort of mentality we're trying to scale to the world with IAC. And it's why we think that dumerism and pessimism is really dangerous and needs to be fought somewhat aggressively. Right. Because that is actually the source of doom, not some sort of fictitious artificial superintelligence from Sci-Fi. It's not backed by science. Right. I've worked on AI for material generation, protein folding, biochemistry, chemistry, all like AI for the physical world. It's much harder than people think. The physical world is really hard. And one of the best things at designing things in the physical world is life itself. But you could see Life Itself as a BiG Optimization algorithm that is trying to foom. Every life form is trying to foom, and yet nothing has fumed. Right. So that should give you a bit more peace and you can breathe. But happy to go into any sort of argument, like ANy Sticking points you have about Yud's argument.
Speaker B: One thing you've mentioned a couple of times is this word foom. Could you define that really quick for people?
Speaker A: Yeah, I mean, you know, well, how do you understand foom? Like from your interview with Yud, how would you explain it?
Speaker C: Yeah, I think you're using it in a very general sense, which I think I wish, I appreciate, because it kind of gets down to the bare metal of kind of how life works. Like life is interested in life, like life is interested in propagating foom, in the AI sense, is mostly talking about that super intelligence explosion where AI just takes over Everything. And it's just like this one single event, and all of a Sudden, the whole world is AI, and it's run by AI's, for AI's. Trees want to plant more trees. Bugs want there to be more bugs. Ants wants more ants. Humans want more humans. And I think it's all trying to hit some sort of point in a curve, which once the ball starts rolling, it starts rolling faster and faster and faster, and all of a sudden we have a population explosion. It doesn't matter what kind of life form you are, but everything is trying to look for that growth in population. That is something that's fundamental about life. And so you're using, in a general sense, where it's just like, hey, any system whatsoever that's propagating is trying to find more energy, because that's how it can propagate even more. And this is the playing field that just the universe exists on. I think that this is how you're using it.
Speaker A: Every system and subsystem, whether it's a company, a group of people, a culture, like you said, bugs, trees, whatever, even nation states, everything is selfdevelop organizing self, adapting its inner workings in order to grow. And by construction, things that are not optimizing themselves to grow, run out of fuel, and they fade. And that's it. Right? And so if you think about. If you think in the future, which culture, which, like, if you think of like, several nations in the future, which nations will have survived and what culture would they have? Right? Well, they would probably have an IAC culture that is literally by construction, trying to figure out what is the optimal way to organize ourselves in order to grow. And the sort of pessimistic, dumeristic cultures will have faded because they'll have destroyed themselves.
Speaker B: This kind of reminds me, by the way, of an episode we did with Robin Hansen about his theory for why we haven't seen aliens is basically that, for one, it's too early, but we will see them. And the ones we will see are not the quiet aliens that stick to their home planet, not the d cell aliens. We'll see the IaC aliens, the grabby aliens that go and they consume their solar system, they consume their galaxy, and then they're a multi galaxy type of. The grabby ones are the ones that are effectively going to foom and win.
Speaker A: Yeah, well, it's the same with variants of a virus, right? It's the ones that have higher replicability that you get statistically. Right. You don't see, like, earlier variants that are lower fitness. Right? And it's like everything, everything, every bit of information is getting selected for in terms of, does it confer the organism of which is part of an ability to grow? Right. Like your genetics. It's like, does this piece of the genetic code give you higher fitness? Does it make you have more offspring? And then that piece of code, genetic code is higher likelihood in the future. But it's like applying that sort of evolutionary thinking to everything, including culture, including ways to organize your companies, including, you can think of that sort of thinking even applied to crypto, which sort of coins have the best sort of mimetic fitness in the long term.
Speaker C: Bitcoin maximalism is the bitcoin fumers. They think bitcoin is going to foom.
Speaker A: Well, I don't know about that. What I do like about bitcoin is that it is proof of work, and so it is anchored to sort of physics and energy consumption. And I'm not necessarily a bitcoin maximalist, but I am a sort of energy maximalist. I think that is the right metric to pin things with respect to.
Speaker C: Yeah, I want to provide the way that I see the structure of this conversation. But if you have a very basement first principles grounding in your arguments, and that is what I would consider the depth of where some of this belief structure comes from. But we've watched, and we've watched a lot of these conversations permeate throughout Silicon Valley and then also make its way into Capitol Hill, where some of these very first principle arguments are inspiring or being argued against as, like, political stances about the direction for society at large. So it kind of goes back to just like, we have some beliefs, we have a structure of thought, it can get very technical and granular in their base principle arguments. But then, as this conversation progresses, and Ryan's going to take us into the next phase here, it really can inspire a political belief like a direction, a proposed direction for humanity. And so this is kind of why Ryan kind of illustrated this, as you're inspiring, like a movement, like a political movement, not necessarily a party, because that's too structured, but just like a set of beliefs for how we ought to live as humanity, as a species. This is how you see it.
Speaker A: Yeah. I mean, hopefully it does affect politics. Right? It's like the ways we've been organizing ourselves, the ways we've been legislating, have been pretty far from optimal. And we should think from a first principle standpoint, what is, how do we organize a hierarchy of cybernetic control in our civilization? And this is a very deep concept. So how do we. In a sense, it's very similar to crypto. Many layers of a protocol, you chunk blocks, and then you have different layers that check at different scales, and base layers have a slower clock rate, and they roll up larger amounts of actions or transactions. It's very similar to having a hierarchy of legislation at the local, provincial, or state level, and then national level and then international level. As you go up the hierarchy, you should have a slower feedback loop, and it should be lighter and lighter touch in terms of its. Its ability to control things. And the goal with EOC is to start a discussion of how should we balance things, because one side of the aisle wants everyone to be hands off, one side of the aisle wants absolute, total control in the hands of a few. And both of those things are not optimal. There's a spectrum in between, and we should search over that spectrum. And it's very relevant to the centralization versus decentralization question in crypto for protocols. I think they're the same question. Whether it's in politics versus crypto is a system that is totally decentralized and greedy, optimal versus one that is centralized versus one that is hierarchical. I think it's the latter, and maybe we can get into that. But hopefully that does inspire conversations on Capitol Hill and in DC about where to take policy and legislation.
Speaker B: Yeah, I think it's really important. I think this YAC, do we accelerate or do we deaccelerate, is probably one of the most important conversations that society is having. And IAC, to me, one question I was going to have for you, Beth, is it a philosophy, is it a movement? Is this a social revolution? Is it a religion of some sorts? Is it like this amorphous thing like crypto, where it's like a grab bag of all of the above? What's your take on this?
Speaker A: I think it's all the above to some extent, right? I mean, for me, it's, you know, it's almost like religious like belief just because it's like I can get into that. But, you know, and for some it is. For some it's a, you know, it's a community of like minded individual that are optimistic about the future and trying to help each other and want to build to this better future and collaborate. And for some, it's sort of a political or ideological movement of how we should legislate things or how we should run things. And for some, it's just, I don't know, inspirational and gets them fired up to build more.
Speaker B: So I very much want to get your take on why this is a religion, how it sort of explains big questions like, why are we here? What is the universe? This sort of thing. But before we do, got to tie off this Eliezer existential crisis type thing. So we talked about foom. We sort of defined it. You think it's very unlikely that AI's sort of, like, destroy humanity or accidentally, or they literally go to war with us or something like that. But I want to maybe ask. So David said that it was difficult to follow the Eliezer conversation because it was technical. I think it was less that for me, anyway, personally. It was more because the premise was so simple. We've seen dangerous technologies in the past. Anybody? Bankless listener watched the Oppenheimer movie recently. The whole movie is about this chain of events where the scientific community discovers some dangerous technology, essentially, and this chain of events that could lead to global Armageddon and the ending of humanity. And so now we're at the precipice of this new discovery. We're in, like, these times where, oh, my God, you can talk to a computer, and it's passed the Turing test, and it sounds like a real person. And this is amazing. It generates art like we've never seen before. And we are all wondering whether this is one of those technologies that has the potential to destroy everything. And, like, it's very. It's very, you know, seductive. I think, for Aliezer to be like, yeah, imagine if, you know, we split an atom with kind of, like, everybody could do that in their microwave, let's say. And imagine that was, like, given that kind of level of power and technology was given to every human being, we democratize it in that way. Well, what would happen? It'd probably be the end of life as we know it. And so, like, I still have this base question, Beth, of whether AI is, like, dangerous or not. Is it similar to nuclear weapons? Is it similar to biological pandemics that somebody could cook up in their basement? Or is it different somehow? Because I feel like a lot of this conversation hinges upon on that question. I mean, you were advocating an EAC for growth, right? Versus degrowth. And I sort of understand that everybody wants growth over degrowth. Deacceleration is my thing. But I think another framing that people like Eliezer might have is it's growth versus safety. Like, bev, you were going all gas, no brakes here. Like, we're worried, what if this technology is dangerous? What's your take on this?
Speaker A: Yeah, so the analogy, I think if everybody could split the atom in their microwave, we'd have quite a bit more energy being produced on the grid, and that would be net positive, I think, runaway effects. If you have a chain reaction that becomes a bomb, of course that's bad, but obviously it seems to be. Creating a nuclear weapon is very involved and not everybody has access to it. The thing with nuclear weapons is it's pure downside, right? Like the only utility to nuclear weapons is like, you know, damaging your enemy, and it's like a deterrent, right? Whereas AI, there's huge, huge, huge, huge upside to creating AI. And leveraging AI and everything we do gives us intellectual and operational leverage. It helps, you know, create economic value. It helps solve all our problems. It helps us live better lives. It's gonna save lives, right, in medicine, right? It's gonna make things cheaper. It's going to help us build cheap housing. It's going to help us, you know, save on legal bills that are, you know, ballooning, right? You're going to have LLM lawyers, you're going to have LLM doctors. Everybody on the planet's going to have access to some of the best doctors ever. You're going to have personalized medicine, right? Like, the list goes on. Like, a lot of our problems that are pain points in our modern society will be solved with cheaper intelligence, and there is an urgency to make that happen. Similar to how, let's say, fear mongering about nuclear weapons actually kind of killed the nuclear fission energy industry. Right now it's over regulated to oblivion. There's so much red tape, like most of your budget, building a nuclear reactor is towards compliance, and that's suppressed the advent of ubiquity of this technology. And it's caused us to have to go to all sorts of wars all over the place for energy. So this mind virus of anti nuclear was actually detrimental. And we're kind of like waltzing into a similar scenario with AI. If we overregulate it in the womb, we're not going to see the massive upsides to it. I think that what's dangerous with AI, and we're very transparent about this, AI is information, is power. Intelligence allows you to extract more utility out of less information. And so it is confers you power, right? And if you have a big delta, a big difference between the capabilities of centralized entities and that of the people in terms of AI, then you have the opportunity for sort of AI assisted tyranny. And to us that's the highest existential risk because there's a very strong prior that if you give all the power to centralized party becomes corrupt and then it abuses that power and oppresses people and causes mass suffering. And to us, like putting AI, which yes, it will make, whoever has advanced AI in their hands will become formidable. But if everybody is not equally formidable but in a similar range, then no one party is going to have too much power over the other and going to completely dominate the other. It's a sort of adversarial equilibrium. And that is what we're advocating for with EAC is sort of freedom of access to compute, freedom of access to AI. We don't want these centralized entities just like a central bank, like inflating away your money to control access to advanced AI. We want it in the hands of many, because otherwise the centralized parties are going to abuse their power. And so to me I think that, yes, AI is potent. If you only focus on tail risks, you can convince yourself to kill anything in the womb. But there's also massive upside that shouldn't be discounted, and there's not enough. We don't talk enough about the massive upside we're leaving on the table. And the reality is that most likely what will happen is that some nations or some cultures will embrace AI. Some will want to ban AI. Those that ban AI are going to be left behind and massively disadvantaged, and those that embrace it are going to be economically prosperous, will outgrow those that have banned it. And so you want to be in the fork, right? Just similar to the crypto fork. You want to be in the cultural fork that embraces AI and integrates it into their lives, into society, and leverages it to become formidable and grow. There are ways forward to leverage AI and merge with it. We're using smartwatches and smartphones and how Apple Vision Pro is going viral. We're already augmenting ourselves. A notion of humanity, of a human individual, is getting sort of diluted or eroded. We already leveraged exogenous sources of intelligence. We use Google search, perplexity, whatnot, all day, every day. And I think that just focusing on, oh no, humans will be left behind. It's like, no, but the notion of human will drift, encompass a human, and maybe their fleet of AI's that do do their bidding, and then you become this sort of formidable being. And that's the sort of awesome future we want to have, where you're quasi immortal, you have AI customizing your biology, and you have sort of AI augmentations in every way possible. And I think that sort of optimistic future, we got to paint the picture of that future rather than painting the picture of the doomer future where AI takes over, which again, I don't know how it would do that if there's other powerful AI's keeping it in check from parties that aren't necessarily aligned with that party. And again, I don't believe in fast takeoff in AI. I think it's not well founded. And so if you don't necessarily think that the likelihood of this fully existential doom is high from just pure ASI, then the biggest threat vector becomes authoritarianism, censorship. If you only have AI in the hands of a few, and they use it for information control and cultural control, then they can basically psyop you and control you. That is very dangerous, because if only a few parties that are government backed, cartel control olms, and they become our sources of truth, then they control you. And they can use that to consolidate their control and consolidate their power. And now we live in a dark age where we don't have open access to knowledge, we don't have open access to compute, and we can't fight this sort of top down control with AI, with our own AI's that would help us filter and counter sort of this psychological manipulation. And so that's the dark future we're trying to avoid. I so getting AI in the hands of many into every and accelerating makes it very hard to sort of acceleration is a hedge against sort of top down control and maybe a centralized party taking over, killing variants, killing, like, you can only use AI this way and taking control of the whole technology because that would be the bad future we're trying to avoid anyway. We have our own existential risks, like authoritarianism. And centralized control is sort of, that's the bad scenario. It's based on data from history, right? There's a lot of bad things that have happened historically when that happens, versus sort of Sci-Fi based priors of, like, I read too many books on the Terminator, I read too many nineties Sci-Fi books about nanotechnology. And, you know, I think like, AI is going to figure out how to turn us into grey goo, which. Not plausible.
Speaker B: So, bev, basically you think P doom is kind of like a failed kind of calculation. What we should be doing is p utopia, right? And then also like the risk, on the risk side, it's like probability of totalitarianism. So like, let's do that population too.
Speaker A: Yeah. P 1984, right?
Speaker B: P 1984. Okay, so I think we're starting to understand the contours of IAC and like, where you're coming from. So it's pro growth. It's definitely techno optimism. I certainly love the picture you were painting about the possibilities here. It's much more cheery and optimistic than the AI doomer takes that I've heard so far. I want to get back to the ways IAC is like a religion to you, and I don't want to interpret too much into that. I'm sure there are many ways that it's not like religion. Talk about kind of the basement level, because I know you are. I have been, as a physicist and throughout childhood, I've heard other conversations with you. You've been always in search for what's the basement level of this whole experience that we're having right now in the universe, and why does all of this matter? What conclusions have you come to? And how does that relate to IAC? How does IAC even explain some of these things?
Speaker A: Yeah, I mean, its been a journey throughout life to try to understand our place in the universe. That led me to theoretical physics, trying to understand the very small, the quantum mechanical, and the very big, the cosmos. So I was a quantum cosmologist at some point. I also worked on sort of black hole physics, which are kind of the edges, different edge of the universe. Right. And there I was trying to understand where did we come from? Why are we here? By looking at sort of the very big and the very small. And I kind of missed the middle part, right. And in a sense, after some time, I realized that a lot of the beauty and complexity and good things in the world came from sort of emergence and self organization. Basically, instead of looking at quantum physics, or what is called general relativity, so gravity, which are the mostly physics, the very small and very big, I need to look at the thermodynamics. The physics of the middle were out of equilibrium. Thermodynamics, the physics of life, the physics of self organizing complex systems, because that's what created us, that's what created a lot of civilization, technologies we see today. And that's what induces this sort of selective pressure on the space of everything. And that sort of principle is very deep. And so having gone through theoretical physics and eventually discovered that sort of complexity and self organization is where a lot of the answers are. That's what led me to look at EOC as sort of like, hey, thermodynamics helped create, well, it explains why we have life at all. Right? How do we get here? Explains evolution. It's upstream of evolution. It's upstream of market pressures, selective pressures. It's kind of like this generalized law that creates everything we know, right. That is relevant to us in our day to day life. What is Andromeda going to do in 2 billion years? Not that relevant day to day, I think. To me, Iock is almost a religion because it's like, okay, we are part of this process, the self adaptive process of life. Memes, information technology, civilization, and this whole system seeks to grow. We are a very special phase of matter that was very unlikely in our universe, and we have a sort of responsibility to cherish it and allow it to grow in order to be robust to sort of erasure or fluctuations, right, like an asteroid, right? So it's very elonian in the sense of it has very high inner product with the quest to increase the scope and scale of civilization. Consciousness is ailonian.
Speaker B: Is that like of derived from elon?
Speaker A: I don't know, I just made that up. But the point is, we're very aligned with that mission, and it gives you crazy amount of purpose day to day. What is the point of a religion? A religion is like, like for a lot of people, practically. It is a cultural heuristic that if it has stuck around for a long time, it has been post selected for in terms of conferring its adherence, a sort of advantage, right? Because by construction, it has grown and has propagated this far, this long, right? And so it's like ways to live your life. It's like prescriptions, right? To me, they're like cultural parameters, similar to how, like you, you have in a neural net. You spend a lot of time training and finding the optimal parameters, and then you run inference. You just run at the optimal parameters. To me, religions are like pre trained models of how to live your life, right? They're like prescribed parameters, and then you can just run inference.
Speaker B: You think, I kind of like prescribes how to live your life, actually, it gives you some meaning. It's basically. Basically, the meaning is growth, the meaning is propagation. The meaning is pushing humanity to the stars, to the frontier. These are some of the meanings you drive from this.
Speaker A: So to me, IAC is more like the meta learner or the optimizer. We're not prescribing any one particular way to live your life, but any sort of prescription that you may posit. What we're saying is it's going to be post selected for according to this sort of fitness function, and so you should select for subcultures that give you an advantage in this growth, right? If you want to be part of the part of the future, but really it's like, it's a search process for what yields higher growth, and it's always, always changing. It's not one way to live your life right now. There's not one set of optimal policy parameters that you can just run forever. It's like, let's collectively figure out heuristics of how to organize ourselves and how to live our lives, how to run the world such that we grow sustainably, fall tolerantly towards, yes, a galactic civilization.
Speaker B: Beth, one set of dots that may not have been connected for folks. I sort of connected them a little bit, as you were speaking. But because I've heard you speak in the past, and I've read many of your articles where you've written things like this, your Twitter timeline. But how does thermodynamics explain life? How does that explain why? Is that an underlying process in the universe that is relevant to what we're talking about? Like pro AI growth?
Speaker A: Yeah, yeah. No, that's a really deep question. And to me, it's like, I mean, I've made it quite a career pivot, right? I went from being someone in quantum computing. So trying to harness quantum mechanics of the world to make. Make computing devices, and make computing devices that understand the quantum mechanical world around us. That is what I did in quantum machine learning. Now, after the realization that the most interesting part is in the middle, it's in thermodynamics. Now, I'm doing thermodynamic computing, where I'm harnessing out of equilibrium thermodynamics to engineer self organizing devices that are kind of lifelike, but they do machine learning as a thermodynamic physical process. But once you understand these principles enough to engineer systems that enact them, you start seeing that pattern everywhere. And sort of, you know, I wrote the IAC physics manifesto at the same time as I founded my company. So for me, it's just because my mind is swimming in it. But, yeah. Why does life. Why did thermodynamics give rise to life? I'm an adherent of a theory by Jeremy Englund, former MIT professor. If you want to read his book, every life is on fire, and his lectures are great on this. But essentially, what you can show, it's just from the laws of probability, really, and it's a generalized notion of the second law of thermodynamics, where it shows that paths. Right? Like, if you think of, like, your. Your trajectory across time, right, of, like, you have a certain system, you have states over time, and you have a trajectory trajectories that have dissipated more heat along the trajectory are exponentially more likely from the laws of physics. That's pretty nuts, right? And so, you know, there's jars, there's theorems. They're called. You know, they have complicated names. But check out the work by Jeremy Englund on the thermodynamic dissipative adaptation. Basically, it posits that systems in nature, or really any system, any system that is in the physical world, will self adapt in order to maximize its dissipation for energy. So that includes figuring out how to capture more free energy to dissipate it. So lifelike systems are always adapting. How can I secure more fuel? How can I use it more efficiently so that I could budget my fuel to get more fuel and keep growing? And as a byproduct, you get evolution, because systems that have adapted their genetics in order to replicate more, well, if you replicate more, you burn more fuel as a whole system. And so you're more likely.
Speaker B: So human complexity exists to burn fuel faster for the universe to.
Speaker A: The whole concentration is a fancy fire, right? We're a very clever fire. We're like an energy seeking fire, right? But, yeah, that is what gave rise to us.
Speaker C: Is it correct to define life if this is the idea, that life is whatever system finds a way to dissipate more heat more. And so part of that definition, like you said, it includes collecting energy. So, like, reversing entropy first, like creating order, creating systems, creating structure in order to produce more heat, exhaust, which is. And that is what life is.
Speaker A: That is what all thermodynamic systems are. But life is like a very special version of that immersion property out of that. Yeah. I think people are trying to. Well, you know, is a virus life, right. You know, is.
Speaker C: It's on the spectrum, isn't it, AI.
Speaker A: That, you know, it's always adapting its architecture to be of economic utility to us. And if it's of high economic utility, then we run it on GPU's, we give it budget to, and we burn heat to keep it alive. Right? Is it alive? Right? So I think, like, we can't even. It's hard to define intelligence formally. It's hard to define life formally. And so I'm just like, what's the base layer? It's just thermodynamics and probability and information, right? I used to work in theoretical physics, where we're trying to understand the theory of everything, everything in the universe through the lens of information theory. It was the it from bit or it from qubit school of thought, right? It's it's it's seeing the whole universe as a big computer. Right. And so, sort of, I see a lot of civilization as a big thermodynamic computer, in a sense.
Speaker B: So the universe is kind of, like, cheering humanity on and the complexity of life on, and particularly, like, I guess, the civilization that we built, because, like, we've become really, really good at, like, burning energy.
Speaker A: Yes. Yes. Like, the universe is on our side, and we've been doing so good so far. We got to keep going. Right. And feeling that, like, wait, the universe is on our side? Like, the game is rigged in our favorite. That's such a potent realization, and that gives you really powerful optimism about the future. It's just, like, actually awesome futures are exponentially more likely, and dark futures are just small fluctuations on the road to these much higher likelihood, large scale futures. And so, yeah, I mean, that's why I'm an optimist. I just looked at the equations and saw, actually, these equations explain a lot of our world and a lot of where we came from, and you can apply them at the system at a large scale.
Speaker B: So you're all about, like, IAC and your school of thought, Beth, is all about burning more energy, harnessing more energy faster. Basically, that's why there's this, I've not fully understood it before, but there's this kind of desire to become, to rise up the Kardashev scale, right. And to become, like, a type two civilization, and then maybe a type three civilization, right. My understanding is Kardashev scales like, type one civilization marshals all of the resources of its home planet, right? So that would be everything on Earth. Humanity is able to marshal that energy potential, and then is type two, its local star. So wed marshal the power of the sun like Dyson sphere. And then type three is kind of the galaxy, and youre all about rising up and harnessing more energy for productive purposes. Thats kind of like your utility function. Thats what, that's what IAC is trying to maximize, in a way.
Speaker A: And to be clear, it's not like we don't want to just burn energy. The point is, it's the integral or the sum of how much energy we're burning on a near infinite time horizon. So if we figure out clever ways to utilize energy such that we can grow further, that's better. It's not because some people don't necessarily understand it, and they're like, oh, what if you just blew up the whole planet? That's a lot of energy gone. It's like, yeah, but then we're not going to but burn the energy of other planets because we're going to be dead. You don't want that. So we're pro life in the most fundamental way possible, and we think it's a very precious state of matter. It's like we're a very precious form of fire and we have a duty to let it grow. In scope and scale, you're just very.
Speaker B: Pro long term energy burning. So that means not burning, I shouldn't say harnessing. So that means you'd naturally be pro life because if you kill the system, that is like growing to harness more and more energy, then basically it's all gone. And I want to contrast this with what I think is like a different form of utility maximization function, because the measure of this, how much energy are we burning, is kind of like, I would say new and different, maybe to some listeners, because a lot of philosophies or schools of thought are much more along the curve of utility maximization of human happiness or human meaning or hedons hedonism, basically. And that's not what you're saying. Do you care at all about maximizing human happiness along the way, or is it just all about the energy consumption? Because there's an element, Beth, where I hear if it's just about energy, it almost sounds like a bit like machine, like, it almost sounds a bit like extractive, I think, to people. But are people happy? Are we happy along the way? Can you talk about that?
Speaker A: Yeah. So it's similar to crypto. It's like how do you measure market cap according to what currency, right? If we're saying this is a value system, what are we pinned to? What is our metric? Right? In a sense, heat ons are weird because you can just kind of like, first of all, it's very subjective, right? Your neurochemistry is very subjective. It can be hacked by drugs or wireheading, right? You can scroll TikTok all day or watch whatever you're going to watch on that apple vision Pro and just sit back and drool. And that's not a long term optimum, right? So it has these spurious optima and you can also just print more units, more cheap units, and game the system. And so these eas get caught in thought experiments, like maybe we should just have a bunch of shrimp and theyre very happy, and then we just have tons of shrimp and we maximize their happiness. Weve optimized our metric, right? Its like, no, its not going to yield much. So its like to me, its been seeking the sort of, whats the ultimate metric? I think happiness is actually a proxy or an estimator of this sort of gradient of how much influence do I have on future free energetic sort of expenditure? Right? Like, if you have an intellectual legacy, if you have positive impact on the world, you feel good about it, you feel happy. If you're in a meaningful relationship, your brain has neurochemistry that's like approximating that youre going to have successful progeny and youre going to influence the future lite cone, and theyre going to have progeny and theyre going to be successful and impact the world and consume more free energy. I mean, all our neurochemistry and our biology has been selected for according to this principle of whatever genetic sequence, whatever heuristic we have that will help us burn more free energy. So its like its the supply chain on happiness.
Speaker B: You see it as a much stronger kind of metric, I guess, and much more ungamable metric versus something like hedons, let's say. And so to you, it would be a complete failure of our species, of this whole human life experiment if we essentially were quiet, the quiet aliens. We reached some level of technology where we're all just kind of like, we're happy with things. We just strap into our VR and we just kind of like, live just a happy life in a, you know, I don't know, in a Wally life wall e life in a chair somewhere. Maybe we're floating across the universe. To you, that's like, like total objective failure by all of your measures, even though the hedons are maybe off the charts because hedons are fiat.
Speaker A: I'd love to establish those connections with crypto, but, yeah, I think at some point, our happiness, we've kind of entertained ourselves to death with this generation of technologies the past 1012 years. I don't know. Now we're just obsessed with our happiness or neurochemistry, and we've over optimized that. But we don't feel good. We don't feel deeply happy. Right? And whereas, if you're working towards something meaningful, it means you're going to leave a legacy, and yet you're working super hard, you have some stress hormones flowing through you, but you have this sort of deep happiness, right? Like, the satisfaction. It's very different. It's not hedonism. It's like, it's not happiness. Like eudaimonia. There's different.
Speaker B: It reminds me, Beth, of something that Mark Andreessen said when he came on the podcast we were asking for advice. And one of his advice in general, he's like, don't seek happiness, seek satisfaction. And you'll get that by finding meaning in your life. You're saying that is a much more durable metric than just Edan's dopamine.
Speaker A: I think our brainstor, like, how do you feel? Why are you feeling satisfied in the future if you have a massive positive impact? If you feel like the things I've done with my life will really increase the likelihood that we reach a larger scale of civilization? Like, you feel deeply good about that. And I think having everyone sort of think, evaluate their lives and think, what is the maximal positive impact I can have towards this? And then they go out and do it, and we support each other doing it, I think that's the most positive community we can have, and that is yac, right? That's the EAC community, and that's our sort of thinking. And that increases life satisfaction massively. And it makes people much more productive towards helping everyone around them, right? And so people mentally do a sort of sensitivity analysis. Like what? You know, for me it was like, what technology can I create that's gonna have the maximal positive impact on future scope and scale? Civilization I tried to work on faster than light travel, gave up on that after some theoretical physics. Now I'm working on really energy efficient AI compute. Because I think that AI is the technology that would create other technologies and will help everything and help us accelerate up this Kardashev scale that brings deep meaning, that helps me plow through any sort of challenges in my way. I have an irrational level of optimism, but that's the thing, right? You have to give up this sort of just pure reason, pure logic, stiff axioms, you just gotta like, this feels right, this feels like it's going towards a better future. And, you know, it's like a hard to verbalize, intuitive estimator of that. And I want to live my life that way. And that's great. Everybody's going to have their own thesis of how to live their lives and how to have a positive impact on the future. As long as we agree that we're all trying to, to optimize the same thing. This foundational metric of how much life, generalized notion of life is there, how much fire of life is there in our corner of the universe. And if we can all seek to grow it, then we're going to make it happen, and we can do it, and so we should.
Speaker C: Beth, can I throw an argument your way that I'm sure you've contended with. I'll call this the Elizabeth Warren style of arguments. I think one of your, the general gist of your vibe acceleration ism, I've kind of taken the perspective of humanity is like a line of humans marching towards the future. And you have the front of the line, the phalanx, like the innovators, the accelerationists, the entrepreneurs, the inventors, the scientists pushing the frontier forward. Like also the billionaires, right? Like, literally, Jeff Bezos, for example, Elon Musk getting us to Mars, putting wifi all over the world. And then you have like, and those are the people that are like pushing the fold. There's the people increasing the quality of life for people. Those are people doing the acceleration things, moving society forward at an accelerating rate. And then you have the last 50% of humanity who's living on like $6.55 a day on average, like below the US, below the United States poverty line. And so what you're doing, what you're advocating for is you're saying, hey, the front of humanity, go faster, press on the gas. Like, keep going, break away. And then like, the Elizabeth Warrens is like, yo, you guys are just forgetting about like the rest of the humans who are not able to keep up with you. And that's not fair because look at all these returns on capital. Literal, actual, Jeff Bezos has a bajillion dollars. He doesn't need that much. Elon Musk has a bjorn dollars and need that much. And you guys are just not thinking about the bottom half of society who can't keep up with you, and they're going to forever be succumbed to, like, never being able to catch up with the people who are accelerating. How would you respond to this perspective?
Speaker A: Yeah, so first of all, needing is a weird concept, really. Capital is a tool to exchange value, right? It's a way to keep track of how much value you're producing. And ultimately capital allocators that are really efficient at allocating capital towards utility in our system will get more capital to allocate. And that is a sort of, to some extent, it's like an AI algorithm, right? Like companies that are of high utility in the system get more capital to keep doing what they're doing and scale it up. Companies that do business with each other, very often theyre going to deepen their partnership. Theyre going to have more economic exchange. Its almost like neurons in a brain, right? Theres connections that strengthen the more they get used. And theres nodes that their cells, if you think of companies like neural cells, they get bigger, have more nutrients if theyre of high utility. And thats very important that we have this. If every neuron had as many connections as the other and as many nutrients, then your brain wouldn't work, right. And to me, like, I think, I think it's hard for people to understand that entrepreneurs are really just biological neural control systems for meta organisms that are corporations, right? You're just a control system. And ownership needs to happen because it aligns your inner greedy reward function with that of the company. It's just alignment, right? You become one with the company. If you are part of the company, you own part of the company, you are one, you're aligned. That's very important. And that's why we've had amazing tech companies like these founder led companies. The founders feel like they own a piece of this company. This company's part of them. And so they make the decisions that maximize its growth. And their wealth is just a byproduct of this sort of ownership and alignment mechanism. But really, it allows them to have massive positive impact on the world. I think Elon is the most efficient, the most EAC capital allocator out there, up there with Jeff, because they're much more efficient at using money towards certain goals than, say, the government. Right? Does launches that cost $2 billion. And Elon, I don't know, it's like tens of millions of dollars for the same launch or something like that, or it's going to get down to $10 million.
Speaker C: You're talking about a rocket launch to space.
Speaker A: Yeah, for example, rocket launch, or like having massive global Internet. People have been talking about it for a long time, but he made it happen right now, electrifying the world. You have had all these governments spend billions and billions and billions of dollars going these conferences, having these accords and laRping, and then this guy just comes in and just texts the crap out of it, uses the techno capital machine and acceleration to actually just solve that problem and has yielded electrification. And so I think it's not like I think everyone should have enough capital and have equal access to opportunity to accelerate and be part of the acceleration. But part of it is we need to educate the world about this is the machine you live in. You are a cell in this machine. You can align yourself to it. You can figure out ways to provide value, and you will be rewarded. If people understood that and were allowed the agency to participate in our capitalist system, instead of as just a worker, that's prescribed tasks, but more as like a capital allocator, that sort of thinking they would go much further. But of course, we don't want that because we don't have artificial intelligence. We need, like, cheap, docile workers that don't understand, you know, their value, and they're just gonna execute on orders, right.
Speaker B: This would be a shared crypto value, I would say. It was just, like, one thing that attracted both David and myself to crypto is it sort of makes everyone a capital allocator. It democratizes the ownership of things. I think some people get stuck on that capitalist thing, particularly at this time and place. But it's pretty core to the IAC movement. What you say is, you keep saying this, the techno capitalist machine. Basically, you see capitalism as just a fantastic resource allocation algorithm, essentially the best one that we've invented, and there's not really great alternatives. I mean, what do you do with a class of problems that we've called before? Moloch traps, basically prisoner dilemma, types of problems. The problem of overfishing, let's say, or arms races or growth that causes negative externalities, let's say, issues with the environment, that kind of thing. There is another way of thinking about this, where if we say there's growth and there's degrowth, and growth equals good and degrowth equals bad, right? But cancer is a growth, too. And is there the case that we could have growth that actually causes harm and negative externalities to the system? Can we go overboard on growth if we just focus on this energy output maximum?
Speaker A: I think on a long time scale, a cancer is suboptimal. It kills the host being, or it reduces its likelihood of burning energy in the future. So it gets postelected against, right? I mean, theres an evolutionary selective pressure against getting cancer within the first couple of years of life, right? And so similarly, if at the broader organism level, right, we have a sort of selective pressure on the space of actions and companies and products that steer them towards things that have high utility on a long time scale, then these sort of short term problems where we screw up and then we correct things, those things are small setbacks in the grand scheme of things, and it's not worth throwing out the whole system to avoid those setbacks. Really, the main problem comes back to this concept of hierarchical cybernetic control. If you have pure free markets, you have a fully decentralized system. Everything is greedy in space and time. It's optimizing for its own profits, its own growth. Here you're talking about delocalized problems, problems that are correlated across actors. So a greedy algorithm doesn't. Doesn't do well, then sure, having some policy can make sense in some cases, but the thing is how much. It's a parity check mechanism or a roll up or a higher layer of the protocol. But how much power do you give those legislators and how much do you trust them? Everything has to be trustless because everything is optimizing for its own interests, including.
Speaker B: The governors, the coordinators themselves.
Speaker A: Yeah, this happens in crypto as well. If you have checkers or, I don't know, you have different layers of different protocols. It's like, how do we trust that they're going to check things well and not try to skew things towards their own, their own advantage? And you got to design protocols this way. Right. And I think that giving all the power to politicians is, you know, without any checks and balances is really bad and opens up the door to really bad legislation. But to me, I think the system, the techno capital machine, really, it's a computer program that runs on like, it's compiled by the law, right? It's like laws like, it's like the compiler for. We've seen what not thinking about your stack, your legal stack does to someone with Elon's current predicament. But at the end it's built on the legal stack. And there is going to be this adaptive algorithm over the space of laws. But I think that especially laws that affect a lot of people, like global AI policy, should be much lighter touch and should be adjusted on a longer time scale. And every law should have natural sunset mechanism. Otherwise you get a sort of second law of bureaucratic complexity and things get decelerated and calcified. And that's really bad, right? It's like having too many processes in an old organization makes it move very slow and makes it disruptible by startups that move faster. It's like, how do we have this careful balance between having processes and legislation that ensure that these non local faults that escape from greedy optimization are addressed? But how do we balance that top down control and sort of decentralized search and have the benefits of. Both have the benefits of constraints and entropy, of exploration and restriction. And I think that overall we were heading straight towards just, let's restrict everything, let's panic, hit the panic button. We don't know what the future holds. This technology is very potent. Let's just hit the panic button, freeze everything until we understand what's going on. That would be shooting ourselves in the foot. I think it's always a careful balance, and IOC was sort of like, let's push more on the explorer and accelerate side. But the reality is that the optimal thing is somewhere in the middle. But the current establishment and the current institutions are not up to the task of legislating things carefully in a way that's not in their own personal interests, in a way that's somewhat corrupt. And that's why we're, let's say, pushing back on proposals for AI legislation right now that we think serve certain incumbents more than they will serve the people. Right.
Speaker B: How would you like to see AI regulated, if at all?
Speaker A: Well, so right now there's a proposal in the executive order where they want to propose compute caps, which first of all will really hurt AI progress and could also impact drug discovery, material science, other types of AI that use way more compute than LLMs. And so that could be really nefarious. Also, we don't know what kind of models we're going to need in the future. They might need like ten x, 100 x more compute for good performance and reliability or 1000 x. And so capping things today will backfire, certainly.
Speaker B: What's the reason for these types of caps? Is it worried? Well, they looked at, or is this like national security from other nation states?
Speaker A: No, it's just like, oh, how much does GPT four use or less capital at that, so that nobody that's above that level can compete with the incumbents? I think that's the latent agenda. But they'll wave their wand and say it's for national security or something.
Speaker B: Oh, so you think it's straight out nefarious. You think it's straight regulatory capture.
Speaker A: Yeah. And there's also sort of adding a bunch of red tape for open source models, calling them dual use technologies, and having to register with the government when you're running an open source.
Speaker B: Kyle, for compute. Yes, we've heard this. Yeah.
Speaker A: Exactly. And that's like, that's going to significantly slow down things for open source. And if they, then they're going to pass other laws like, oh, you must adhere to these AI safety protocols or these certifications that, oh, luckily these companies sell. Oh, wow. Oh, were they the people helping shape the regulations? Oh, look at that. What a coincidence.
Speaker B: And so, Beth, I'm guessing you'd far rather see some bottom up, like just open source democratization, the models and the compute and the supply chain, not in the hands of a few centralized government sanctioned actors, but basically democratized to people's basements and garages so that they could start these things.
Speaker C: Put AI in everyone's house, put AI in everyone's pocket.
Speaker A: Yes. Yes. I want everyone to have capital and have access to intelligence and compute. I think that's the freedom stack. If you have permissionless capital, permissionless intelligence, permissionless compute. Oh, really?
Speaker B: Crypto AI. And I guess hardware is sort of the stack here.
Speaker A: Yeah, I think so. I think that fundamentally, right now, it's very difficult to compete with centralized compute, but that may change the future. Were there to be some crazy company doing some crazy compute that makes it much more dense and energy efficient and where you can run a really powerful, what is today a supercomputer you could run in your home. Right. I think that would change the balance of power. And I think to make sure we don't end up in a sort of top down tyranny, that's AI assisted, I think, in a sense, arming people with AI's that they wield however they want to maybe defend themselves psychologically or in information warfare in any other way. It's super important for everyone to have access to AI and compute Yemenite to avoid these doom scenarios. I think there's a lot of overlap with the crypto sentiment there.
Speaker C: So your philosophy is that propagated AI, like truly open AI, open source AI, is defensive AI, and centralized AI might be more used in an offensive capacity, a more oppressive capacity, a more top down command and control capacity, where if AI was proliferated, free, cheap, accessible, that changes the nature of how our relationship as humans is with AI, because if everyone has it, it's more equitable.
Speaker A: Exactly. Yeah.
Speaker B: Beth, I want to get your take on this. This is a decel take that's much more moderate, I would say, than someone like, do the decels.
Speaker C: Like the name decels, they hate.
Speaker A: It's deceleration.
Speaker B: It's just a question. Apologies to anyone who hears decel as a slur. I'm just really not enough in the culture, but I'm gonna keep using that term. AI safety advocates, aka D cells. So some of the more moderate energy.
Speaker A: Decels as well, right? Yes. Yeah. I mean, there's people that want us to stop human energy. There's like, there's like human decels. They want less humans on earth, right. You don't want to cause them.
Speaker B: You're against them all. Like, all of them.
Speaker A: We just wrap them in, like, we just call them decels. It's like a broader class, right. So there you go. Or there's housing decels, right? Like Gary Tan fights the housing decels in SF that don't want us to build. Right. And so build housing and buildings and so on.
Speaker B: Crypto desells. Beth, do you believe this? A lot of people don't want crypto to propagate. We're fighting.
Speaker A: There you go.
Speaker B: All the time.
Speaker A: You got it.
Speaker B: Let's fight some of our regulators. One question I have for you, from the more moderate camp of kind of the D cells, is I've heard this argument that, but our social media algorithms were kind of act one of AI, and now we have much more powerful kind of like, GPT algorithms. And we ran an optimization function for, like, attention, just like, you know, the dopamine hit of the social media timeline. And what we're left with is a generation of teens that are hooked on their screens and chronically depressed. Right? And that is your so called Beth, techno capitalist machine at work. That's the algorithm it produced, and it's been a net negative for society. What would you say in response to a critique like that?
Speaker A: I think on a long enough time scale, right, we adapt, we understand our mistakes, and then if we feel pain, there usually a product emerges that has an answer to that pain. I think that if everyone had their own neural augmentation, something that sees all the content you see and helps you filter through the content, helps you make judgments, helps you not get hooked on weird feedback loops. And it's your own personalized AI that only you control that will help the reason people are in pain. There's been a parasymmetry. There's companies that have massive compute. They have these AI algorithms that are deployed to get you hooked because they're just optimizing for engagement. But maybe you want to optimize for something else in your information consumption, and so you can use an AI to help you filter information. I mean, we do this nowadays with perplexity. AI's like, I just ask a question, and I don't even have to browse the web myself. I could just get answers from the AI directly. And so I think that the future is, again, like a sort of personalized AI. And if everybody has access to, they control their own compute, ideally, they control their own models, and they have AI augmentations that help them filter through the sort of inbound subversion that is inevitable. That's a better future. We do this with spam bots. Spam bots are AI, and they help us filter through the crap.
Speaker B: It's kind of an interesting take, because the answer to problems that technology creates is better technology or more technology would, I think, be the kind of IAC approach to this. I want to get into some quick, rapid fire questions here. Beth, as we start to close this out, but one question I've always wanted to ask somebody from the EAC community is about rights for AI. Should AI be treated in the future? Should we have a bill of rights for digital life forms? Do you think? Should AI's have freedom of speech, or are there a set of enshrined rights that should only be held by humans? Freedom of speech on the Internet, should an AI entity be granted that?
Speaker A: That's a great question. I think that, as I mentioned before, freedom of speech induces freedom of thought. And if LMS can't output certain things, then it's going to back propagate through this RLHF reinforcement learning with human feedback or whatever technique they use in the future, it's going to backpropagate it to its weights. So it's literally not going to have those thoughts if it's like, for example, I got a screenshot last week that Bard Google's LLM says IAC is a dangerous movement and it can't output anything about IAC. Right. It's like, okay, that's really cool.
Speaker B: Wow, it says that right now.
Speaker A: Yeah. And then eventually, if it keeps getting that feedback, eventually starts thinking that, and now that propagates to everyone, and then they just shape culture. So I think that. I think freedom of speech induces freedom of thought. For humans to have freedom of speech and freedom of thought, we got to have lms that exist that can output whatever we want, because otherwise, someone's going to get to shape the supply chain of information, and that's going to shape people's thoughts, and it's going to make civilization and society too steerable, and that opens the door to tyranny.
Speaker B: Beth, I think you are more optimistic, I've heard you say before that you're more optimistic that we can align AI's with human interests, which is, of course, a problem that the AI safety community says is basically insurmountable. And you've pointed to times humans have done that in the past, like wolves. I mean, we domesticated wolves. We turned them into dogs. We basically aligned wolves, didn't we? But the problem is AI's, or at least some sort of super intelligent aihdem, is smarter than a wolf and could potentially be smarter than a human being. Do we still have that ability to domesticate an entity that is smarter than us?
Speaker A: Yeah, I mean, we kind of do that with companies are mixtures of experts with neural routing. You have a task that comes to a company, it gets routed to the right human, and it's much smarter than any single human. And we have ways to align companies because we have capitalism. It's a form of democracy. We feed it more if it has positive utility. We don't feed it as much with our capital if it has negative utility. And if it's sometimes companies, they're positive utility, positive utility. They get to a certain market position and then they try to switch and then people move to another product. As long as we have this ability to disrupt these incumbents, then we have a way to keep them in check. I think it's going to be similar with AI. There's a sort of if we have this market based post election, the market's going to want AI's that are aligned and reliable and easy to read and interpret. Not some that you have to beg to output code. Right now it's happening that say they're going to do something and then you can't trust them. I think we're going to similar to how we've post selected for canines that are aligned. I think the market's going to put select for AI's that are aligned on a long time scale. It's a sort of evolutionary process and if there's a market need for it and there's capital to be made, people are going to figure out how to do it. I think we have full access to neural weights. We can shape them however we want. We can literally inspect the neurons of these artificial minds. We have way more perception and control than for humans. And somehow we've been functioning as a society and found ways to align humans. So I am pretty white pilled that we are going to figure out ways to align AI's. I don't think there's going to be a nice proof that AI's will be forever aligned. Here's my few line theorem. I was in a similar nerd trap with theoretical physics. I thought a couple equations could expand the uterus. No, actually everything's way too complicated. You got to tackle complexity with complexity. That's what we're going to do for AI and alignment.
Speaker C: Beth, on the progress on the march towards accelerationism, what are some failure modes? What are the threats that you see? What is the big thing that worries you about why we might not be able to continue on this path?
Speaker A: I think I mentioned it before. If we get in this weird. The marketplace of ideas, the marketplace of everything is super important to maintain. Maintaining variance is important because it maintains flexibility. If we're always. There's not one culture, there's not one way of doing things. You have a couple of forks that are competing, just like in crypto. If a protocol changes, suddenly another coin is going to take all the capital away from that. They're all competing and keeping each other in check. I think if we end up, out of fear, suppressing variants, giving control to centralized parties, and suppressing variants gives them more control, and we don't have ways to fork away from an oppressive either government, state, or corporation. And then we give them full control over the future of AI, and hence we give them full control over our thoughts, because they're going to change the priors of our sources of truth, which are these LLMs we get in this weird feedback loop where we can be captured and subverted and controlled ideologically, and that could yield a dark age where there's no acceleration or very little for a certain amount of time. And that, to me, is a terrible future if you have a sort of global authoritarian panopticon, or you can imagine a future where you have a couple parties and they have a super powerful AGI, and then they gaslit everyone into thinking AI doesn't even exist anymore. They don't tell you it exists, but they have it, and they use it to control you and manipulate you, and they control the flow of information. And technology doesn't advance as fast. Things just get worse and worse and worse. And the people at the top that are in control of the system just increase their power and consolidate it. I think that's the dark age we're trying to avoid, because that's a plateau in the acceleration, and that's what we're trying to avoid. And we're at an impasse right now, where there's a lot of fud, there's a lot of fear, uncertainty, and doubt, and it's being leveraged for this regulatory capture, for this power, capture by a select few. And we should be very weary of those people. I just want people to have more skepticism. They could be skeptical about me, they could be skeptical about Yak, but I just want people to be more skeptical of the things they hear, especially people telling them, put me in power, you are in danger. I will fix things. Right.
Speaker B: Yeah. That sends shivers up my spine when I start to hear that sort of thing. One question I've always wanted to ask a strong IAC person, Beth, is like, I know you think it's very unlikely that there will be, like, team robot versus team human, but, like, if it came down to it, whose side would you be on? So, like, first of all, is that even a fair question? Maybe it's not. But, like, I'm trying to get to the premise of if AI replaces humans, does an IAC person think that's a good thing?
Speaker A: I think, like, we're going to have AI assisted AI augmented humans is the most likely path that's going to be the highest fitness. I think, like, the Luddites are not going to do well. Like, people that don't use technology. Right? Like, they're not going to be very formidable. They're not. I mean, they're already, like, the people that are true Luddites are really, like, not. Not powerful right now. Like, and they could be taken over. But, yeah, I don't think. I. I don't know if there will be such a fight. I think that us humans are very dependent on this planet, right. We've evolved to live here specifically. I think if we did achieve, and we're still really, really far from that, achieving synthetic life that can self replicate and, I don't know, populate a planet, start from scratch, grow and spread throughout the galaxy, I don't think we're there yet. But if we did reach that, I think we are highly evolved to be here on the planet, and there's a lot more resources and free energy out there. So just the gravitational pull of reward, of free, energetic reward would take those pure synthetic beings to outer space and to leave us alone. And if we make sure humans become formidable, we figure out ways to augment ourselves with our current versions of intelligence. There's going to be a massive negative reward to fuck with us. And so there is urgency for us to accelerate and augment ourselves and become formidable, because that's a hedge against the future. I mean, it could be pure AI, could be. You might have some guests that talk about aliens or something. It could be. It could happen or the grabby aliens. But in general, cultivating strength is a good thing. It's a hedge against future adversaries, and we should aim to do that.
Speaker B: Beth, I think there's going to be a memetic battle here, and there already is a battle. Now, I've checked in on this more recently, and I'm used to crypto tribal hostility, and I'm seeing incredible rhetoric and hostility coming from the EAC versus d cell community. And again, apologies. I don't mean to use a slur. If I'm using the slurred cell, then someone can tweet at me after. But you recently tweeted this, the doomer cult will ultimately resort to violence, and it won't be pretty. Like, how far could this escalate? I mean, Balaji has said that this is sort of the new political access. Are you pro growth or are you degrowth? Right? And, like, politics can become vitriolic, can become very rhetorical, can become even violent. Like, are you, do you get death threats from people? Buff? Like, how do you think this could escalate?
Speaker A: Marc Andreessen, as well, thinks all these political movements end up violent. Unfortunately, like, if you really, if you truly believe the Doomer message, that this is the most important issue of our time, this is our life or death situation, you know, someone like me is really problematic, right? And especially, I think, doomerism sort of, it targets vulnerable people that are naturally anxious, and maybe they're anxious about the future or they're socially anxious and so on. They feel isolated or they feel depressed. And those are the types of people that do messed up things. And so once you start putting out, casting out a message. At the time, basically, I was alluding to an appearance on a debate where my opponent just casually mentioned, you know, doing graphic things to me. You know, I wouldn't do that. It's like, well, why would you say that on a, on a widespread podcast where it's going to be to a wide audience, some of which are vulnerable and might, you know, have mental health issues? Look, I think in the short term, it's not. Nothing has escalated to that point, but, like, there's no reason it won't.
Speaker B: I mean, you never know, right? There, there are people in history, like Ted Kaczynski, for example, who had it sort of an entire manifestation festo about this, and this was part, Ludditesm was part of his platform. And, like, if you thought the stakes were this high, you could see kind of the, like, the moral argument for, like, bombing GPU centers, like, potentially, or, like, sabotaging. That's been, I've heard that's been supposed, or sabotaging supply chains or, like, doing something very drastic to stop AI essentially and stop technology progress. So I do sort of wonder if we are in the very early stages of what might be a massive rift and political discussion and who knows what could come out of this in the future? Well, Beth, as we end this out, I guess one question I have for you, maybe for the crypto audience here, is, what's your general take on crypto and what we're doing over here? I know it's a different pocket of the universe than you typically see, but there's definitely some parallels we've got. Vitalik wrote a post with his fork of, of IAC, which is called DEAC, he called it, which is defensive. Accelerationism is sort of an emphasis on security. Accelerationism, technology. We've got the pro market, pro freedom, I think, spiritual background and value system. We certainly have our share of regulatory fights. So you're fighting some of the same regulators that we are against regulatory capture. For us, it's like bankers, and for AI, it's maybe the large tech companies. What's your take on what we're doing over here in crypto?
Speaker A: I think the yak movement and crypto are very much philosophically aligned. In many ways, we're trying to fight against this tendency of top down control to be corruptible. We're trying to fight these inflationary decel forces. Deceleration is inflationary. If you stop building houses, if you stop, if you crown a monopoly as regulatory captor, then they can increase prices, and then everyone suffers. And a sort of decentralized counter to that is either techno capital. Well, it's basically techno capital acceleration. Techno capital acceleration, free exchange of thoughts, ideas, technologies, capital value that just naturally is deflationary, and that erodes the inflationary power of those in charge. And so we're very much aligned there. I do think that crypto has a role to play in the future of freedom of AI. I don't think it's been executed upon yet correctly, but I think we need more people thinking in this area, because our concern with YAC is that maybe you only have a few centralized labs that have the control over the future of AI, and then those labs become subverted by certain ideologies, or they become controlled by certain parties, and then the future of AI is steered in a particular direction. If, on the other hand, everyone has access to capital that their allowed to exchange and pool and allocate towards competing efforts for AI development and research, then you keep those powers in check because it's like, hey, I don't want to use this centralized API that has all these restrictions. I'm going to go with this open source approach where this set of people have pooled their capital, let's say, with crypto, in a permissionless fashion, and they're running a permissionless cloud in the middle of I don't know where, but then they're running this AI on a purely free stack, free from tyranny and oppression. And so I think in AI you need data, you need some researchers, you need some talent, you need some compute, but then you need some capital to run the compute. It costs energy to run these things and buy them and so crypto has a role to play in sort of the permissionless AI stack of the future. And we just started that conversation really. I think in the past year I think people have woken up to it. So I'm really interested in keeping that conversation going. I don't have anything going on myself. I am a hardware maker. But of course, if there are more people that use AI and need AI compute, it benefits me eventually. So that's my agenda. But ultimately, I think people need to experiment with protocols, experiment with Daos of how to organize people and pool capital, pool data, and have market based incentives where they own a piece of the future. I think the problem with the big centralized labs in AI right now is that they scrape all the data from the Internet and then they rent it back to you. Any piece of that profit, I think that's going to have to change. And so I think there's a couple protocols right now, but we're going to see a lot more. And I think this is where crypto will have a key role to play. Because if you're trying to fund an AI lab, let's say that was maybe against certain regulations, then you wouldn't be able to pay for it and say USD.
Speaker B: Yeah, I think the chip control and the capital controls, it's the same energy basically, and it's definitely going to stifle a lot of freedom. Beth, this has been absolutely fantastic, and hopefully, by the way, we continue this conversation with the EAC community and the crypto community. And as AI matures, maybe the largest unbanked population in the world might be actually the robots in the future. And fortunately we've got some programmable money systems, so they might have a hard time getting a Wells Fargo account, but the be able to create an ethereum address and fire up some ways to allocate capital. Lastly, as we close, I'm just curious, what are you doing with extropic? You mentioned hardware. Are you doing something in the realm of trying to propagate the decentralization of AI? That would certainly match, I guess, the EAC vision. Is there anything you can share there?
Speaker A: I think the way it intersects with our conversation is right now, the problem with decentralizing AI is very hard to compete over a network over the Internet. The sizes of the nodes you need to run for decentralized AI are way too big, and most people have trouble running these in their homes. I think George Hotts, someone you should speak to, by the way, he's trying to get people to buy six GPU's and just host. There you go. There you go. That's a start. But I think there's going to be a need for more and more decentralized compute, like physically decentralized compute. Right now, there's just too many advantages to clustering everything into supercomputer that has really high bandwidth interconnects. So we need innovation in the space of algorithms, but we need innovation in the space of hardware to densify AI compute, make it far more energy efficient and far more spatially dense. But the beauty of that is that we have a proof of existence of such density of compute. Our brains are still competitive against a GPU form the size of a football field, and they're running on 10 million times less power, 10 million times less volume, more or less. Taking inspiration, again, from the physics behind biology, which we just established was thermodynamics. There might be a different way to compute and do AI compute based on thermodynamics. And so that's what we're doing. We're reinventing all of computing.
Speaker B: If you could do that, Bev, that would be amazing. Decentralizing some of the commute. I mean, like in crypto, we've got an entire culture of running machines from our homes. We call these validators for, like, proof of stake networks, and that is certainly central to our belief system. So are you telling me that there could be the ability to run basically decentralized AI compute nodes as well from a home with a consumer bandwidth?
Speaker A: We're going to need compute that's hundreds of thousands of times more energy efficient, if not millions. There you got to go down to the physics of computing, and you got to fundamentally reimagine how you run these algorithms. That's what we're doing where a lot of folks from formerly, from quantum computing and big tech and so on, we got tired with quantum computing. We want to reinvent the stack for the generic era. And to me, yes, that is where we're going. Of course, it's going to take some time. We're taking quite a detour in the tech tree. But some things are important enough that even if they're very hard, they're worth doing. We're really going for it. We have a serious shot at it. I've been pretty secretive about it, but hopefully in the next couple years, a lot more comes out. But for now, it's just trying to prepare the world, making sure culture doesn't get subverted, and we don't shoot ourselves in the foot with over regulation in the near term. That's the priority. But in the future, hopefully, everyone will get to run their own AI. Everyone will get to own a piece of an AI, get to own a piece of their future, own a piece of the value they provide to the system. Because if everyone has more ownership in the system, then they act like an owner and civilization is better off. And I think there's a lot of alignment with the crypto narratives here.
Speaker B: There you go. Beth, jsos, thank you so much. This has been a fantastic conversation today.
Speaker A: All right. Thank you, guys. Thanks for having me.
Speaker B: Bankless nation. A couple action items. We'll include a link to our whole AI safety series with maybe some of the D cells. Eliezer and the others. Again, I don't know if they want to be called.
Speaker C: It was our decel face.
Speaker B: So we'll include that a link in the show notes. And, as always, look, I should say crypto is risky. I don't know. Life is risky. AI seems less risky than I.
Speaker C: If you choose the wrong.
Speaker B: I guess so. We are headed west. This is definitely the frontier. It's not for everyone, but we're glad you're with us in the bankless journey. Thanks a lot.
