Speaker A: Let's keep all the blockchain wars. L one, l two, l three aside. Just let this sink in. It's insane. It's amazing. It's unusual. It's like we're becoming this much, much more coordinated. In fact, as a species, I think our evolutionary advantage is that we are able to cooperate at a scale that is simply not possible for other species in a flexible way.
Speaker B: Welcome to bankless, where we explore the frontier of Eigen. Laird. Eigen Lair is just about two quarters away from Mainnet. And the excitement and demand for Eigen Lair has been relentlessly crescendoing. And while recording this very episode with Sriram and Teddy from the Eigen Lair team, Eigen Lair passed a billion dollars in deposited value into the Eigen lair system, making the future of Eigen Lair in 2024 a very interesting topic to explore here on the show today. And to help me explore a more technical topic, I brought in a technical co host. Mike Norder from the Ethereum foundation is joining me today. He's a researcher at the EF. He is a milady on Twitter, and he's most known on bankless as my rock climbing buddy in Brooklyn. Mike. How's it going, my dude?
Speaker C: It's going great, David. Yeah, thanks for having me on. I have been a bankless listener for a long time, so to be here hosting with you is a real treat. So thanks again.
Speaker B: Well, whenever we need technical co hosts to explore technical topics, I always learn more than a few things. And that's definitely what happened here on the episode today. We just finished recording the episode with Sriram and Teddy. What were your big takeaways? Did you get all of your questions answered? What did you think?
Speaker C: Yeah, I'd say my biggest takeaway was kind of a new mental model for thinking about how Eigen layer fits into the system. And that is as a way of democratizing access to restaked rewards. So the point Sriram made was that in today's world, like, without Eigen layer, a dominant liquid staking token issuer could have internalized all of that restaking yield and given it only to people who issued liquid staking tokens with them. And this would be like a stronger centralizing force, because only that single pool would have access to those rewards. So the way Eigen layer kind of fits into this picture is by creating an open, permissionless marketplace for both the buyers of economic security and the sellers of economic security. And to kind of allow everyone to access it in a more transparent way, hopefully will help democratize and distribute those rewards more evenly.
Speaker B: I think perhaps said another way, I think that description, I think really fits into what the EF people care about, the Ethereum foundation people care about, which is anti trust, antitrust forces around protocols. I think that's kind of what you're alluding to with Eigen layer is that without Eigen layer, there might be a monopoly in a single liquid staking token becoming the dominant restaking token. But I think maybe your mental model after this is Eigen layer is kind of the resource traffic controller for restaked assets and networks and yield and security. Is that a fair way to articulate this?
Speaker C: Yeah, exactly.
Speaker D: Yeah.
Speaker C: A way of kind of opening up the market and making sure that it doesn't centralize around one single shelling point. Yeah, and also another really cool point just to kind of add onto that is how he described Eigen layer as a way of kind of propagating the meme of ETH as a unit of account. Right. So the initial set of tokens that can be restaked are all denominated in ETH. And so this Eigen layer is kind of a vehicle by which ETH as the unit of account for economic security in the whole ecosystem continues to be spread. Was another really cool mental model that he brought up.
Speaker B: Yeah. That facet specifically, I think I resonate with on a very large degree, and I think that's going to be a big theme in 2024. So, Mike, we're going to have more restaking content throughout the year. I think it's nerdsnipe to me. I think it's nerdsnipe to you. I think it's nerdsnipe to a lot of people in Ethereum. What questions do you have left? What is still on the frontier of this restaking meta that you want to explore?
Speaker C: Yeah, I think the last thing that still sticks with me, and this is kind of one of the first things we talk about with Sriram, is this idea of what is economic security in the context of delegation. When there's the principal agent problem, where the principal is the person who owns the stake and is restaking it and the agent is the node operator. How can we think about economic security when the slashing is associated with the node operator, not the person who actually owns the capital that's at risk? So, yeah, I want to keep deep diving on that. And as Sriram mentioned, that applies beyond just Eigen layer. That applies in eTh, delegated staking and across the board.
Speaker B: I think the principal agent problem is one of the main problems that plagues not just crypto, although definitely crypto, but really humanity at large. And now we are also discovering it inside of the Eigen layer system. Guys, we're going to get right into the episode with Sriram and Teddy from the Eigen Labs team. But first, quick disclaimer. Me and Ryan are both advisors to Eigen Layer. All bankless disclosures are available@bankless.com. disclosures. And with that, let's get into the episode Bankless Nation. I'm excited to introduce you to Teddy Knox, a research engineer over at Eigen Lair, working on the Eigen DA team. That's data availability. Previously, Teddy was working inside of the Cosmos ecosystem and later as a protocol specialist over at Steve Steakfish, and has joined Eigen Lair, bringing all of his previous expertise into the world of restaking. And with Eigen DA is as the first AV's developed into in house by Eigen Lair. Teddy skills being put to the test. Teddy, welcome to bankless.
Speaker D: Thanks for having me, David.
Speaker B: And returning to Bankless, we have Sriram Kannon, the father of modern restaking. Sriram was a professor over the University of Washington, where he ran a lab focusing on information theory and his applications in communication networks, machine learning, and blockchain systems. But eventually, the nerd snipe of crypto economics got him like the got like it got the rest of us. And he started Eigen layer in 2021 in order to open up a new dimension of trust networks built on Ethereum. Sriram, welcome back to bankless.
Speaker A: So we're excited to be here. David.
Speaker B: Guys, I'm really excited for this conversation. The excitement around Eigen layer has definitely been heating up, and there's been a lot of things happening inside of the Eigen layer ecosystem. And so today on the show, I kind of just want to get a download as to where things are and where things are going with the world of Eigen layer as it approaches real time like production in house, the main net, all the cool things that is going to impact ethereum and all the trust is going to bring. So I kind of want to start just getting a high level snapshot of where we are with Eigen layer. Sriram, I'll start with you. Just the current state of Eigen layer development. Where are we on the roadmap? What is in the near term roadmap, and what are people over on the Eigen layer side of things excited about?
Speaker A: Yeah, the few things. Number one, on the mainnet, we launched the protocol, just a staking site on Mainnet around June, July. And we started conservatively, it was a goddard launch with a small TVL cap, and we've been successively raising that over time as we test the stability of the protocol. And so there was a cap rise a day before yesterday of this recording. And I think we are now at $1 billion Tvl for restaking. So that is on the main net. The broader ecosystem of Eigen layer comprises stakers, node operators, people building new services, and our own service called Eigen Da. And all of these are live on our public testnet where, you know, stakers have staked and delegated to node operators. Either the node operator can be themselves or they can delegate to a third party node operator. We have a bunch of really strong node operators from the blockchain ecosystem, blockchain and Coinbase cloud, Google cloud, peer to peer figment. All the major operators on our testnet on the, and also our service Eigen DA is live on the Testnet. So this is a data availability service which is intended to expand the data bandwidth available for Ethereum rollups and layer tools. And then finally anybody can build and deploy actively validated services which are basically, you can think of them as Eigen apps, like applications. But these applications are not necessarily consumer facing. These applications will be used by consumer facing applications. These could be oracles, data availability, bridging, finalization services, all these kinds of things. So that's where we are on the ecosystem, so that the testnet is public and live. We are going to this to the main net, this exact same configuration to the main net between q one and q two, depending on audit and hardening. So very excited to have the full ecosystem kind of get together to start up more open innovation.
Speaker C: Yeah, congrats on the recent raise of the amount that you're allowed to restake. So just to add some color here for the listener, there's about 447,000 ether denominated restaked tokens. So yeah, that's almost exactly $1 billion and about 200,000 of that. So nearly half of that is with steeth. So I was just kind of curious how you choose the different limits for the different liquid staking tokens that you allow people to restake. And also a follow up question, why did you choose only ETH denominated tokens? Have you thought at all about people restaking USDC or other tokens? Because generally speaking, it's just the value of the token, more so than the fact that it's ETH denominated that adds value to the system.
Speaker A: Yeah, absolutely. Thanks, Mike. Also excited to have this conversation with the mic here, why are we choosing this particular set of tokens? Why denominator? How do we choose the caps? All kind of complex questions, but the first thing is we chose a guarded launch so that we can test the protocol at various levels of TVL and safety. That's the first thing we chose the liquid staking protocols to have a cap, whereas native staking does not have a cap. So native staking is uncapped. This is because native staking is already very complex to actually go and execute, because you have to go, and when you stake in the beacon chain, you have to set the withdrawal credentials to the eigenpon. And furthermore, any lags. So the withdrawal lags exist on the Eigen layer ecosystem, on the Eigen layer platform. So whenever you want to withdraw any unit of ETH or any other token from the Eigen layer staking platform, you can actually, it takes seven days before you can withdraw it. This withdrawal lag is there so that if, when you're staked and providing services to operators, if there's anything that malicious that you've done, you can be slashed within this period. So it's standard in all kinds of staking protocols, but it also acts as a measure of safety for us, because actions do not happen instantaneously, like if you're doing on a bridge, who knows, somebody can drain a pool's TVL, like instantly, whereas staking is a necessarily long term activity. So having this kind of like a one week withdrawal gives us a measure of safety that simply other protocols may not be able to achieve. Just because the timescale of staking is fundamentally very different from the timescale of other kinds of financial activities. But adding on to this is when you have native restaking, you have the additional lags on Ethereum itself, right? Because you have to go and like, withdraw from the beacon chain becomes more noticeable. So all of this means as far as the safety limits are concerned, we can be more aggressive on the native restaking than we can be on liquid restaking. So that's why the native staking is uncapped. And we have to decide on to sum cap for all of these different services. And we just chose these numbers based on both market representation that we do know that some lsts are more dominant than the others. So we don't want to say that they're all very low, but we want to also have representation of multiple different liquid staking tokens in the platform. So that's why we did that. Regarding your question, why restrict to lsts you can think of the question's premise is absolutely right. Eigen layer, even though we popularly call it a restaking platform, and that's a narrative, the fundamental thing is it is a permissionless programmable staking platform. It's staking. You stake your ETH, you could stake your USD, you could stake a bond, you could stake whatever you want. It is programmable. So anybody can come and program it to like what the staking conditions are. And it's permissionlessly programmable. It's not programmed by us or anybody we know. Anybody can come and create these slashing, staking and slashing conditions. So yes, the premise is absolutely right, that Eigen layer can incorporate all kinds of tokens. But the reason we focus on the ETH and ETH related tokens to begin with is that we think, number one, clearly there is a big market opportunity there. That is a lot of the lsts as well as native staking is locked in. And when you're promising to validate Ethereum, you can might as well promise to validate some of these other networks. But more broadly, I think we are also trying to support a lot of the services for the Ethereum ecosystem. And when your risks are denominated in ETH, it is much better for your underwriting economic safety mechanism to also be denominated in ETH. Imagine I'm doing like 100,000 ETH transaction between one roll up and another roll up, and you want to say like, hey, I have enough economic safety out of like Eigen layer to do this transaction. Now, if I know that I have lsts worth maybe 120,000 ETH backing this claim, that's actually like a much more rigid, you know, mapping than to say, oh, I have 100,000 ETH, but I have like some x dollars USD backing it, because now I have to account for the volatility and slippage between these two different tokens over the period of the collateral and unwrapping. Add to this the capital efficiency of lsts because LST is already earning certain amount of reward. We found that this is the best configuration to stop this platform offer.
Speaker B: Sriram, is this just an articulation that the ether unit of account has network effects, and so it's just easier to use that unit of account because the risk is denominated in ETH. The collateral is denominated in ETH. In these networks, people tend to think in ETH. And so while it doesn't necessarily need to be eth, it just kind of makes sense to be eth. Is that just a fair summary that is absolutely right.
Speaker A: And this is what we want to incentivize the most. And so, you know, the idea being that initially, so over time, we are going to completely make this permissionless. Like anybody can listen any token and each AV's can decide how to relatively value these tokens. Somebody may not like to use USD, they may only want to use certain lsts, some people may want to use any of them as long as they have enough economic value. So this is up to the services. So we want to get out of like this layer of saying, hey, you can only do this or that, but we just have to steward this platform in the beginning. To add to one of David's point, I think when people think of the network effects of Ethan, I think this is a new dimension of network effect of ETH, which is that when you are transacting and denominating in ETH in the system, that means the right backing collateral for economic safety and validation is also ETH this creates. So this is a network effect between the monetary premium of ETH, which is that this is used as a unit of denomination to the utility of ETH, which is, it is actually used as the backing system for economic safety. I think this is a new, I would say, emergent effect that Eigen layer brings to this market. So that strengthens actually the dominant position of Ethiopia. Cool.
Speaker C: Yeah. And just to kind of double click on this economic safety, economic security point, I think we might have talked about this offline. But just so to kind of bring it into this conversation, I guess one thing that always feels a little weird about the meme to me is the fact that the economic security denomination is in ETH, and the owners of that ETH aren't necessarily the ones running the services that could be slashed. So this is the classic principal agent problem. It shows up in Ethereum staking too, I guess. How do you think about economic security when the, the people who are at risk of being slashed aren't actually the ones doing the task of the AV's operation, they're the ones who the capital was delegated to, but they're not actually the owners of the capital itself.
Speaker A: This is a great question, and I think maybe one of the most important for our entire field to actually consider and understand. So I wouldn't claim to have simple answers to this question. So to rephrase this question, the idea is economic safety is coming because somebody is putting down their stake and then running the node operations, let's say themselves, and saying that, hey, if I don't run these operations correctly, then I'm willing to lose my ETH. So the first point I want to bring here is that if the staker and operator are the same person, this is a very unusual type of risk. I call this endogenous risk. Endogenous risk means, unlike going and putting your ETH into a lending platform with ten x margin position or 100 x leverage, where you're underwriting certain kinds of price volatility risk, that's what you're doing. When you're doing that, when you're staking in the Eigen layer, platform and eigen layer is constrained to validation tasks, you are underwriting endogenous risk. Endogenous means something that you do yourself. You can control yourself, you not being malicious, and if the protocol is correct, you will not get slashed. It's very different. This is why the usual mental model of people thinking of, oh, this is leverage. Leverage is not quite accurate because you know it is endogenous. Whereas all other forms of risks, you know, that people are used to, when you think of rehypothecating steak, or like rehypothecating your house or any of these, are subject to exogenous, priceless, okay, that's number one. But the risk is purely endogenous only if the staker and operator are the same. Like, that's what Mike's alluding to here, and it's absolutely true. The staker and operator have to be same, or in our view, to be inside the same trust zone. So the staker has to trust the operator that the operator will do right by them. The fact that the staker and operator are not necessarily the same means now they have to establish some other mechanism of trust between themselves to actually make sure that I will delegate to somebody while putting my eth at risk. So these mechanisms can be manifold, and one mechanism is social or legal. Oh, there are major operators, and they are legally regulated, and they're not going to go and do like something which is provably malicious. When we think of all the kinds of this is, I think, very important, and people in crypto don't fully appreciate it. I think that among the set of ways in which a company or a system can cheat, they usually choose to cheat in ways that are not observable, because observable means you're liable. And what these systems do is make it completely transparent because there is a slashing condition. There is an observation that you actually double sign this block, or whatever the set of things are. So it makes it perfectly naked that you're cheating. Like, this doesn't happen very often. I think this is something when people think about, oh, you know, all these Wall street guys, they do this and that and all that. Nobody goes and like, does something where it's perfectly universally observable that they are actually cheating. Like, this is very important. So what the principle, so how to solve the principle agent problem? The real world mechanisms are, hey, I am in a certain jurisdiction, I trust certain other like, entities outside my blockchain protocol, and I am therefore going to delegate to them. This might be one mechanism. Another mechanism is they use technological substrates to actually minimize the principal agent problem. For example, we are working with this platform called a project called Cubist to build anti slashers. Anti slasher is this idea that, hey, there is a piece of code that simulates the slashing conditions and then makes sure that when I'm issuing a signature, the slashing conditions will not be violated. And this piece of code alone runs inside a trusted execution environment, like an Intel SGX or an AMD trust zone. So what this does is it gives a sense of correctness between the principal and the agent, because even if the agent wants to manipulate it, they're still running it inside the tee. So therefore they cannot really cheat the principal. And in our platform, we have a protocol called buffer, which is based on trusted execution environment. And they are actually doing liquid staking for Ethereum itself and also restaking based on these tes. These are two different ways, legal, social, and number two is technical. There's also like a third way, which is economic, which is the rocket pool way, which is saying, hey, yeah, you know, the principal and agent are, the agent's gonna, the principal's gonna lose something, but the agent's gonna lose something too. So like, you just try to correlate the fates of these two people. But in our, like, fundamental analysis of the economics, this really only works if the slashing is bounded or bounded for some reason or the other. And on Eigen layer being a fundamentally economic safety platform, it's not clear, like, what will be these bonds. So that's the three different ways, social, technical, and economic, to minimize these kinds of principal agent risks. And I think this is a generic question, not for eigen layer, but for the entire field to actually answer.
Speaker C: Yeah, for sure. And just kind of one more high level question before we dig into some more of the details of Eigen DA and stuff. Yeah. One thing I think that comes into question when thinking about restaking is that it does fundamentally change the incentives of being a staker in Ethereum, if you think of the protocol as having two incentives, now it has the consensus layer rewards, and then the execution layer rewards. Consensus is for participating in voting on blocks. What's the head of the chain? Execution rewards are these congestion fees, like gas fees, and also the MEV rewards given to proposers eigenlayer kind of tacks on a third set of rewards. These are restaking rewards. So the main issue I see potentially with this is that these rewards are outside the purview of what the protocol can see and what the protocol is designed for. So if this kind of warps the incentives of the protocol, it might, for example, increase the demand for staked eTH significantly. Or also it might make it so that solo staking, kind of the opportunity cost of solo staking is very high because restaking yields are bigger than the other two components of the reward. And so in order to be competitive as a staker, you also need to be a restaker. So these are big kind of themes that I've been thinking about, but would be curious to hear your high level response on these before we dive deeper.
Speaker A: Yeah, absolutely. I think also complex question and landscape to think about and filled with second order effects which are not totally anticipatable. But I'll start with one thing. This is the hard thing about building permissionless platforms. Who knows what somebody else can do when Ethereum is building in? The MEV was one example, liquid staking is another example, restaking is another example where these are emergent effects that, you know, could not be anticipated fully. So, having said that, I want to make a bunch of observations. So the first observation is that anything you could do with restaking, you can already do with liquid staking. Right? One major LST, the dominant LST, could just simply say, hey, you know, the economics are simply not only being used for ethereum staking, but I'm also making this promise as the dominant LST protocol, that ABCD will happen. And this leads to a completely different set of effects, which is that LST, because it has figured out that it can do ABCD, now completely consolidates the market because it is able to tack on additional things. This is exactly the kind of the problem that Mevboost was trying to solve, which is that if you're a major player, you can do auto protocol deals, and if you're a smaller player, you cannot do auto protocol deals, and you're completely subverted by an auto protocol deal. So just like MeV Boost and the PBS roadmap basically tries to democratize the opportunity for making these auto protocol deals. Eigen layer is an opportunity to democratize these out of protocol deals and make it as formal, transparent, clear, and verifiable as possible so that anybody can enter into these kinds of agreements, not only the dominant player. So that's the first thing. Anything that you could do with restaking could have already been done with lsts. The second thing, I think, in order to affect Ethereum's protocol economics. When I hear some of the concerns about Eigen layer and restaking, makes me wonder in one sense, because these people are much more bullish about Eigenvector than I am, because they're basically saying the statement, once I formalize it, we'll make it clear. They're saying that the total amount of reward and yield that will come out of restaking should be higher or of exactly the same magnitude of all the defi yield that would come out of any kind of LST and other things. So it's only at that scale that this starts to become significant. But having said that, maybe it can happen, and we are of course, believers in the technology. That's why we're building it. But how does it affect Ethereum's protocol economics? It does definitely warp the incentives, but it warps it lesser than if Eigen layer wouldn't exist. And one LST basically significantly integrates this kind of an idea inside of its own protocol. I think people don't see it, like a lot of people on Twitter, for example, saying, why doesn't Eigen layer commit to self limiting or whatever ideas? And I think it is the same reason why mev boost is a neutral platform, the same reason why PBS has to be neutral. There has to be a mechanism for new protocols to be built to be completely neutral so that the, the playing field is level. Because if we self limit the dominant lsts, what are they going to do? They're going to say, hey, I have to internalize this because these guys are going to self limit. So there are all these second order games that people don't transparently understand, but these are, you know, and I'm not claiming to have all the answers for the second order games, but at the minimum, the observation is that the presence of a more neutral platform democratizes restaking yield, rather than centralizing restaking yield into only the ls. Now at least, like if I'm a home staker, I can opt into Eigen layer and then adopt at least a few of the protocols which are lightweight and easy to run, and participate in that additional rewards, whereas in the absence of Eigen layer, that would just simply not be possible. So that's number two. Number three, we know and hope that the number of such protocols is high, but we know that there are some protocols which fundamentally rely on decentralization, rather than relying purely on economics. An Eigen layer is a highly expressive platform because it has this feature we call double opt in. Double opt in means a staker and operator have to opt into the protocol, and the protocol has to accept the opt in. So double opt in basically means protocols can express subjective opinions on who can opt into their protocol, into an AV's, as well as give additional rewards to certain people than to other people. So because Eigen layer is this highly expressive platform, and there are services which fundamentally rely on decentralization, rather than fundamentally relying on economic safety, those services could actually incentivize decentralization itself. Like, for example, one of the services building on top of us is this thing called Witness chain, which offers a proof of location protocol, basically offers a geographic location oracle, which itself is geographically decentralized. It uses like stakers and then tries to measure network latencies across various nodes to certify that, hey, you are in this zone or that zone. Now, it's possible for an EV's to say, I want to add a geographic decentralization bonus to my reward structure. And home stakers, being more geographically distributed, could potentially take part in that. Other people can offer other kinds of subjective oracles which try to analyze stake flows and stake correlation to determine whether it's the same guy staking across these different entities, or it's actually distinct home stakers. All these things give me confidence that there'll be some amount of incentives for decentralized homer operators that can come through Eigen layer, which in its absence, actually just makes it significantly worse than centralizing.
Speaker B: That was a fantastic, just high level overview of, I think, some of the big questions about Eigen layer and kind of restaking specifically. And I want to bring Teddy into this conversation to open up the Eigen Da rabbit hole. Because I think this can be a more narrow understanding of what it means to be an AV's. And because Eigen DA is being incubated in house by Eigen lair. There's some, definitely some additional knowledge I want to pull out of you, Teddy, here. So I want to ask the question, what is Eigen DA? But I want to ask it in three different ways, because I think we can kind of get three different answers out of it. There is Eigen DA, the data availability network. There is Eigen DA, the first internal incubated restaking network by Eigen layer. And then there's Eigen DA as this, like very proximate data availability layer to Ethereum. So what is Eigen DA as? Like as it needs to be for Eigen layer to incubate its own network. Like why does Eigen DA need to be a thing internal to Eigen layer? How does Eigen da compare to other DA layers? Like there's many DA layers out there. How is Eigen layer different? What are the unique properties? And then lastly, what does Eigen Da specifically do for Ethereum, the Ethereum ecosystem that other data availability networks don't do? So, three questions all about what is Eigen Da? You can start however you want to start, Teddy.
Speaker D: Yeah, sure. Well, so Eigenda was both an opportunity and a necessity for Eigen Labs, because we had the plan for Eigen layer and we needed a way to demonstrate it to the world. We wanted to build a product that was truly useful for people to attract stake and other AV's projects to Eigenlayer. But on the other hand, it was also an opportunity, because we looked at the landscape of DA providers and saw an opportunity to build a DA layer from first principles that was better. The goal of Eigen Da is a trustless, decentralized hyperscale Da layer built on top of Eigen layer. And alluding to one of your questions about alignment with Ethereum, that sort of implies alignment with Ethereum, given that we're building on top of eigen layer. So I think the main thing people want to know is how is it trustless, decentralized and hyperscale? And how does that set it apart? I guess I'll start with trustless. Eigenvate operates on the basis of operator nodes which opt into the eigenda network via eigenlayer. And so operators are providing storage bandwidth to the eigenda network on the basis of the amount of stake that they have attributed to them. So if I'm an operator and I manage to get 5% of the Eigen da network stake, that means I'm going to be receiving roughly 5% of the data. This is how we achieve this trustless quality to eigenda. We ensure that every operator is only handling the amount of data that it is on the hook for providing. And this is also what sets it apart from a naive data availability committee, which, although very simple, does not provide these trustless decentralized guarantees. The hyperscale part is what I think is the most interesting about Eigenda, which is that Eigenda's capacity scales with the total bandwidth of the operator set. This means that as the number of operators joining the Eigen DA network grows, the amount of data that Eigenda can support writing and reading to also grows. And how do we do this? I mean, most other DA layers either involve relatively simple data availability committees or maybe involve some amount of consensus, and we try to take a hybrid approach where we remove peer to peer consensus from the dispersal process of data. So Eigen DA can generally be thought of as this operator set, which is interactive via a disperser service. So data is sent to these operators and this dispersal service, which can be understood as something like a decentralized sequencer in a roll up analogy, is responsible for collecting these various signatures that form a data availability proof and posting these signatures on chain to Ethereum to certify availability. And there are several other pieces of technology which I can't go into yet, which ensure that data is available not only that it's stored, but that it's not being withheld, and systems for payment and for slashing.
Speaker B: I want to go into the unique properties of Eigen Da just a little bit more. My mental model, my map for understanding Eigen DA is kind of like a dank sharding sidecar, where it has a lot of properties that EIP 4844 full dank sharding also have, except that it's also a separate network, except it is also secured by ETH. So it seems to be like a very proximate replication of dank sharding, just as a sidecar network running in parallel to Ethereum. And why is it in parallel to Ethereum? Well, because it's using ether as stake. It seems to be the closest approximation to dank sharding data availability while also retaining the security of Ether. But yet it is a separate network from Ethereum. Data availability first, Teddy, is that a fair articulation would do? I need to amend that. Is that accurate? Is that inaccurate? And to what degree that is accurate? How is that extra useful to Ethereum versus other far more distant data availability networks that aren't so close to Ethereum?
Speaker D: Sure. Yes, well, that's generally accurate. I like to think of dank sharding as being sort of the public option that will eventually arrive, and Eigen layer as being a very closely aligned private option. So eigenlayer has plans to support greater throughput than dank sharding, but in the short term they generally align in terms of bandwidth planning.
Speaker B: And so why would someone use eigenda over some sort of more third party networks or alternative layer ones. What benefits does eigenda bring to the table?
Speaker D: Well, so eigenda settles to Ethereum. This means that roll ups will have lower latencies when settling to Ethereum themselves. This is one of the larger advantages. The other is that eigenda is going to be generally Ethereum line product going forward. We don't have any plans to try and move away from the Ethereum ecosystem. When roll ups who are already deciding to commit to Ethereum use Eigen DA, they can be assured that we're planning on the basis of 4844 and dank sharding in the future.
Speaker A: You asked about how we got started with Eigenvalue, and I think Teddy gave a good answer there, which is that it is not only a proof of concept that, hey, you can build something interesting, but also a proof of value that you can build something useful on top of Eigen layer. And value is needed when you want to get this kind of a platform bootstrapped. You know, you want to start Eigen layer, who's going to use it? Who's going to come and build protocols on top of it? We have at least one useful kind of product on top of it. That's Eigenva. But how did we actually arrive at this is, there's an interesting story. This was back in 2021 when we were working on just coming up with some of the core ideas around Eigen layer and restaking. And, you know, I had decided to fund this startup, just, you know, bootstrap, use my own money to do it. And we were several, you know, maybe more than six months down the journey at that time. And I was talking to many VC's, and one of the VC's I talked to was Kyle Samani from Multicoin. And I gave this pitch, hey, you know, here's Ethereum, you can stake and then you can use it for other networks, and you can have these kind of slashing and things like that. And I said, oh, these are just looking like fraud probes and optimistic roll ups. They suck. They're not going to work. I was curious why he said it, and then I asked him, why do you think optimistic roll ups suck and they're never going to work? And he said, because they're very expensive. And I hadn't, like, I was not paying close enough attention to know that optimistic rollups are more expensive. And I said, thank you, and like, finished the call, and then we went back and called the team, and I said, hey, I heard that optimistic roll ups are more expensive. Can we dig into why this is the case? Just go in and look at it and we find it's all just data costs, right? Because, you know, you don't even have to write a proof to Ethereum, so why is it more expensive? And then I looked at it and we have actually been working on data availability for a much longer period as an academic. In fact, one of the first papers on fraud proofs and data availability that Mustafaan Vitalik and others wrote, I was actually on the program committee and I championed this paper to be accepted in financial cryptography. We'd been thinking about data availability for a long time, and I knew that of all the things we know how to scale, data availability is the one we know most how to scale. And looking at the costs, it's like, oh my God, there's a huge opportunity because we didn't know how to get this platform started. That was the other question that we couldn't answer in any of these vc pitches is, oh, you build this platform, who's going to come and build anything on top of it? I was like, I don't know. It was useful. From that it became, we are going to build Inda, the first data availability service because we know exactly how to scale data availability. We have this platform. In fact, we even had a paper called a data availability oracle, like two years before this episode, and basically that. Basically saying that, hey, this is an off chain network that certifies data availability to Ethereum, and we just didn't know how to bootstrap this. And Eigen layer was designed to solve the bootstrapping platform, so we became our own customers to actually then build Eigenva. So that's. And I did tell Kyle this like a few months back, and he's like most people when I say something like that, they just get annoyed and, you know, but you're the only one who took it positively and came back and thanked me after some time, I guess. Yeah. So that's so going to the other question on why Eigen Da. And one of the things is, I think Teddy was alluding to earlier, is Eigen layer is built as the only eth centric data availability layer. Okay, what does it mean by eth centric? Ethereum centric. So it's Ethereum centric in many ways. Other data availability layers are actually blockchains. They say modular, but it's actually an entire blockchain. There is a consensus layer. There is a new asset. There is a new trust layer. There is also a new data availability system. All of them packaged and actually packaging. It brings you certain superpowers. And the superpower is if you natively run a roll up on top of that blockchain, you actually inherit much better security, because if data is not available in that system, let's say something like Celestia, if data is not available, then the blockchain itself will fork around such a failure. So if you're a native roll up on Celestia, you actually get a lot of security guarantees. But if you're an Ethereum roll up, your ordering primitive in Celestia has no barring on it. From the viewpoint of Ethereum, everything is just a committee. Like all that. If you're an Ethereum roll up, you have a roll up contract sitting on the Ethereum blockchain, and it's just viewing some certificate from some committee, like that's all it can do. It cannot do. A contract cannot do data availability sampling. A contract cannot do like whatever set of features that are actually available on that other platform. So we, instead of like clubbing all these things and clubbing two separate goals, which is I want to build a blockchain of my own and I want to provide data availability services to the Ethereum ecosystem. Started from first principles. How would one build just a data availability adjacent, a data availability layer which adjoins the Ethereum network? So the first thing is Ethereum has roll ups rely on Ethereum for ordering and consensus. Therefore you should not need to have a separate consensus. A separate consensus adds nothing to your own Ethereum roll up ecosystem. Okay, so that was the first thing. Remove consensus. And once you remove consensus, you see that the design space for actually maximizing throughput and reducing latency and all these other things explodes rather than reduces. Because now you don't have to do another thing. You know, another module consensus being another module. You don't have to do it. You only have to do data attestations. You just have to certify that data is available. Now we can start to think from first principles, what set of things you can do. I can give you some examples. I think Teddy alluded to this earlier. The idea is, for example, in Eigen Da, the way it works is there is this committee, this committee is staked. What do you stake? There's a natural thing to stake, which is eth. And we have Eigen layer. So that's one side of this. The second side is because we had to think wearing the ethereum hat on. We have to make sure that nodes that participate in Eigen DA need to have very limited resources or can manage with very limited resources. Now, people say, like, oh, this is a big constraint. Like the Solana people, for example, say, oh, this is a big constraint. But, you know, there is a power in adopting constraints and then seeing actually like, how to, because these constraints are meaningfully adopted, right? They're adopted for, I want to maximize decentralization or whatever, and then ask, like, let's say each node has low amount of bandwidth, but you have an insane number of nodes. Like, you know, Ethereum has, whatever, 900,000 validators, right? They may all not be distinct nodes, but the whole system is set up in such a way that if you have 32 ETH, you should not need a lot of bandwidth to participate in the network. So one of the things we said is, hey, I want to adopt decentralization, but if you centralize, I'm going to make you pay more in the sense that now you have to store more data, now you have to expand more bandwidth if you're more centralized. So till now, even if you're validating Ethereum, if you're a single operator running 10,000 validators, versus if you're a single operator running a single validator, both of them have the same expense basis. So there is like an inevitable, you know, a drift to centralization, because if you centralize, you are more efficient. Eigenda breaks this. Oh, if you centralize and you have 30% of the stake and somebody has 0.1% of the stake, you have to do 300 times more work to hold 30% of the stake than you would if you're holding 0.1% of the stake. So we start not only from like, okay, when we think of Ethereum centric, the first thing, no ordering. Let's just do like, data availability. The second thing, what is the asset to use? Use eth. The third thing, what is the principle? What is the philosophical substrate on which you're operating? Get as much decentralization as possible. Get node operators to participate with very limited resources while minimizing the benefit of centralization. So these are some of the decisions that went into building like eigenvalue. There are certain emergent benefits from it. For example, when you want to do ordering and consensus in your own, like, chain, one of the things you have to do is you have to lock the state, have a leader, the leader proposes the next block, and that's how you build the chain. Whereas in Eigenvae, what happens is because we are not doing ordering. Ordering can be just data attestations or data availability. Claims can be completely paralyzed. So David sends a data availability claim to eigenda nodes. They all receive a chunk of the data availability claims and send David a certificate. And similarly to Mike is sending a data blob, splits it into small chunks, sends it to the eigenda nodes, they all send him the claim, they send the same thing to Terry, all of them in parallel, so you're not deadlocking on the state at all. Second, your censorship resistance is not gated of Eigen DA is not gated by some leader. There is no need for a leader. It is what we call self leader protocols. Like if you're a sequencer, you can yourself decide, hey, I just send the data chunks, encode, and send the data chunks myself. I don't need to wait for, like, you know, that leader. And the leader becomes the MEV center of the entire data availability system. So essentially what we found is actually being a data availability adjacent to Ethereum as a first order design principle unlocks a variety of different things that we had to actually innovate from first principles on.
Speaker C: Sweet. Yeah, no, this is super good. And I want to drill down into some of the mechanics. Teddy, I'll ask you two questions about the day to day of running eigendae. I guess tying it back to 4844. The initial design is set up so that there's going to be target of three blobs per block, three blob transactions. Each blob will be like about 128. Design is conservative so that we know that all Ethereum home stakers can keep running this on the same Internet connection before just alluding to what Sriram was mentioning, just curious on the numbers for Eigen Da as far as how many kind of kilobits per second. And also like me as a solo staker on my home Internet connection, will it be reasonable for me to be able to run Eigen da restaking service and kind of participate in that network? The second question I have is around these slashing conditions. When we're thinking about the settlement assurances of Eigen Da. Like someone who posts DA to the service wants some guarantee that that DA will be available with some economic security. Can you just talk about what the slashing conditions actually look like and how those violations would be resolved if someone didn't fulfill their promise of making that data available?
Speaker D: Sure, I'll take the throughput question and then pass the slashing conditions to you, Sriram. But we've launched our testnet guaranteeing around 1000. We've launched our network according to the design Sriram just talked about the amount of bandwidth you need to run an operator is not at all related to the total throughput of the network. It's related to the amount of stake that you have assigned to you. This should make it possible for smaller operators within the eigendi network to still receive chunks of data and earn on their staked assets. So on our testnet, the benchmarks we've run locally have suggested that it can support roughly up to three megabytes per second. We're pretty confident that we can get to ten megabytes per second by Mainnet. All of this is powered by roughly two things. One is improving the speed at which we can encode blobs. Each blob has a combination of reed Solomon encoding and KZG encoding, which is somewhat expensive, but also not an optimization problem. That can't be solved. On the other side is just increasing the number of operators. We see it as very possible to get to 100 megabytes per second one year from the launch of Eigenva on Mainnet.
Speaker B: Teddy, you threw out some numbers just now, like 1000 kb/second which, I mean, when I download stuff, I'm downloading stuff on my computer, that's, I need way more speed than that. But also, at the same time, we're talking about crypto economics, and so cryptography compresses stuff. And then you started talking about like ten megabytes a second, which is starting to be a number I can reason about just overall. Is that a lot? Like, how much is that? What do we get from that? How can we compare these things in maybe more qualitative, less quantitative ways?
Speaker D: Sure. So one ethereum block is roughly 200 kb. This is every 12 seconds, and you can do the math to figure out how much that is per second. EIP 4844 is moving us towards something like 32, just to do apples to apples comparison. Celestia is about 167 kb/second this is just comparing with the status quo. But I think that what we're looking at, just from a market perspective with eigen DA, is a situation where the cheaper block spaces, which is essentially what DA is providing for rollups, the cheaper block spaces, the greater we're going to see induced demand. Everybody knows in their heart that blockchains would be mainstream if it was cheap enough and reliable enough for people to use. And so, obviously, ten megabytes per second worth of transactions doesn't sound like a lot, but that would represent a roughly 50 x increase over the current status quo. When people are using Ethereum or Ethereum.
Speaker C: Roll ups, one quick follow up before Sriram talks about slashing conditions, because I'm sure that's going to be an interesting part of the conversation. You mentioned that if you have more restaked ETH as an Eigen DA node, then you're responsible for more data. Just trying to understand the mechanic here. Would that potentially incentivize large node operators to, like Sybil themselves and have many smaller node split into many smaller operators to not have as much data responsibility, but still earn the same rewards as people solo staking? Or is there kind of just a linear scaling on the amount there?
Speaker A: It's linear on the amount. It's basically proportional to the amount of stake.
Speaker C: Okay, cool. Yeah, and so I guess going on to the slashing conditions question, Sriyam would love to hear the.
Speaker A: Yeah, I'll just add a little bit on the throughput thing. When I first did the numbers and tried to calculate Ethereum's data bandwidth is 80 something kilobytes per second right now. I was like, oh, why is it so slow? Why is it so small? And I think there is a lot to improve here, but I want to kind of phrase this in the broad arc of human evolution. I think of these bytes inside the team. We say our goal on building eigenvalue is to maximize coordination bandwidth. If you think about it, these are complex coordination systems. We have all these parties in Ethereum certifying and maintaining this ledger based on which lots of coordination is happening, like, you know, movement of money and other things. So usually just, you know, if you just neglect the last, like, whatever, five to ten or maybe even 20 years of history, we were, as a species, able to coordinate on very few things. Like we would elect who is the president. And, you know, a president can specify very simple immigration policy. Immigration good, immigration bad. Like, that's one bit, right? We had coordination bandwidth, which was like five bits per five years, like something really, really, really small as a species. And suddenly I think we are, you know, this is, this takes some time to, like, sink in. We are scaling that to kilobytes per second, to megabytes per second, to gigabytes per second. So what this means is certainly the rate at which we can coordinate as a species, maintain common information, enact powerful coordination conditions, just like insanely skilled. And I think this is just like, if you think about it like this, the Internet unleashed this information super highway. Like, we can kind of talk to each other, but that's still not the same as the ability to coordinate with each other, because I may be talking to you something. I may be telling somebody else something, because this is not global, verifiable state. And with systems that promise, like, common data availability or, like, more block space, essentially, we are talking about the rate of the bandwidth of coordination as a species. Right? Let's keep all the blockchain wars, l one, l two, l three, aside, right? Just let this sink in. It's insane. It's amazing. It's unusual. It's like we're becoming this much, much more coordinated. In fact, as a species, I think our evolutionary advantage is that we are able to cooperate at a scale that is simply not possible for other species in a flexible way. Like you all know, our harari, in his thesis, says, our humans are special because we cooperate flexibly in large numbers. And it is cooperation when we are talking about a shared bandwidth. It is flexibly because I can program all kinds of new vms and conditions and contracts interesting arrangements on top of it and in large numbers, because we can have everybody agree on this common state. This is insane. And, you know, as a community, that's what we are setting out to accomplish. It's just good to put it in that perspective rather than the day to day thing of, hey, you know, I'm doing x better than Y or z. We have two mechanisms right now for ensuring the fidelity of data availability. Number one, proof of custody, which ensures that people store the data, but it doesn't have a mechanism to ensure that people serve the data. So that's proof of custody? Proof of custody is basically if you don't store the data and you have to respond in certain ways, you have a secret, you have to sign some blobs based on, like, the state of the data that you're storing. And if you don't do it correctly, you'll be slashed. So, proof of custody is a mechanism to ensure that you're storing. But what if you're storing? All the nodes are storing data, but they all collude and coordinate to not serve the data to anybody. So that's a problem. So while proof of custody relies on economic security. Okay, because you'll be slashed if you don't do your proof of custody. How do we ensure that people serve? It is by ensuring that the operator set remains decentralized and collusion resistant, so that there is a competition to serve. Because no one node has all the data, the data is dispersed across many nodes, and as long as you can get a quorum number of nodes, you will be able to retrieve all the data. Now you have a market where there are many independent players who have the data and are willing to serve. Unless everybody colludes together, or some large number of stakers collude together, you will be able to retrieve the data. So that's the mechanics. So it borrows both decentralization from Eigen layer as a separate principle, as well as borrow economic security from Eigen layer. Furthermore, we are actually building new security mechanisms which are on top of this, which I think we are not yet ready to share, but we'll be ready to share in the coming months.
Speaker B: One thing that I think is pretty cool about just the primitive of restaked capital is that it opens up opportunities for interoperability across networks that wouldn't otherwise have been interoperable. One of these things that I've been keeping an eye on is the superfast finality layer out of near, which is in partnership, in collaboration with Eigen layer Sriram Teddy I've been on a quest to learn how all of the many, many Ethereum layer twos, which are fragmented, how do they recompose back into one unified network? We have so much scale on Ethereum. We have horizontal scaling, we have vertical scaling, but we don't have yet a composed, coherent network, at least from the perspective of the end user. And there seems to be many different answers as to how all of these networks become recomposed. But one answer that I always come back to is low latency settlement finality. If one roll up can have assurances that settlements from a different roll up are final, all of a sudden we can unlock a lot of composability. And this is something that I think is what you guys, Eigenlayer and Nir are pioneering with this super fast finality layer. So, Sriram, maybe you can just walk us through this partnership, this collaboration with NiR and the super fast finality layer. What is it and what is it doing? And what is its impact upon the Ethereum roll up landscape?
Speaker A: The idea of a super fast finality layer is to ensure that you get instant finalization guarantees. So what happens is the roll up writes to this layer. This layer is. Think of it like a chain. The chain is getting economic security from each staking. Let's keep that on the side. Just think of it like a chain. You just write the roll uprights. Settlement commitments to this chain. This chain then writes the settlement commitments to Ethereum. Okay, so, but the order in which these commitments can be returned is rigid based on this chain. So what happens is I write a commitment to this chain, and this chain gives me a certificate that, yes, this is the order in which this is going. Now I can take it to another roller, which is also tethered to this, like, fast zone, and say, yeah, this fast zone is verified that this is actually happening. So I can move value back and forth. And what this does is solve some of the liquidity fragmentation problems. Because instead of liquidity residing primarily in the roll ups, the liquidity can reside in this, like, fast zone. And people have hooks to draw liquidity in and out of it. Like, each roll up has a hook to draw liquidity out of this layer and then give it back. This is like just in time liquidity across all the different roll ups. Because now you have a common zone which can move really fast, which has economic security, because it's borrowing it from each staking. And now it becomes a zone where a lot of these things compose. So this is the idea that we are exploring with Nier. But other projects are also building somewhat similar things on Eigen layer. One is omni, which is building a shared liquidity layer. Alt layer is building super fast final layer, specifically tailored around, like, you know, the roll up and eigen layer ecosystem. So these are some of the attempts at solving the roll up interoperability problem. There's another interesting thing, David, that goes on with the roll up interoperability even without this fasten finalization. Imagine I want to move value between one roll up and another roll up at a time scale which is much faster than seven days. Let's say both are optimistic roll ups, and I want to move value between them. Then the way to do it is if I had, if I want to move like 100,000 ETH, if I have from an eigen layer service, a promise of more than 100,000 ETH slash ability, then I can take that commitment as final and then use that trigger to move value between these two roll ups. So the roll up fragmentation and the fact that rollups are fragmented. And second, the fact that roll ups are fundamentally denominating in ETH basically gives another utility to ETH as a staking asset for moving value across these different roll ups. And so we have bridges which are specifically building around this concept of what we call attributable security. Each bridging claim buys a certain amount of security from Eigen layer and then moves value around. And as long as the total attributable security that the bridge holds is less than the total value moved around, you are actually completely safe. So this adds another interesting utility to ETH as a staking asset in backing these bridging claims a fast finality layer accelerates the rate at which you can do it. That's basically the kind of overall landscape that we're looking at here.
Speaker C: Yeah, super cool. And I think now that we've kind of covered a number of different use cases for eigen layer, I want to bring the discussion up to this idea of aggregation across many different AVss. I think a common theme that's been discussed is this idea that as someone who has capital they want to restake, they might have a hard time deciding which AV's to delegate to, which AV's to opt into, which node operator within that AV's to delegate to. And so this kind of brings forward this idea of some abstraction layer between the restakers and the actual AVss. I think this is kind of where the liquid restaking token discussion usually fits in. So I would be curious to hear how you think about layers building on top of eigen layer and how they're like managing risk of many different AVss and many different node operators to ensure, you know, the fungibility of their, the liquid restaking token. And just generally your opinion on these tokens as a concept and the potential implications to the Eigen layer ecosystem.
Speaker A: Yeah, I mean this is this layer of abstraction that Mike is talking about. The idea is that as a staker, I don't want to sit and make these decision as to what is the set of node operators, what is the set of AVSse. Maybe I should allocate some portion to some avss, some portion to others. Like should I, should I accept rewards in only ETH or can I accept rewards in new tokens? Like there's just like a lot of different dimensions that simply don't exist as a staker in Ethereum. You just go download, stake and run. Like that's, it's, it's clear, well specified. Being a double opt in platform Eigen layer also brings all these new, new things, like at what price to accept an AV's? Does it offset my operating costs? There's all kinds of questions that go around it. So these liquid restaking tokens are one subset that basically tries to address these kinds of questions. The idea being they create a decentralized organization which basically tries to adjudicate and make these decisions and take stake on people's behalf and then go and delegate it to various operators. So the questions there are firstly, are LRT's good for the Eigen layer kind of ecosystem? And I think I had a somewhat different answer six months back. And then actually considering various things, I think on net, they're actually very good. The reason is imagine that somebody wants to build a lending protocol or some other thing based on your Ethereum stake that's staked on Eigen layer. Now, there are two ways to do it. One is to do it inside the Eigen layer platform and say that, hey, if you get slashed, I'll actually go and withdraw your stake from Ethereum. If you get liquidated, I'll actually go and withdraw your stake from Eigen layer and Ethereum. And another option is I have a liquid token, and then I just have people exchange hands. Like I get liquidated, I give my liquid restaking token to David. And the previous one actually has worse cascade risks, because instead of when the liquidation happens, I now have to unwrap my eigen layer position, which means eigen layer security fluctuates and also eigen layer goes and unwraps your ethereum position. So the Ethereum security fluctuates. So what I've started seeing liquid staking tokens, as are a layer of buffering, basically, let the financial thing be buffered at a higher layer rather than any financial thing like create a shockwave that goes through the entire ecosystem. Imagine there is some big decentralized stablecoin or something built on top of this, and some eth to USD price change happens. And this, without, like this layer of buffering, you undergo this massive like shockwave which goes through Eigen layer, which goes through Ethereum, rather than just like getting buffered out at the top and people just exchange, oh, you know, you got my liquid restaking token instead of I got it. That is much safer, I think, for the entire ecosystem, knowing that these systems are permissionless and knowing that these things are anyway going to happen. Okay, given that now the same kind of alignment problems that Ethereum had to wrestle with get either amplified or similar problems show up with Eigen layer. Because one of the things is if there is one single dominant staking token and that Dao makes all these decisions, Eigen layer loses the free market property. Like one of the, at least in Ethereum, price discovery was automated and algorithmic. Eigen layer relies on two sides of the market. AVSS is bidding a certain price and stakers accepting or rejecting that price. If you had one collusion on one party representing the interest of the entire other side, then you don't have the free market movement. So this is something that we have to figure out, like Ethereum had to figure out. We have to figure this out over time. But one high level lever is unlike Ethereum, which has to be absolutely neutral and completely protocol, Eigen layer can have some governance levers to actually move the system to be healthy across the multiple sides of the market. So that's just a lever that we have. But in general, I think these liquid restaking tokens, we are seeing many highly talented teams come in and build this, which I think is net positive not only for Eigen layer, but for Ethereum itself, because it induces more competition, a new opportunity to actually participate in the liquid staking market by also being part of the liquid restaking protocol.
Speaker C: Yeah, and just to kind of continue on in the risk direction, because I think it's super interesting. So you were talking about how Eigen layer can kind of be more opinionated about some of these risks. I think maybe part of that equation is the slashing veto committee. So I guess I'm curious how you see the importance of that as a tool to underwrite the risk of things built on top of Eigen layer. And also kind of the trade off between being a permissionless kind of platform that anyone can build on, anyone can launch any AV's, but then also some of them are going to be kind of king made in that the committee is supported for them versus not for others. So can you just talk about the tension there?
Speaker A: Yeah, absolutely. Absolutely. So the goal of Eigen layer is to be completely permissionless. But how we start up the platform, I think in Justin's words, these platforms have path dependence. So you want to make sure that the platforms start off safe. So we are going to start with a bit more permissioned than totally permissionless on day one. And the way it's going to work is the slashing veto committee needs to know what slashing to veto. So, which means it needs to know what is your AV's. So there is an onboarding condition. Either the slashing veto committee themselves do, or they trust some other committee to do basically the onboarding of various avss. That minimizes the risk profile. You know, it has to be auditor, it has to follow certain guidelines. All these kinds of things are enforced in that layer so that we can onboard safe and useful services before eventually becoming a completely permissionless platform. So each service, over time they either are on, they start out with the slashing veto, and then over time, as the platform matures, and as also the services mature, there's going to be an option to be free of the slashing veto committee. You can just go and say, hey, I don't want the slashing veto committee because I'm rigid. I'm ossified. I don't need to actually trust us. Okay, so that is an option. But more generally, I think, you know, I made the MEV and PBS analogy earlier. We can follow some of the kind of like, ideas from that space. One of the things that in MeV boost happens is there's this concept of a relay. A relay is a doubly trusted party from both the block proposer and the block builder, both of them trusted for different properties, but that is a doubly trusted party. Similarly, a slashing veto committee is a doubly trusted party from the AV's and from the staker side. The staker are trusting the veto committee to veto illegitimate slashing. The AV's is trusting the veto committee to not veto legitimate slashing. So it's a doubly trusted party. So you can take that abstraction and think of like this veto committee as an entity, and now you can say just exactly what happened in Mev boost. You can create a marketplace of veto committee. There doesn't have to be one veto committee that, like some small group of us decide. There can be a marketplace where people can come in and say, hey, here is a new veto committee we have self coordinated to form. Essentially, this is like an adjudication committee between like the, you know, AV's and stakers. And of course, if the AV's is completely rigid and solid and ossified, you don't need adjudication. So you go to a null or empty adjudication committee. But the more untrusted you are, you are underwriting some amount of trust from the adjudication committee. So we envision this as one of the ways in which Eigen layer, as an ecosystem evolves to remove this permissioning. This is one of the ethos that we want to follow is minimize subjective decisions at the eigen layer level. This is one of the reasons we don't build a liquid restaking token. This is one of the reasons we don't say which tokens can be staked and which tokens cannot be staked initially, even though we are starting with a permissioning procedure that over time is going to be completely permissionless and people can decide, this is the aesthetic. I think a protocol should minimize subjective decisions. Subjective decisions should be made by agents who have both rights and responsibilities, and they can figure out how to exercise it. So that's the ethos in which we're building.
Speaker B: The credible neutrality of Eigen layer, of course, is going to be super important, especially as, oh, by the way, congrats guys, because during this podcast, Eigen layer just crossed $1 billion in TVL. I'm sure that makes you feel fantastic and also perhaps a little bit nervous. At least I think it should. And we have all of these liquid restaking token teams that are going after that pie. It's a big pie. It's super valuable. And the credible neutrality of the Eigen layer protocol, of course, makes plenty of sense. It ought to be that way. But what about the neutrality of Eigen Labs, the organization, as it tries to help some of these liquid restaking tokens bootstrap? Because of course we do want these things. But I can name five names in my head about who would enjoy to be the Eigen layer approved liquid resaking token. And of course, you also can't work with every single liquid resaking token down the long tail. A, because you don't have enough resources, and b, because one of them will be a rug. How do you guys think about just neutrality when it comes to supporting the liquid restaking token ecosystem from the Eigen labs perspective?
Speaker A: Yeah. This is not only for liquid restaking tokens. This is the case for avss. Let's say there are three bridges which want to build or like finality layers and all of these things. So we face this kind of a problem. So one of the base at least, we want to minimize our own role in many of these processes. So we want to create external committees which will make a lot of the decisions over time. For example, onboarding, right? Onboarding Avss. We don't want to say, oh, I like this AV's more, therefore we're going to onboard it, but not that AV's rather the onboarding process should be merit neutral but risk sensitive. Right. You cannot onboard based on oh, this is going to be a bigger AV's, that is going to be a smaller AV's. Instead it is onboarded based on this is going to be more risk. This is going to be less risk. So it's risk aware, merit neutral kind of onboarding process. But it is a hard, hard trade off because we have to also make sure that at least some people build. If nobody builds, it's an like you can have all the, you can be credibly neutral if then nobody's building. So it is a hard trade off. I think layer ones, many of them had to do this. Ethereum itself had to do it. Do you support Uniswap or not support Uniswap? And there's a position of the protocol, there's a position of people and Eigen labs itself is a big team. It is complex. So I'm not going to pretend I have some great answer here, but we are trying to both make sure that projects build on top of eigen layer, but also make sure that other projects don't feel like there is a barrier to come and build on top of Eigen layer. So that's how we split the difference here.
Speaker B: Teddy, as a research engineer at Eigen Layer, one of the things that excites me about Eigen layer is that it can spawn an entirely new dimension of crypto economic networks. The way I articulate Eigen layer is that we were living once upon a time in flatland space, when all we had were blockchains. Now with Eigen layer, we were entering a third dimension, a new dimension of crypto economic trust networks. And I would imagine that that is a fantastic nerd snipe for you over. As a research engineer, just the success of eigen layer depends on many networks coming on board. And so I'm sure you engage with some of these networks. Some people have ideas about networks that could be built. There are stealth networks being built, I'm sure, because that's just how it works. Can you kind of just, like, give us a vibe, a charcuterie board, a taste test of all of these different kinds of networks that maybe you're working with or just in discussions with, are we going to see millions of networks in the fullness of time? Or maybe just like ten to 100 networks in the fullness of time? What can you say about all the yields that these networks are going to spit off? Are they going to be great? Are they going to be little? Just give us a taste of the future for Eigen layer in 2024.
Speaker D: Yeah. So Sriram has this slide in his talks where he goes over. There's like five different categories of absss and probably more. We've got co processors, Oracle networks, obviously a variety of DA layers. There's going to be a cambrian explosion in the same way that there was with l one SDHE and other sort of general token based crypto economic schemes.
Speaker A: Yeah. One way of thinking about it is how many SaaS services exist on the cloud era. People think about modules. They think about, oh, there's data availability, there's settlement or whatever. But if you look at, if you took us, put a similar hat, you may say on a cloud, there may be one database platform and one virtual machine. There will be two layers. But actually, if you look at the cloud, you'll see thousands of successful SaaS companies software as a service companies. It's because as a civilization, I think our tendency is to hyper specialize. Because when we specialize, there is a lot of value in specialization and composition. I specialize and I just do a database for games. For games which are coming from AAA studios or whatever, something very, very, or just a database for games for the Unreal engine, something super special. But that itself is a big enough and interesting enough market that somebody will build that kind of a special layer. So that's a long range thesis, is there's going to be like lots and lots of modules, just like there's lots and lots of SaaS services.
Speaker D: We can think of it as a new kind of financial market because I don't know, if you look at, for example, bonds. Bonds are a generalization of a loan where money is actually changing hands over time. Restaking is different from that because money is not directly changing hands. It's enforcing a crypto economic set of incentives and scheme. But I guess the gravity of the invention of a generalization of this kind of financial instrument is on the same order of magnitude. We think that similar to how the number of bytes and kilobytes and megabytes and gigabytes of coordination we're able to generate through blockchains and DA layers continues to grow, that the number of crypto economically secured services will also grow.
Speaker B: All right, guys, well, I've already learned quite a lot in this episode. I'm going to have to relearn this, rewatch this, relisten to this to make sure I can understand some of Mike's questions and Sriram's answers. Sriram, I think you said Mainnet sometime. Q one, Q 220 24. Is that all of the details you can give us? And what else are you looking forward to in eigen layer in 2024?
Speaker A: Yeah, we have an upcoming main entry, Q two. We will have, will expand the scope of what kind of slashing and attributions that that can be done. For example, we have this mechanism we call attributable security, where you're not just getting this idea that, hey, if my service goes wrong, X dollars will get slashed, but I will be able to redistribute specifically a portion of that X dollars. So combining the idea of pool security and attributable security and creating mechanisms where a particular AV's has a portion of that pool as a specific attribution, that's something that we are expecting to launch also in 2024. We have, of course, Eigenvalue, which I think is going to be a very important and useful primitive for rollups. We have many partners launching roll up services like decentralized sequencing from espresso. We have alt layer launching these finalization and other services for rollups. We have major bridge partners like Polymer Lagrange Wormhole, launching a bunch of different bridging services. We are also excited about AI related services coming up on Eigen layer. Like imagine you're sitting inside an Ethereum contract, and you can make a call and get an AI service and its answer certified with a certain amount of economic security, so that if you take an economic action, you have protected till that amount of value. So you can think of DeFi itself becomes more intelligent because now you have this ability to get much more computational resources on your disposal. This is the category of coprocessors that Terry was referring to. We're excited about all these cascading. There's also other things. For example, there is MEV, which, you know, value, which can be controlled more by a protocol rather than MeV value necessarily going all to the validators. So an application can say, hey, you know, this group has to consensus in order to like certify the MEV, and they will redistribute a portion of the MEV back to protocol. All these kinds of really interesting use cases coming up with Eigen layer this year. So looking forward to encourage more builders to come on top of our platform, both as roll ups on Eigenda, but also as building brand new services on Eigen layer.
Speaker B: If people want to just learn more, get information, open up the docs, get started. Sriram where should they go?
Speaker A: Eigenlayer.org dot yeah, there's also forums where we have research discussions on the Eigen layer system. There is a blog where we put up regularly material on new things. Also the Eigen layer Twitter handle, which points to all these different things over time.
Speaker B: Well, Sriram, I think the evolution of restaking networks, AV's networks, is going to be one of the more fascinating things in 2024. And one of the reasons why I like it is because it goes right down to the heart of ETH, which I think the monetary arc and development of ETH as a monetary asset, I think is one of the most fascinating things in crypto. And Eigen layer seems to be the next evolution on that story. So as a podcaster who likes interesting podcasts, I thank you for bringing this evolution to the world. Teddy Sriram, thank you so much for coming on bankless today.
Speaker A: Thank you so much, David and Mike. Our pleasure to be here.
Speaker B: Bankless nation. You know the deal. Crypto is risky, staking is risky. Restaking is even riskier. But, you know, it's probably more fun, too. You can lose what you put in. We're headed west. This is the frontier. It's not for everyone, but we are glad grow with us on the bankless journey. Thanks a lot.
