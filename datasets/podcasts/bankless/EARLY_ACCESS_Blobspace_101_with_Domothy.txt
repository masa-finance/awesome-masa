Speaker A: Welcome to bankless, where we explore the frontier of Internet money and Internet finance. This is how to get started, how to get better, and how to front run the opportunity. This is Ryan Sean Adams and I'm here with David Hoffman. And we're here to help you become more bankless. You know how we say blockchains sell blocks? Well, soon Ethereum's going to be selling more than just blocks. It's going to be selling blobs, too. That's right, blobs. So we're just a few months out from the biggest Ethereum release since the merge, and I think no one has fully mapped out the implications of this, but it's going to be huge. So Ethereum is getting a new product to sell. It's called blob space. That is, in addition to block space, the cost of transactions on layer two s is about to drop towards zero. The economics of ETH gas and the burn are about to change forever. We're calling this upgrade the blob space upgrade. EIP 4844 proto dank sharding. That's what the geeks are calling this new Ethereum feature upgrade. And we want to cover today everything that you need to know about blob space and this coming Ethereum upgrade with Ethereum researcher Domothy. And this is an absolute banger of an episode. I couldn't be more proud to present this to the bankless nation. A few takeaways here. Number one, we go through what is blob space? Number two, we go through the history, how we actually got here, this roll up centric roadmap. Number three, we go through the economics. What does this mean for Ethereum's economics, for eth value accrual for Ethan the asset? David, why was this episode significant to you?
Speaker B: I think if there's any sector of conversation that you and I really just love is the intersection of cryptography and economics, of numbers and economic manifestations on Facebook. I love protocols. That's our love language, this episode. Is that so? I mean, we've talked about EIP 4844, we've talked about proto dank sharding. Those are the same things. We've defined it a handful of times in a number of different capacities. We've never done the aggressive, headfirst dive down the rabbit hole and come out the other side of economics. So, like, we have technically scaled data availability at a technical level, that is a protocol improvement. But how does that connect to the markets side of Ethereum? The two marketplaces, the one marketplace that are now being fractured into two block space and blob space are now two different independent markets that are being contained, each inside of an Ethereum block. What does that mean for ether? What does that mean for the marketplaces that arise around these things? How does the equilibrium of the supply and demand of each push and pull on each other? What does this do for layer two? Scalability. What does this do for economic use cases on top of layer two? S? How does demand for layer one change the demand for layer two? There's so many good stuff at the bottom of this conversation. We're going to start with the basics, the ones that, you know, bank listeners probably know all about 4844, just to lay the groundwork. But then we're going to poke out the other end of the rabbit hole into the economic side of this conversation, which I don't know if many people have even had before, I don't think other than back channels and Harvard DM's and speculation.
Speaker A: Yeah. So going through the implications is perhaps my favorite part. And this is one of my favorite types of episodes that we do because it's both frontier and imminent. I mean, we're just talking months away from, from this upgrade. So it's right around the corner. It's very relevant to our here and now lives. This is not Sci-Fi actually, this is not Sci-Fi Ethereum stuff.
Speaker B: This is like the near term stuff.
Speaker A: Yeah, this is stuff we get to look forward to. So, guys, we're going to get right to the episode with Domothy. But first we disclose. Nothing big to disclose today. Both David and I hold ether. You already know that.
Speaker B: Shocker.
Speaker A: We are long term investors. We're not journalists. We don't do paid content. There's always a link to all bankless disclosures in the show notes. All right, we're going to get right to the episode with Domothy. But before we do, we want to thank our friend and sponsor, Bankless nation. We are excited to introduce you once again to Dom, also known as Domothy. He is a researcher at the Ethereum foundation. He's working on research and development of some key Ethereum upgrades that are coming down the pipe, including EIP 4844. That's the subject of today. Also full dank sharding, and also Mev Burn. We last had Dom on the podcast, in fact, to talk about Mev Burn. And this time we're bringing on to describe this new property that Ethereum will get post EIP 4844. And that is blobspace. Yep, you heard that right. Not block space, blob space. If you've not heard about Blob space, you don't know what this means. This is the episode for you. We're going to take you through the 101 of Blobspace and get all the way to the 400s level. Dom, welcome to Bankless.
Speaker C: Yeah, I'm happy to be here. Thanks for having me on.
Speaker A: All right, so we're going to get into what blobs are blob space, how it's different from block space. And I think we want to do that in kind of three different sections here. The first is history. We're going to talk about how we got here and why rollups are really going first. And then we'll get into the technical of what blob space actually is, and then we can conclude with the economics, what all of this means. I just want to give a quick TLDR at the very beginning of, like, why we're doing this episode, why it's important, and I think there's probably three reasons. Number one, Blobspace is a new resource on Ethereum, okay? And I think we think that this will be as important a resource as block space. That's key insight, number one. Number two, blob space makes roll ups really, really cheap and scalable, and that's going to change everything we know about the Ethereum ecosystem. And number three, blob space is coming like possibly this year. Some estimates are possibly November. Of course, we don't have firm dates from the Ethereum core developers, but that might be consensus. Bet. And once it does, it's going to reshape just about everything, economically, structurally, about Ethereum. So that's why we're doing this episode right now and why it's timely. Uh, Dom, did I say anything incorrect there? Is that. Is that a decent summary?
Speaker C: Yeah, it's pretty much all correct.
Speaker B: Before we get into the. The history, Dom, can I just ask a very simple question? Is to understand blob space. Is blob space just block space for layer twos?
Speaker C: Pretty much, yes. All the data that layer twos need to commit on chain are going to go into these blobs, which is the new resource, as Ryan said, into layer one. And the layer one doesn't know what's inside these blobs. It's just there to prove that layer two committed that so that no one can cheat.
Speaker A: So that's all it is. Blobspace is just block space for layer two s. And right now, layer two s are actually using Ethereum block space instead of blobspace. And what we're doing with this new upgrade, EIP 4844, is we're partitioning off this new resource called Blobspace and we're making that change cheap and very available for rollups. And so now they can start consuming more blob space than they do block space. Is that about right?
Speaker C: Yes.
Speaker B: So Dom, in order to fully understand how we got here, how we got to blob space, I think it's worthwhile going back into memory lane to understand the fullness of the Ethereum roadmap, because it came to a very logical conclusion of blobs and blob space. So maybe we can go back in time, because at one point in time, Ethereum's roll up centric roadmap was not a thing. We had this thing called execution charting, which we never actually got, but we kind of are now. Can you take us back to wherever in the history of Ethereum's roadmap is appropriate to really understand the full context of blob space?
Speaker C: Yeah. So in the research space, it was always assumed that the solution to scaling a blockchain was going to be some form of sharding in one way or another. And back when we were at proof of work, it didn't really work to split the blockchain into 64 or I 1024 or whatever power of two number of mini blockchains, because each blockchain would have a very small fraction of the overall security. So the plan was always proof of stake first, and then we do this magical split into mini blockchains that run in parallel where each validator is randomly shuffled. That was basically the general idea for execution charting, to really flip the trilemma of scalability on its head, where instead of being limited by the lowest requirement node on the network, now you would have more nodes equal more scalability, which is what we're, as we're going to see, this is what we're going to get with EAP 4044 and full dent sharding, that's pretty much the research goal was to get to charting. And then we had more research discovery along the way. But that was the, the earliest design to get sharding.
Speaker A: So Dom, when you say sharding, that's basically just splitting ethereum into different pieces, right?
Speaker B: Or any space.
Speaker C: This is concept that already exists in computer science or database management. Basically it's sharded as in a lot of smaller shards of the blockchain get much more scalability in parallel. So this is the, that was the goal, but it turns out it's pretty hard to do that safely with execution charting. And it was a very debated and area of research until rollups came.
Speaker A: Yeah, I was going to ask you, and this is, by the way, we're going to memory lane. So this is somewhere around 2019, I think just about, yeah, that was around the timeline where we sort of discovered that, oh, sharding. In this way, full execution layer sharding is going to be really hard and it might overly complicate the protocol and it might take us years to get there and we might never be able to get there. And so there was a lot of, like I felt anyway, as not, not an Ethereum researcher, but like an Ethereum, I don't know, advocate user, like investor. It felt a little bit sad and hopeless back in 2019, like we were never going to make it. This technology that we hoped would be possible, scaling blockchains, was never going to happen happen because there were all of these dead ends. Can you describe what the dead ends actually were like? Why was this so difficult?
Speaker C: It was a very big upgrade that was planned, if you remember, around 2019, that was the ETH 2.0 slogan, where the goal was to have one big upgrade with proof of stake, with sharding, with everything along, we were going to.
Speaker B: Go from the stone age of Ethereum to Sci-Fi Ethereum in one single upgrade. We were going to get all the upgrades that we've gotten in the last four years at once.
Speaker C: Yeah. So one big jump and it turns out that's hard to do. There's a lot of complexity involved and it was split up into different phases. And then we said, okay, first we're going to launch a beacon chain, then we're going to figure out how to actually merge it with the current execution layer. And then we're going to do phase one, which is just data sharding. So no execution, just all these smaller blockchains, they're just going to contain data, and then we're going to figure out how to do execution charting. So it was a lot of let's figure it out as we go, but also in a safe way so we don't do something we regret later and then break the whole blockchain because there's a lot of economic activity going on on it.
Speaker B: I want to keep on going and defining execution charting just because once you understand execution sharding, the current roadmap of Ethereum, the roll up centric roadmap of Ethereum is placed into even better context. I think a quote from your article, Dom that you wrote, is that sharding? Execution sharding, the original plan for sharding of Ethereum. I did like how you said that we were going to get to sharding one way or another. We just didn't know how we were going to do it. We knew sharding was the answer. We thought originally Ethereum, the roadmap was like, okay, execution sharding. Turns out that was a dead end, and we picked a different path, still ended up getting to sharding, a different flavor of sharding, which we'll get to. But in the beginning, you wrote this line in your article about execution sharding and the technical details about it. And you said sharding is the shuffling of validators, the Ethereum beacon chain layer one validators that we know today, shuffling validators randomly across distinct shards of the blockchain, each shard essentially being its own mini blockchain running in parallel to the beacon chain, which sounds a little bit like what we have today with roll ups. But the difference here is that the shards of Ethereum are actually a part of the layer one protocol. It's like something that has been in my mind lately is like the relationship that a chain has with its own infrastructure. And so execution charting to me is like state sponsored charts, as in the protocol, the layer one protocol actually determines what the shards are, and that stands in contrast to the role of centric roadmap that we have today. But really, to me, execution sharding is first, we started with 64, with a planned upgrade to 1048 shards. But it was a, the ethereum layer one was going to be all 64 of these shards operated and managed and produced by the Ethereum layer one protocol. And that's where we started. Am I articulating this correct?
Speaker C: Yeah, exactly. So I would say that the way we're getting execution charting this way is more indirect with roll ups and data sharding. But it's kind of like a cheat code from a research perspective, because Ethereum layer one has much fewer things to do and worry about, and then the rest is offloaded to rollups, which is, in my view, better than the original plan. Like you said, state sponsored chars where everything is the same, it's the same blockchain, same EVM, same trade offs about everything which was imposed on users. And now instead of that, you can have roll ups competing against each other to get the best environment, the better trade off. If you personally prefer super speed over super security, then you can go on a different roll up and you have more choices and there's more innovation and competition at layer two.
Speaker A: There's so much, I guess, to unlock here and there's a lot of, I know we're trying to explain blob space at the 101 level, but there's so much, I guess, stacked knowledge that feels almost necessary to explain. Going to this episode. We have a ton of resources at bank list to explain these, these various topics, but one I just want to touch upon is the three different layers of a blockchain and this modular world that Ethereum is in. There's the consensus layer, and this will be a recap for some bankless listeners. But there's the consensus layer, there's the data availability layer, and then there's the execution layer. The consensus layer defines what's true with Ethereum mainnet, by the way, all of three of these layers are kind of combined in the same thing. The consensus layer is arguably what a blockchain should really be focused on doing, figuring out what's true in the most decentralized, corruption resistant way possible.
Speaker B: What's true being the order of the blocks.
Speaker A: Exactly. The order of the blocks, exactly. And the processing of those blocks. The next layer out is the data availability layer. My rough approximation of what that is is what's happened. You've got what's true consensus, what's happened. That's the data layer of a blockchain like Ethereum. And then you have the outside layer, which is kind of where all the activity is. That's where all the action is. That's what's happening right now. That's execution as we define it. And the original Ethereum 1.0, let's call it combined. All of those three things on main chain in just one environment couldn't be parallelized. And now what we're doing with roll ups, the roll up centric roadmap with Ethereum is we are, I guess, sharding out of, I'm resistant to using that word, but maybe increasingly we will use that word sharding out execution from the main chain into these rollups. But the roll ups, in order for them to be still secured fully with the similar security guarantees as Ethereum Mainnet, have to post their data. That is what's happened back to the Ethereum mainnet in order to get the consensus benefits and the data benefits when they do that, it cost money. It cost right now block space, and it costs a lot of money relative to what they're doing, EIP 4844 and what we're calling Proto Dank sharding. The reason for this whole discussion is the economics change post this update in a very roll up favorable way. But anyway, important for us to know that when we're talking about consensus and data layer and execution, all three of those are separate layers in this new Ethereum paradigm that we're moving towards. Dom, David, do you guys have anything to add there?
Speaker C: I would say you got it pretty much correct. Hints this more on a certain point is that data availability right now is more implicit and it boils down to trustless verification. We want everyone to be able to verify the chain by themselves and not have to have a trust me bro a third party in the middle, which right now this is the bottleneck. You need to be able to verify everything, which implicitly means you need to have the data available to you to check the state transitions and execute the transactions so that anyone who sends you money, then you can verify that it's legit and they really did send you that money. And it's not like on some other worthless fork or something.
Speaker A: So let's talk about that for a minute. So the rollups post the Ethereum data availability layer so that it's not a trust me bro type of situation. Exactly what does that mean for a user inside of a roll up? Does it mean they always have a way to withdraw their assets from the roll ups? Or like, what kind of certainty security does that give them?
Speaker C: Yeah, so at its core, we say that rollups inherit the security of l one, which is a very quick tagline for rollups. The way it works is you have the layer one contract that bridges asset between Ethereum and rollups. Ideally, this smart contract can and should in the future all be immutable and not upgradable. Which is why layer one is going to enforce some constraint on what these layer two blockchains can commit to layer one, and so that they can't steal assets. And if they censor, then you can still rely on the layer one contract to enforce withdrawing your asset or forcing a transaction to go through on your tube.
Speaker A: This is why, by the way, I think Dankrad said recently that his definition of an Ethereum roll up is a roll up that posts its data on Ethereum. And if it doesn't do that, then it's something else. It's not an Ethereum roll up. Maybe we use the term validium or something else for that. Do you subscribe to that way of defining an Ethereum roll up?
Speaker C: Dom, he actually said this about layer two, not roll ups. The roll up term is very specific. That is what he means, he says layer two are roll ups and there's no other. There is no layer two thing that is in the roll up. According to Dankrad. He means that as in, like you mentioned, validiums that have data availability off chain, not enforced by Ethereum. And once you leave this Ethereum bubble of security, then you have more trust assumptions regarding the data availability, which leads to a sort of semi trust me bro situation, as in you. If you want to exit your assets, then you need to get the data from somewhere. If it's not Ethereum itself, it's going to be someone else, and you have to rely on them to give you that data.
Speaker B: Not only do you have to rely on them, but you also have to rely on the mechanism, right? So you could post your data on bitcoin if you wanted to, and bitcoin is super trustless, but then you need to trust whoever's facilitating that relationship. Typically we call that a bridge. And so I just wanted to add that nuance there. There's another quote in your article, Dom from italic, that I'm hoping you can kind of put us into the shoes of tall, tall order. I know the quote that you put in there is. It seems very plausible to me that when full execution sharding finally comes, essentially no one will care about it. Everyone will have already adapted to a roll up centric world, whether we like it or not. And by that point it will be easier to continue down that path than try to bring everyone back to the base chain for no clear benefit and a 20 to 100 x reduction in scalability. Basically, we could do the complex thing in scale layer one execution, but all that would really achieve is that roll up sequencers would be like, oh cool, more data, and then barely touch the execution that we took so long to shard. This is Vitalik saying that ethereum layer one was always destined to only ever be for settlements, even if we sharded layer one execution anyways, can you kind of just put us through the thought process of Italic? Why did we know that even if we did do execution charting of the layer one, that we would still ultimately end up at the roll up centric.
Speaker C: Roadmap anyways, the idea is that rollups can afford to do sacrifices that layer one can't do, because layer one enforces constraints on layer two with the bridge contract that I mentioned earlier. So it's something that we can't just achieve the scale and the order of magnitude that rollups can. Even if we shard execution at layer one, the quote about Vitalik is that we're going to spend so much time doing execution sharding, and it's not going to matter because rollups are still going to be much more scalable even if we do scale execution. That also means scaling data. Because like I said earlier, to check execution, you need to have the data. So scaling execution implies scaling data, but not the other way around. So to get back into context, scaling layer one just means it scales layer two exponentially. So whether we want it or not, rollups are here, and they're going to be much more scalable than layer one. So the pragmatic thing to do is to just go down that path and work with rollups rather than try to be some kind of layer one maximalist. That said, no, you stay on my modular, my monolithic chain that I spend so much time sharding, everyone's going to say why it's so much faster on layer two.
Speaker B: Isn't this related to the dynamic I was talking about earlier, where execution sharding is like the state sponsored version of sharding, where every single shard is totally homogenous. They're all the same size, they all go at the same speed. And then what you're saying is like, well, and Vitalik is saying in this quote is like, even if we have that world of execution sharding, they're still too rigid, the shards are still too homogenous. We want more pluralism of core value, of Ethereum. We want more pluralism in our roll ups. And so even if we do execution charting, what we get out of that is still too rigid. We can still get more expressivity, more optionality for rollups anyways, on top of execution charting. So we might as well just lean into that. Am I connecting the right dots here?
Speaker C: Yeah, it's related, but the quote about Vitalik was really about scalability, because even an EVM roll up is going to be much faster than a sharded layer one.
Speaker A: One thing I find interesting about this roll up centric approach, which hasn't been mentioned, but I think kind of implied with what you were saying, David, is the original version of Ethereum was very top down, central planning. It's like the Ethereum protocol is expanding main chain in all of these different directions. That of course has some benefits. But we've had discussed many of the trade offs versus the roll up centric roadmap is very bottom up, like free market developing its vision, going off in many different streams at the Ethereum foundation.
Speaker B: Is not developing rollups.
Speaker A: Yeah, and there's something interesting with the analog that we so often use of, like, nations and how they're built. And the first model is, how much should a government actually do right? It's kind of like an open question in different societies, I feel like, have different answers to this question, but the extreme answer almost never works in any organized society, which is like, the extreme answer is the government should do freaking everything right, like your clothes and run all the companies and do all the banking and do everything okay. And I think the most well functioning societies are those that do just the right amount of, like, goldilocks government. It's kind of like this Goldilocks zone. And of course, you know, there's probably a. An Overton window of what sort of works there. But this was Ethereum trying to figure out how much work the central government should effectively do here. And where it's left off is we're going to focus on settlement, we're going to focus on this consensus layer and then the outer ring, kind of giving a spot for settlement, for cheap data availability, and we're going to let the free market, the private market. All of these experiments run in parallel in the roll up world. And I feel like this is almost like a hidden genius that I didn't, like, I didn't see at the time, 2019, the kind of the pivot from ETH 2.0 felt to me like a crushing blow. I almost felt like a little bit like Fred Wilson, the notable VC who says. Who said at that time, ethereum has failed to execute. I felt a little bit of that, like, oh, okay, so we're giving up on the dream because it's too hard. And all of this, like, research about sharding was for naught. And now I look back at it in 2023, I'm like, holy shit, that was genius. I can't believe we pulled that off. And I don't even know if all of this was intentional or, like, you know, there was foresight or we just kind of got lucky with how this experiment has played out. But the amount of innovation I see in the roll up world and all of these experiments being pursued in parallel, and by the way, funded. Funded adequately, I'm putting it mildly, but funded well by VC's and token incentives and all of these different things have really accelerated Ethereum development to warp speed. Anyway, I don't know if there's anything there, Dom, that you can glom onto and respond to, but that's certainly an observation that I've had recently.
Speaker C: Yeah, it's a very astute observation, but in terms of being intentional on that, I would say it was kind of forced upon Ethereum researchers because many points we've already touched on is execution, charting is complex. And the way we're doing with the sharding plus roll ups, it's kind of a cheat code, but a cheat code, that's going to happen whether we want it or not. And then we just lean into that. And as you said, it turns out that it's one of the best way to scale a blockchain if we want to have everyone in the world to have access to a scalable, trustless environment.
Speaker A: Okay, so that's a little bit of the history and how we got here. And I think, I hope we explained, gave you a tour de force on how we got here to a roll up centric roadmap. Now take us to the current state, if you will, Dom. So right now we do have many roll ups. And right now, these roll ups, these ethereum roll ups, many of them do use Ethereum, I guess all of them, if you take Dankrad's explanation, do use Ethereum for data availability. That means they post kind of the fraud proof type data back on Ethereum mainnet and they consume block space in doing that. It was a few days ago, I checked so that these numbers will be somewhat inaccurate. But I think layer two is, according to ultrasound money, consume about 200 300, say 200 to 500 eth worth of block space on the daily. And this is them paying for data availability back on Ethereum. Tell us about the current state. What's wrong with that? Yeah, that sounds pretty good. We've got roll up centric roadmap. We've got layer two experiments. The fees are fairly cheap across layer two s, I think. I don't know what you guys have seen recently, but on the order of cents for most of them, what's wrong with our current state?
Speaker C: What's wrong with it is that a few cents is still too much for scaling blockchains worldwide. Like Vitalik said years and years ago, the Ethernet Internet of money should not cost $0.05 or $0.50 or the exact quote. And it's something that people make fun of him because of layer one gas fees. So the current state of things is, like you said, we do have roll ups and they do use layer one's data availability layer, which is like an implicit layer that we'll see later. And it's still very expensive because of two things. It's that one called, they use the, the cheapest way to commit data on chain right now is call data, which is about 16 units of gas for every byte that they commit on chain. So it goes into the gas fee market, which can be very expensive, as we see when one NFT drops and then everyone's using the chain, that makes it more expensive for roll ups, gas goes up. The cost of putting the one byte is the 16 gas, and that price goes up too. And it's also limited. The block sizes have to stay limited because of the problem of scaling a blockchain with the cheapest node has to be able to verify the chain. And that's why blocks have to be small. And there's no cheaper way to put data on chain right now for rollouts.
Speaker A: So it's too expensive, it's limited, and we've got this weird kind of resource issue wherever. Yeah, coupling resource coupling issue. That's a great word for it, David. And so what happens is 100,000 users on a roll up are competing with 200 NFT bidders on main chain, and they're all competing economically for the same block space, and it drives the 100,000 users the prices up. So there's this resource coupling problem as well. Those are the three problems with the current state.
Speaker C: Yes. So the resource coupling problem is kind of a weird thing economically, but there's a technical reason, historical reason, why ethereum went to couple everything into a single unit of gas. But it's weird when you think about it, because these 200 NFT bidders are using execution mainly to send money and settle an NFT trade, whereas these roll ups are using data. So it's two completely different resources that shouldn't be linked together. They should have their own supply and demand market. One goes up, the other goes up as well. So that's kind of the problem of the current state with rollups.
Speaker B: So not only are these resources coupled, but what you're saying, Dom, is that these resources are inappropriately coupled, as in they don't have to be coupled. One is using data, the other is primarily using execution. And these are two different resources that just because everything is contained inside of block space, that these things are coupled and inappropriately so yes, that's exactly it. Okay, so Dom, we're going to get to what is about to be my favorite question that is ever about to be asked on the bankless podcast. Are you ready for it? Yeah, Dom, what's a blob?
Speaker C: So you want the technical answer because we already end waved it. Blob is just a piece of data that roll ups can put a onto main chain.
Speaker B: Yeah, I think it's time to get into a technical. Okay, so I'll ask again. What's it technically? What's a blob?
Speaker C: Technically, a blob is 4096 field elements that are just under 32 bytes each. So that's the, that's the. The playground that roll ups have to put data and they have to abide by this format.
Speaker A: What is the difference between a block and a blob?
Speaker C: The blob contains basically everything that trade everything at execution. And I don't know how to explain this. It's like it contains the transactions, and then you execute the transaction, and you see that I sent you one eth, and that changes the state of ethereum layer one. It's like I have one less eth and you have one more, because we executed that transaction on chain, and everyone agrees that this transaction happened before and after some other transactions.
Speaker B: Wait, so we have blocks and then we have blobs, and then we have data of transactions. Transaction data goes into block space. Blobs go into blobs space. And everything is contained in a block, correct?
Speaker C: Yes.
Speaker B: The new thing is this new blob space. So we have block space. And I think people might get. And I'm getting confused, but I'm trying to parse this apart, that we have blocks and we have block space and we have blob space. But blob space and block space are spiritually equivalent, and both of them are going into blocks. And so there's this weird dynamic where block space and blocks have the same name, the same block, but blob space and block space are shoulder to shoulder with each other, equally contained by a block.
Speaker C: Yeah, so we're getting into metaphor. Land on the consensus layer. What we say is that these blobs are some sort of sidecar alongside the block, so the block can stay small. And then there's the big blobs next to it that are not exactly contained in the block, but the block contains a reference to the data, which is what rollups are going to use to do all sorts of zero knowledge, magic and that sort of stuff.
Speaker A: I think one of the first learnings here, the first takeaways in this section, because it's going to take us a few minutes to really understand what the heck a blob is and what the heck blob, blob space is. But you know that thing that we say on bank lists so often is blockchain. Sell blocks. Well, I think, David, we have to amend that. Blockchains sell blocks and blobs. That this is a new resource, I think, is the key insight that Ethereum is kind of going to market with, if you will, after EIP 4844, which again could happen this year. It's not only selling block space to the market, it's also selling blobspace. And Blobspace is a product tailor made for rollups. What does it do for rollups? It gives them a space to park all of their beautiful fraud proof data. A data availability layer is so beautiful, and it's incredibly cheaply, so it's a much more efficient resource to get done what roll ups need to get done, which is post these proofs into Ethereum, I guess block space, but it does it. Ethereum is now in the blob business, baby.
Speaker B: Dom, you said that a block references a blob, but that a blob isn't in a block. Is that correct? And can you elaborate?
Speaker C: Yeah, that's the metaphor. The blob is the sidecar that's alongside the block. And then basically one key insight is that, like I said earlier, if I send one ETH to you on layer one, every single node has to compute the transaction to verify my signature. Verify, access the state of your account and subtract a number from me, add a number for your account. And that's the very expensive thing. That's kind of the bottleneck that doesn't scale very well. And the way that rollup scaled this way is that I send you one eth on a roll up and that's cool. The roll up is going to batch that alongside a lot of other transaction onto layer one inside of a blob. And now layer one doesn't care about the data inside that blob. That's the roll ups business. I send you one eat. That doesn't impact layer one nodes. They're just going to see the data, say, ok, the data is there, it's cold, everything on the roll up is happening, it's legit. But there's no expensive computation at layer one happening for layer two transactions other than this very small. The transaction to verify a proof or update optimism state route every time a sequencer commits a transaction. So that's why the biggest resource they need is data, and it's how we scale the whole blockchain.
Speaker B: Right? Okay, so the way that I understand, I give a metaphor to blockspace is that block space is like a container of data. And if I send Ryan some ether, that's a very small bit of data that I'll throw into blockspace, start to fill it up. If I send Ryan an NFT, that's an even bigger bit of data. If I mint 13 nfts at once, that's an even bigger amount of data. And that fills up block space. Block space is like this bucket that you fill with different sizes of transactions. Eventually you become full. Is blob space like that, as in there's a container that is filled, or does it operate with different properties?
Speaker C: It's similar, but it's a decoupled market too. So there's this other different bucket that only gets filled with data. Like the layer one doesn't care. It's very agnostic about how who posts what into these blobs. All it cares about. It's getting these blobs out there for whoever needs them to commit to them and download the blob content. This is what layer one enforces.
Speaker B: Okay, so post EIP 4844, when we introduce blob space, aggregate Ethereum blocks, the size, the data size of aggregate Ethereum blocks will be blobspace plus block space, correct?
Speaker C: Yes.
Speaker A: Okay, let's look at this through the different lenses of actors and stakeholders in the ethereum ecosystem. So I want to go through validators in a second, but let's first start with rollups. Why is blobspace a better product than blockspace for rollups? Is it just because it's cheaper? Or what other properties does it have that makes it appealing and better for rollups to consume?
Speaker C: From the point of view of a roll up sequencer, it really boils down to being much cheaper and much more plentiful. Layer one is going to do more technical stuff to scale this blob space, but from the point of view of a roll up, it doesn't really matter. It's just data for them. That's all they need and that's all they care about.
Speaker A: And how do they sort of activate it? Is there anything that they need to do on their side? Is it basically like, so all of these proofs that they were posting and buying block space in order to prove they just switch over? So it's just like, rather than using, I don't know, electricity for your furnace, you're using now, natural gas, you just use a different resource because natural gas is so cheap. But the system works the same way. You still get your heat. That's what they're doing here. Yes.
Speaker C: Yeah, exactly. So after 4844, they'll have to update to support these blobs and stop posting everything on layer one in the expensive call data section of blocks.
Speaker A: Is there anything that they'll lose by posting doing this via blobspace versus block space? Or is it just basically an equivalent substitutionary good? It's just as good for roll ups.
Speaker C: It's just as good and even better for them because it's cheaper and more plentiful.
Speaker B: Can I put a transaction in blobspace? Can I send Ryan some ether in a blob? Or what prevents me as a layer one user from consuming blobspace?
Speaker C: Nothing prevents you. It's a permissionless system. It's tailored for rollups. But of course you can put any data inside layer one blobs as you please. But it's up to you if you want to pay for that.
Speaker A: And I'd be more expensive. Maybe a bad idea.
Speaker C: Yeah, I'm predicting that there will be some NFT project where the jpegs are inside blobs at first because blobs are going to be pretty cheap.
Speaker B: Wait, okay, so there's a phenomenon in the NFT world about like nfts that are on chain like cryptopunks and mfers and a few other NFT projects. What's the famous one, the generative one that auto glyphs are like the biggest on chain art? Does blob space just simply allow for more? Perhaps as one use case of blob space, of which I'm sure there's infinity, that we can just put more nfts, more jpegs on chain?
Speaker C: Yeah, I'm pretty think there will be a lot of Degen stuff happening with blob space because it's going to be very cheap at first.
Speaker A: Oh no, we don't want that though. We want the blob space for our roll up transactions. We don't want to create another Degen market. Not the point of resources decoupling.
Speaker C: Yeah, it's decoupled because if they want to put data on chain, we can't stop them. Right. They could just pretend to be a roll up and we can't. Yeah, we can't be top down about who gets to use what from a credible neutrality at layer one. And I don't think there's a way to enforce that to that. To enforce that only roll up sequencers use blob space.
Speaker A: But probably the properties of blob space are such that they are most conducive to, well, it's like, isn't there, so one example, Dom, isn't there like an expiry on the blobspace? And this brings us to we'll talk about the expiry. But this brings us to the second lens I wanted to talk about, which is from a validator's perspective or a staker's perspective, like I'm running a node and I'm staking, does this, the introduction of blobspace change anything for me? Does it increase the requirements of the validating machine that I have to run? Do I now have to store, in addition to all of this block space, do I now have to store a blob space? Maybe those questions are related.
Speaker C: Yeah. So with 4844, it's going to go up a little bit. You have to download and serve these blobs all the data that's inside the blob, but you don't have to store it forever. So that's what you mentioned. Blobs will expire after, right now the specs say about 18 days, I believe, and after that it becomes like it was attested that it was available during those 18 days. But if it expires, then you're not going to be able to get it from a node, if the node pruned it. So you have to get it from some other source, and then you'll be able to see that it was available on chain during that time.
Speaker A: So this might be a reason why it's not as conducive as block spaces to an on chain NFT, although there may be ways around this. But why is this 18 days not a problem? So why isn't that a problem for layer two is like if the data goes away after 18 days, aren't I like, yeah.
Speaker B: Wait, pruning data away from blockchains is blasphemy. How dare you?
Speaker C: Dos.
Speaker A: Yes. How could you?
Speaker C: So this is kind of the weird thing with the name data availability, because it's not data storage and it's not continued data availability forever. So it's kind of the opposite. There's a big problem with storing everything by every node forever that doesn't really scale. If you were to pay once to get your NFT JPEG on chain, and then it's stored by everyone forever for all of eternity, with the data just going up and up only forever. So instead we use Blob expiry to just ensure that the data was available once it was published and available to be downloaded by everyone. And this is also from the lens of rollups, is that if you're 100 years from now and you want to sync the optimism state from scratch, you'll have to get the blob data from somewhere. But then once you have the data, nobody can lie to you and say, oh, I 100 years ago I had 1000 e that came from nowhere. And then here's the data, and then you can check that data, and you can just check the integrity of that data, but check that its availability was enforced by Ethereum long enough for anyone to snitch on an invalid transaction on optimism. So you'll have these properties of being able to verify the data yourself. It's just that Ethereum is not the one that's going to serve you that data forever for free.
Speaker B: Okay, so data availability, in contrast to data storage availability, as in Ethereum, is making this commitment to the world around it, that it is making data, it is enforcing. That data is sufficiently available for the surrounding universe to be able to take and do what they need with that data in order to do whatever they want to do. And we've chosen 18 days as some time length. That data is being made available to individuals, to node operators, to other layer two infrastructure providers, to other protocols like Filecoin or data availability solutions like Celestia, anyone to take the data that Ethereum has made available to the world. Hence data availability for 18 days. And then that data has gone from Ethereum to some more long term storage solution, which there are so many of, it's not even worthwhile for me to list them off. But we can start with data availability solutions like Celestia, eigen layer, data availability, something else, something else, your hard drives, like roll up operators themselves. And so we're just making this statement that so long as we can say that Ethereum enforces data availability of all blob data for 18 days, then the universe is going to be able to do what they need to do with that data in order to have a fully trustless this chain of data that goes back to Genesis. Is this correct?
Speaker C: Yes. Yes. So the goal is to really have lightweight nodes be able to verify everything, including the availability of what's been published in a scalable way. And also, another thing to point out is that this blob data is not part of the state where you need to read and write to it many, many times a second. You can just store it on a cheap hard drive. If you're like a hobbyist or any stakeholder in a specific roll up, you can just download the data you need from the roll up you're using, and then you can just store it for pretty cheap on hard drives. And data storage is just a thing that's continuously getting cheaper and cheaper over time.
Speaker B: Right. Notably, terabytes of data are under $100 right now. So maybe if I'm. Maybe we can theorize that in the future, there will be some software application that you run on your desktop computer, and you link it to your main addresses, and then it follows your addresses around some pre selected layer twos, like optimism and zksync and arbitrum. And then it looks at your addresses and then automatically downloads all of the blob data for your addresses that you've specified. And then it just stores that on your personal computer, and then you can be the one that verifies the history of blob blob space. In addition to the many other data storage solutions that may also be downloading and saving the same data. Is this perhaps a version of the future?
Speaker C: Yes. And also rollups can have their own designs to incentivize storage of their own data if it's something that's very important to that particular roll up in its community. So the design space is interesting.
Speaker B: So in addition to pushing execution out to the free market and making layer two teams optimized for execution, we're also just pushing data storage out to the free market as well.
Speaker C: Yes. So one metaphor that I've used, Vitalik uses that Ethereum is supposed to be more of like a billboard, where you can have information in real time about what's on the billboard, but once it's erased, all you can do is verify that this information wasn't on the billboard, available to everyone to see back when it was published.
Speaker A: Yeah, that's kind of the image I get from Ethereum as well. It's kind of like a present moment of time type of consensus mechanism. It's not trying to store the full consensus truth of the universe, the full computer. It's more like kind of like a.
Speaker B: Hurricane or just kind of like tip of the chain.
Speaker A: Yeah, it twirls around. And so that is this distinction between data availability and data storage. So Ethereum is trying to be a data availability computer, but not a data storage computer. And so this 18 days is also significant because it does have to be like, there does have to be some reasonable time period here, right? Like, it can't be like 15 minutes or like 2 hours or even 24 hours, because there are things like, with roll ups, right? We have kind of like optimistic roll ups, kind of a seven day proof challenge type window, right. And that's why it has to be greater than seven days or just 18 days feels like an okay time horizons, but it shouldn't be like six months, because that gets into data storage and it shouldn't be an hour because that's not enough time for all of the data storage and fraud proof type solutions to react. Is that accurate as well?
Speaker C: Yes. It basically just has to be long enough for anyone who needs that data to download it. That's why I said earlier, it's kind of a cheat code for execution charting because then you have these roll ups posting blobs, and if it's a roll up you don't care about, then you just don't download the blob. You're going to see that it's there for, you're going to as a layer one validator in layer one node, you're going to participate in securing that roll up. But even if you don't care about it, you don't download that blob. And that's, it's like the original plan with these mini blockchains where you would only care about the blockchain that has your fund, like one of the shards that have your fund and you don't care about the others. But this is even better because then the security is not split around these mini blockchains the same way it would have been with the original sharding plan.
Speaker A: Just to tie off this question, you said the requirements for a validator do increase a little bit.
Speaker C: Like what are we talking about with proto Dank sharding? I did the math in my blog post. With the current size of a blob and the target number of blobs every block, we're looking about 50gb extra storage by every node. But this is for 4844 after full denying it's going to go down somehow, blobs are going to be bigger and the requirements are going to be lower for every node, which is the magical sharing.
Speaker A: The goal of ethereum in general is to be able to run a validator on consumer grade hardware. Hardware, right. And we meet that goal right now. Like you can run a validator on a raspberry PI. And what you're saying is this blob space changes the hardware requirements by about 50 gigs of hard drive space. So doable, I think, you know, on most consumer hard drive rigs. So it's not, you know, it's, it's not nothing, but I mean, 50 gigs, like you can get, you know, thumb drives for like, I don't know, $10 that are 50 gigs these days. I. Okay, Dom, there's two other lenses I just want to put this through. One is developers, which I think a lot of the development. A lot of the app building will kind of migrate to layer twos. And probably the obvious answer. The good news for them is your application can support cheaper, more transactions per second because they'll be a whole lot cheaper. So go have fun. That's probably the answer for developers. How about users? Is there anything that they like? How will they experience this new blob space world? This post 4844 I know David's going to try to send me some eth using blobspace somehow. I don't know if they'll be able to do that, but maybe the typical user.
Speaker C: Not truly your wife.
Speaker A: Yeah. How will they be able to do it?
Speaker C: The typical user? Hopefully all the technical stuff is abstracted away from them. It's more the heavy users who really insist on having trustless verification of everything to secure their funds themselves with their own keys. And this is what blobspace allows them to do. They just keep that data, only the data relevant to them about their funds, where it's stored on the state, how to move it, stuff like that. So the overall trustless and permissionless is not sacrificed from a blockchain point of.
Speaker A: View, yeah, I guess this will just accelerate the migration from Mainnet to roll ups as well. I'm sure we could probably predict that this economic change will result in that.
Speaker C: You quickly mentioned developers. I would also like to add that right now, developing on layer zero one is kind of a weird thing, because every block space is so limited that as a developer you have to do all sorts of fancy tricks to use less gas for the same transaction. But on layer two, you're going to have so much more block space of layer two blockchains and roll ups that you're not going to have to worry about these fancy tricks to use less gas, because you're just going to be way more bandwidth for layer two.
Speaker A: Dom, you just said as a developer that block space, and you use that term block space, and I think that's because for layer twos. So layer twos basically are almost like a value added reseller of blob space. So they take this blob space, it's now a lot cheaper and subsidized by Ethereum, and they convert that to block space in their own layer two, and then they sell that to devs and applications.
Speaker B: Oh, layer two has turned blob space into block space.
Speaker C: That's the tagline of the roll up centric world, is that it's much easier to scale data availability on layer one. And roll ups are going to take that data and convert it to scalable execution. So scale one, you scale the other for much easier.
Speaker A: It's just like, you know, unrefined oil that you then you then process and turn into like a petrol or something like this.
Speaker B: Amazing. That's a great metaphor.
Speaker C: So as a developer on layer two, you have much more freedom to do whatever you want, however you want. And it's something that kind of goes under the radar with layer two execution is that you can only batch the state transitions onto layer one. So if I do a very, very complex execution at layer two, that sends one ease to like, a thousand people, and then, I don't know, loops around and buys a bunch of nfts and trades them somewhere, buys collateral, you can think of this hyper complex transaction, and then that goes back, that patches onto layer one, only the state transition. So people don't need to verify the whole execution of the transaction. They only need to verify the output, the actual outcome of the transaction. And that's much more scalable. And it's pretty cool how more complex transactions at layer two become cheaper compared to the same transaction at layer one. It's not the same ratio of scalability for each transaction, and a more complex transaction has way more savings. So as developers on layer two, there's way more use cases and way more freedom that become available to do all sorts of things that we aren't even conceiving of right now.
Speaker B: God, computers are so cool. Okay, Dom, let's do our best to get into probably what's about to be the most technical part of this conversation, which is actually, how does a blob become a blob? There's some crazy math involved here. There's things like polynomial commitments that make me scared. How does a blob become a blob? How do we start this conversation?
Speaker C: It's all polynomial magic. Basically, you have these 4096 elements that contain your data of your blob.
Speaker B: Sorry, what's an element?
Speaker C: A field element? It's basically a number, but inside of modular arithmetic. I don't know how technical you want me to be about this.
Speaker A: I have a feeling we're getting to the firmware level of technicality.
Speaker C: Yeah, this is the stuff that everyone on layer two doesn't need to care about, including developers and roll up sequencers. All they do is send the data to layer one, and then layer one transforms it into this big polynomial equation that if you have a bunch of data, you put them inside these things, and then you interpolate into a polynomial. So it's kind of like, you know how two points make a line, then you can just treat your two point data set as a line equation, and then suddenly this line is extended to infinity on both sides. So that's what we're doing, but with 496 numbers instead of just two. So it's going to be a big degree polynomial.
Speaker B: I think at the end of this technical conversation, the mic drop punchline is that this is what sharding is, and then there's a bunch of middle ground question marks, math steps that I don't understand, but the end result is like, oh, the execution sharding that we originally planned for in the roadmap. Now we have data sharding, and it's done with these polynomial commitments. And ultimately you have more data that is actually contained by a much smaller amount of data that, like how you said with this extending the line, this smaller amount of data contains references to every other bit of data that is in the blob. And once you quote, unquote, extend the line, you can like, unfold the packet of data to create the full amount of data that is the blob. That's my summary of this. Do you want to add any more of that?
Speaker C: There's more polynomial magic to reconstruct the whole data from just a portion of it, but also to check its availability, which is the crux of Deng sharding. You'd want nodes to be able to check that data availability. That data was actually published by a block producers and a roll up sequencer. You can check that the roll up sequencer was legit and posted the data on chain, but without downloading that data yourself. So you're not suffering that burden from trying to enforce the availability of data. This is what the polynomial magic.
Speaker A: So really quick here, Dom. So you just mentioned data availability sampling, I believe, and this is a future upgrade that is beyond EIP 4844. So EIP 4844, another synonym for that is proto dank sharding. And the proto means like, I guess, pre, it's first, right?
Speaker C: First it comes from proto lambda.
Speaker A: Proto lambda, that's right. Never mind, I forgot about that. So proto land lambda, it's also implies maybe it's first. We do that first and then the data availability sampling you were just discussing, which also uses some magic polynomial math that comes later with full dank sharding, and the benefit there with full dank sharding, later, TBD, we don't have any dates on this. Like think years, not months. Okay, so, and that the benefit that, that gives us with full denk sharding is. And with data availability sampling, full dent sharding is. Is what the, the validator clients, um, just hold less data so that 50 gigs that you were talking about earlier, that drops down. Or, like, what does, what properties does full day sharding give us a. Yeah.
Speaker C: So 4844 sets the stage for full deng with all the polynomial stuff. But also, like I said earlier, Ethereum right now has this kind of a data availability layer in the form of block space and call data, but it's not scalable because every node needs to download everything, which makes the bottleneck. So, to answer your question, the data availability sampling aspect of full denk sharding will enable this data availability layer to become much more explicit, where even a full majority, like a super majority of validators, can't fool you into believing that an unavailable block is available. And this is what your node is going to sample. Just a tiny amount of data to be sure, with, like, a one in a trillion probability that the data is actually there.
Speaker A: Okay, so, Dom, just again, high level, nothing in the weeds because there's so many things we could explain here, but high level, this branch of magic, polynomial math, which enables proto dank sharding and full dank sharding, this feels like a free lunch to me. It's like, wow, here you go with some math. We just put some math on it, and we get scalability. Is this new stuff? Is this, is this branch related to cryptography at all? Because we're used to exploring on bank lists, of course, Azkay snarks. And this whole new branch, this relatively new, decade old branch of cryptography that's completely revolutionized everything about blockchains and how we scale them, what we do on them. But this, you're talking polynomials. I learned some polynomial stuff in algebra in high school. Is this a new branch of math, or is this stuff that we've used all over the place and are just now applying to Ethereum and blockchains just because we figured out a clever way to do it?
Speaker C: Yeah, I would say it's a combination of many things that we already knew inside cryptography. So, like erasure coding and data reconstruction and polynomial commitments. And this is basically, it's new in the way that bitcoin was new, right? Because Satoshi did not invent proof of work. It did not invent cryptography. Cryptographic signature, like the novel thing was combining these things together and getting consensus. So it's kind of like that from a research perspective. I'm not saying that data will be sampling is as novel as bitcoin was in 2000.
Speaker A: But like this stuff like data availability, sampling and ratio coding, that sort of thing. I mean that's existed for a while with just like hard drives, right? Am I right? Like Meg, like the old style of hard drives, when you put them in raid arrays, that sort of thing, you use some of this. Yeah.
Speaker C: You're talking about redundancy.
Speaker A: Yeah, redundancy, exactly.
Speaker C: Yeah. So that's kind of a, it's an old concept. I know raid doesn't really do it with polynomial, but it's kind of the same idea where if you lose a portion of the data, you can reconstruct it. So of course this part is not new or novel, but applied to blockchains, we can combine all these elements together that we already knew into this data availability sampling to solve the problem of data availability in a scalable way, because right now it's not scalable. To confirm that a block is available to the network, your node has to download the whole block. And that's why it's kind of implicit where it's like, of course I need the block to verify the signatures and the transactions and everything. So that's an implicit step. You need to download the block. But now we're really thinking about it. How do we solve the problem of checking if the block and the blobs are all available without having to download the whole blocks and all the blobs? And this is what full dank sharding will solve.
Speaker B: Dom, are you able to put numbers on the scalability that proto dank sharding? And then full dank sharding enhances? Like how much more scalability, is that a valid question? Can that be measured?
Speaker C: It's very speculative, I would say right now people are throwing numbers around like ten to 100 x with just 4844. And then with Deng sharding we're talking. I know Vitalik throws like a hundred thousand DP's, but it's, it's a bunch of speculative and weird metrics because a transaction can be anything. And the more complex one get more scalability, like I said earlier. So I don't really have any numbers, but with four and 44 we're gonna get those numbers and something I'm very excited for. Right.
Speaker B: It's impossible to really measure these things. Like a transaction is a different thing depending on different contexts. But I think the point that I was trying to get out of you is that there's a number of zeros. It's a number of zeros. Of scalability increases depending on how you measure it. And there's this one little part about full dank sharding that I think is actually just a really emblematic about the balance that Ethereum takes between hardline trustlessness and pragmatism. And that is the number that you said earlier. I don't know, maybe you just threw it out there, but it is spiritually aligned with this number. It's like 1 trillion and in full deng sharding. We do this data availability sampling mechanism where you sample a data, and if you are somebody who's trying to produce a fraudulent block or hide data, there is at most, after one sample, a 50% chance that that is a fraudulent block. And then you take another sample of the data and you cut that in half, and then you cut it in half, so it goes down to 25%, 12.5%, 8.275%, whatever. And you do that like, I think, 30 times, and you get to one 1,000,000,000th odds that this block is fraudulent. And at some point, we like Ethereum in dank charting, pick this number, it's like, okay, we'll do 30 samples, and then that will give us a one in 1,000,000,000th chance that this block is fraudulent, and we will accept that probability. We are not so crazy hardliners that we won't trade off multiple orders of magnitude of scale increases when we are giving up a 1,000,000,000th probability that this single block in the blockchain has fraudulent data. That is a trade off that Ethereum is making multiple orders of magnitude of scalability increase for a one 1,000,000,000th probability that somebody hid data inside of a block. I think that's just like such an elegant way of articulating Ethereum's values and the way that cryptography allows us to do cool things by compressing the bad and magnifying the good. I just think it's so elegant, it.
Speaker C: Ties in with the concept of weak subjectivity, right. Because with blob aspiring, we're kind of losing that aspect that a caveman can just go hibernate in a cave for a thousand years and then come back out and then verify the whole blockchain from Genesis. That's something that we already kind of sacrificed with proof of stake with the weak subjectivity. But it's such a small trade off compared to the scalability it enables and everything. And, yeah, compared to what you said, I would say we're probably going to do more than 30 samples with full denk sharding. So it's going to be even lower probability that you believe an invalid block is valid and available. But also this is the probability that your node personally gets fooled like you need. There's no way you can fool the entire network, right? With all these probabilities, even a supermajority can't convince the network that unavailable blobs are available and then do nasty stuff at layer two.
Speaker A: You know, David gave a shout out to computers earlier. I just want to shout out math right now because, yeah, this is great. Like the statistics, the math behind this is absolutely fantastic. I do want to just ask a general question here, Dom, and order of magnitude is fine, right? But just this will lead us into the next section. So we talked about the history, we talked about the technical, and I think we have a sufficient definition of what blob space is. At least I feel like I know blob space now. I know what it is. Now we want to talk about the economics of blob space and this decoupling of the market. But before we do, I do want to get some rough approximation of what David was saying based on the scaling factor. If I go to l two beat scaling activity, there's this chart here that shows me the current scaling factor of layer twos. And it gives an approximation, and it says layer twos are operating at their activities about five x mainnet right now, and that's useful for me. And this is five x mainnet. And right now, this world, they're consuming a very small amount of block space, aren't they, per day of Ethereum. And they could be consuming a lot more block space. And if we had the activities to support it, layer twos could be much higher than a scaling factor of five. I don't know if that's like ten or 50 or 100, I'm not really sure. But then when they have this new blob space resource available to them, then they have this whole other scaling factor. And as you and David were saying, it totally depends on what they do inside of that blob space. So they're going to take the blob space, they're going to resell it as block space. And the block space that they resell could be very complicated transactions, or it could be very simple transactions. But rough order of magnitude here, right? If we have blob space, and it's almost like, let's say it's 80% used, this blob space is now used post EIP 4844. How much of a scaling factor are we talking for, like this type of typical defi type transactions? Are we going to be able to turn this like five x into a 50? Are we talking about like a 500 x Ethereum mainnet? And again, just roughenous approximations here.
Speaker C: Well, the amount of scalability really depends on the users. So rollups could update to 4844 and then have the same amount of users and the same five point something X on l two beat would stay the same. The difference would be that it would be much cheaper for each layer two users, to the point where it's practically free. So like you said, if the 80% of the blob space is used, then the price of blobs goes to zero, right? Because it's anything below 100% the price. It's like EIP 1559. If we're above 100% of gas target usage, then the price goes up and down to manage congestion. So the prediction would be that at first, blobs are going to be mostly empty and only used scarcely by rollups because they don't have the actual user base to fill them up. So we're going to see rollups operate practically for free, with just a tiny amount of execution gas at layer one and some other expenses for rollups, whatever expenses they have. But then that layer two transaction can be subsidized, and then you can have a world where layer two is basically free for a while, until there's enough users to fill that blob space and congest it and make it go up in price. So that's kind of the economics of 4844 in a speculative prediction of mine. And that's one of the things I'm really excited to see is raw data and how it's actually used.
Speaker A: So, Dom, let's spend more time defining the economics of this. And this is a quote from your article, and we've alluded to this earlier in the episode, but you said this. Another fun aspect of EIP 4844 is the introduction of the two dimensional fee market, meaning execution in blobs will be priced separately according to the individual demand for each. The price of execution is simply the gas fees we know today with EIP 1559 and all that good stuff. So can you explain what you mean here? So this is a two dimensional fee market. There's like two types of gas. Is that one way to think about this? And then you mentioned the hallow EIP of 1559. So how is that related? What does that imply about the gas market for blob space.
Speaker C: So it's a two dimensional market in that these two resources are going to be priced separately. So one concrete aspect is that the famous NFT drop at layer one example, if it happens, then the roll ups are shielded from that because the price of blobs that they come in on chain is going to stay the same relative to the demand of other roll ups. Whereas layer one execution gas can be very expensive. That's a cool aspect, that layer two users are shielded from whatever degen activity happening at layer one.
Speaker B: Right. We'll use the word decoupling now. We're originally coupled markets. Now we are decoupling the markets.
Speaker C: Exactly. So the high level overview of 1559 that I'm sure listeners know very well is that there's a target of gas used per block, and if it goes above that target, the price of gas goes up for the next block, and then it goes down if there's below the target. And that's the goal is to manage congestion. Right. If there's too much demand for gas, the price goes up and it makes a more efficient auction than we had before. And then that base fee gets burned because you don't want validators to be able to manipulate it to their own benefit, to have, like, high gas fees and reap the rewards. And we get the same kind of market for blobs, but it's a completely different market. So we, right now, the specs say that we target three blobs per block, but we can handle six. And if it's six, then the price is going to go up on the next block. And it's that same idea. Like the price goes up exponentially if there's all the blocks always filled with six blobs. And by the way, these numbers might change because it's still a discussion in progress. And that's the blob market. We're going to have a separate EIP 1459 and with another base fee for each blob gas being used, which also gets burned for the same reason.
Speaker B: Okay, so both gas fees to pay for block space, aka the status quo, that's burned. We know this blob space is also burned in the same mechanism. It's basically one to one parity with block space, correct?
Speaker C: Yes. Just the price function is a bit different for blob space, but that's technical stuff.
Speaker B: Yeah. You gave the numbers three. Three is the target. So we target three blobs. Blobs are all uniform size. Right? One blob is always the same size as another blob.
Speaker C: Yes. But if a roll up has more data, they can post two blobs in the same transaction if they want. So the data requirements for rollup, they can use as many blobs as they want. They just have to pay for it like the rest of other rollups.
Speaker B: Okay, so in post 4844 in Ethereum block space will. A block will have space for three blobs. So when we talk about blob space, we're actually just talking about three slots for blobs, but with tolerance to flex up and down.
Speaker C: Yes. So that's where the 50 gigabyte number comes from. If there were six blobs every single block, then that number would be 100gb. But that's completely unsustainable. If you have six blobs for every block for 18 days, then the price of blobs just goes up exponentially. And at some point, there's just not enough eth in the world to pay for those blobs with that high base fee. So the target is going to be reached at, on average, three blobs per block. But that's to manage congestion. But if there is no congestion, then you have one blob or zero blob in a block, then that price goes down to basically zero. So that's where at first, when there's no congestion, blobs will be practically free.
Speaker B: So one perspective about this is, oh, we have a brand new resource called blobspace, and it also burns eth. There's another mechanism, a brand new mechanism to burn eth. Yay. We're going to burn more ethnic. But actually, I don't think that's true, because the existence of blob space is likely going to pull away demand for block space, because why are we doing this in the first place? We're trying to make roll ups cheaper. And so there will be a tension, a balance between block space and blob space. They won't be formally coupled in the protocol, technically, because we are decoupling this block space, but they will be coupled because if it is so much cheaper to do whatever you're doing on layer one, on layer two, then transaction demand is going to flow out of the layer one towards the layer two, and then rebalance the demand between blob space and block space. So these are like informally coupled. If it is so much cheaper to buy blob space than it is to buy block space, then these things will probably balance out a little bit. But in aggregate, I think total burn will come down because we are encouraging incentivizing layer two activity, which is fundamentally cheaper than layer one activity. This is my intuition. Is this right, Dom?
Speaker A: Yeah.
Speaker C: In aggregate, it's going to go down at first, and then as layer two hopefully gains more adoption and more users, then it's going to go back up eventually with way more users doing like half a penny or thousands of a penny per transaction on layer two in aggregate, if that's enough to fill the blobs, and then the roll ups have enough fees from these many, many thousands of a penny transactions, they can afford to pay for high gas prices for blobs once that's congested. But right now, I don't believe blob space is going to be congested at first with that level of activity, which is why it's very alleviating. Just three blobs per block is going to be very alleviating for rollups needs. But we're going to lose 200 or 300 ETH a day that Ryan said earlier. Like, that's probably going to go way down.
Speaker A: So we're going to drop towards zero.
Speaker C: We're going to burn slightly fewer etH at first, but it's not really a problem because if we're getting much more adoption from that and scalability, then that's a win, right?
Speaker B: The line here was like, we're just going to make it up in volume, so where things are going to drop to zero because we're reducing fees to layer twos, but then layer two fees also drop to zero. And then all of a sudden we're like, we can take off the brakes of layer two economic demand and pick it up in volume. And so this is Ethereum opening up its layer twos to the long tail of economic use cases that can approach 0.001 pennies per transaction because we've enabled block space, blobspace, excuse me.
Speaker C: And like I said earlier, rollups can have their own incentives and pay for these fractions of a penny for each users. And then you have a layer two experience where it's completely free and you can just do whatever you want. And more complex dapps can now be viable because there's basically no fees. And that sort of activity at first, that's going to be incentivized by these effectively zero transactions. That's going to drive more users into layer two and that's going to fill up the blobs eventually, and then at some point it's going to start burning.
Speaker A: So this is the part that I think is like, at least for me, one of the, all of this has been a fascinating conversation, very interesting, but one of the least explored areas, right? So I think some people listening will just maybe early in the podcast or up to this point be like WTF? There's going to be a whole new resource market. How does that impact demand? There are these ideas, and some people still think this, that layer twos will compete against Ethereum and kind of the layer one. This is a whole new variable. I guess I'm saying that I'm not sure many have looked at when it comes to how do you try to predict the future price of ETH and the future demand for block space. What's effectively happening here though is we get a new furnace. We've had the furnace one of just block space demand, and that burns some ETH. Then we have the second furnace firing up. In the short run, this will be all of the layer two block space consumption will drop down towards zero. In the longer run, we may end up burning more ETH, and we probably will, but that will take some time. I don't know if that'll take months or that'll take years. In fact, it's probably impossible for us all to predict here. But one of the side effects, I think, is it depends how you model out the value of ETH and where that sort of comes from, right? Because there's certainly some value of ETH accrual that is related and correlated to kind of burn, how much block space is consumed, how much blob space is consumed, how much ETH is burnt, that sort of level. But then you also have to think about in the layer two world, all of these new DeFi applications, or in general applications that open up based on cheap block space, how much eTh will they actually consume? As what we've termed before, economic bandwidth or ETH based collateral. You look at an app like friend tech on base and it's purely denominated in ether. That has been a net accretion point for ether value. The question, I think becomes, with these new layer twos opening up, how much more do they start to use Ethereum's monetary properties and Etha's money and ETH as collateral and ETH as a store of value and all of these things? And does that compensate for the short term reduction in burn? It's a very complicated kind of economic model, but those are the puts and takes of this one. Question I have for you though, Dom, is you mentioned this use case of later twos will no longer have to compete against the degen NFT drop on Mainnet. What would a world look like where other layer twos, though, are competing against other layer twos for blob space. So that would have some contention and resources. Can you imagine a world where we are not 80% of blob space consumed, but we're doing the full three blobs per block thing? We're getting into four and we're getting into five and getting into six. Does this mean I like an arbitrum? Layer two competes against an optimism. Layer two. I'm not even sure how to imagine this world. What does that look like?
Speaker C: That's pretty much exactly what you said. They're going to compete each other. You can think of rollups taking small pennies from a lot of users for transaction fees on layer two. And that's enough to pay for layer one blobs. And whatever is left over is the roll ups profit. If the blobs just go up, then layer two becomes, I don't know the word. They're going to lose money. If the blobs are too expensive for them, then the most efficient roll up that can have more activity can batch more transaction, can compress better. They're going to make more use of that more expensive blob. That's a world where we're heading to where if block space becomes congested, the best roll up that uses this more efficiently is going to be able to provide cheaper transactions on their layer two, which going to drive them in. Like you say, arbitrum versus optimism. They're going to fight amongst each other for who can use that blob space better.
Speaker A: All of these rollups are in this race to convert this raw material of blob space from Ethereum into the most valuable product it can in the form of layer two block space. And some roll up ecosystems will be more successful than others at doing that, whatever that means. Whether that's kind of the network effect of the roll up, or it means.
Speaker B: The same thing as the Ethereum layer one. We call the ethereum layer one block space is the most valuable block space in the world because people do NFT drops on there, people network effects stuff on there. People pay more for ethereum layer one block space and layer two block space will be judged on the same merits, like how dense is layer two block space will be a function of how much demand there is, which is going to be a function of how much aggregate economic activity there is on average respective layer two.
Speaker C: Yes. And it's also on a spectrum where, like a roll up that aims to be ultra secure, like they're going to want to commit to a blob every single block too, because from the perspective of a layer two roll up, once a transaction is batched onto a layer one blob, it becomes finalized. From the roll ups perspective, the security is now offloaded to Ethereum. So if you're a roll up whose community really values high security, then it might be worth it for them to post a blob every single block, even if it's not completely full or stuff like that. So that would make the roll up more expensive. But a roll up that doesn't care too much about security, like a roll up that has game assets or something where it's not too valuable and they can afford to just wait out a few blocks and have the price of blobs go down. And now they commit a lot of data. Compressed rollups don't have to post a blob every single blog. So that's another aspect of individual trade offs that roll ups will make.
Speaker B: This is what I was going to. Something I wanted to unpack is there's this new variable in blob space resources, which is exactly what you said. How frequently does a layer two choose to commit its date route to the layer one with proto dank charting? You said that there is space for three rollups per block to submit their blobs for three blobs, excuse me, three blobs per block on the layer one. So like three total rollups can submit a total of three blobs per block. That's not, that's a low number. That is a very low number. I mean, that goes up with full dank sharding, but only three blobs per block every 12 seconds. There's, and so this is also going to be a vector, a variable that layer twos tinker with. If you submit your blob every single block to the ethereum layer one, you are maximally secure. But that might not be the optimum point, the optimum balance of security versus expensiveness or efficiency that your users on your particular layer two demand. Maybe like the typical optimistic roll up, which is already operating on fraud proofs, maybe they only need one blob every, I don't know, ten minutes or something. And all of a sudden going from 12 seconds to ten minutes is massively more efficient. This isn't really anything that's part of the core Ethereum protocol, but that is a variable to consider when we talk about layer two economics, which is, how often do they commit their blob to ethereum layer one block space blob space.
Speaker C: Yeah, I'll just add something for the listeners. If you're a layer two users and the layer two posts every ten minutes, you don't have to wait that ten minutes. Your transactions instantly confirmed by a sequencer, and then you can instantly do other stuff. But every ten minutes would be the full commitment and batching on chain where your transaction on layer two now becomes completely settled and finalized and the sequencer can't change it. So that's sort of the trade off like we have today or with bitcoin, where you wait more blocks if you want more security. If you're on a roll up and you want ultimate security, then you're going to wait until the transaction is committed and batched on chain before considering it finalized. But if you don't really care, you're just doing small transactions and the sequencer is going to be the one in charge of the security in transit until it gets onto layer one.
Speaker A: One word you used earlier in this conversation, Dom, I want to return to, is subsidy. That's kind of an interesting word because we like analogies on bankless. And this sort of feels like a. Almost like a government subsidy of a particular sector that the government wants to grow, right? Like, say, solar. There is going to be a government subsidy in the form of tax credit or tax reduction for a particular industry or set of resources. Because why? Because let's say some government wants to subsidize green energy in the form of solar. You can agree or disagree with that policy. Let's not get caught up in the weeds here. But this is effectively the Ethereum protocol, subsidizing in a way. Roll up the roll up roadmap. Would you use these terms? Maybe the net effect is going to be the propagation of rollups because the resource is now much cheaper. How do you think that analogy holds?
Speaker C: I would just argue against a tiny thing you said about it's not the layer one subsidiary. The layer one just says, here's the blobs, do whatever you want with it. And if they're so cheap, then it's up to the roll ups to decide if they want to subsidize and run at a loss just to attract more users. It's like we've seen this war with dapps. It's like subsidizing liquidity or giving token incentives. Roll up sequencers can do the same thing and see, here's few transactions, here's the token incentives come use. Our roll up comes increase the network effects just so that in the long run they have these users and compete against each other for network effects. But this is, in the end, it's positive for Ethereum because it's all getting settled on Ethereum and it's really up to the roll ups to decide if running at a loss for a little couple of months is worth it. But while the blobs are very cheap and not congested, then that's probably what a lot of them are going to try doing.
Speaker A: But the broader point here, Dom, is that the Ethereum protocol in general and Ethereum researchers are choosing to implement this change in order to make roll ups far cheaper. This is towards this goal of what Vitalik said. A blockchain transaction should be fractions of a penny, not even pennies. It's all towards that goal. But it is an intentional decision that the protocol is making. It.
Speaker C: Yeah, it's the roll up centric roadmap. It basically puts rollups as first class citizens on the layer one chain, but it's not really a subsidy as such. It's just a decoupled market to have more efficient usage of resource that can then be scaled up.
Speaker B: I think you might be actually able to take the alternative perspective on this, guys, which is like Ryan, the way that you articulated that is that the protocol designers of Ethereum want a specific outcome. And I think that is actually totally true and exactly what happened. That's why we call it the roll up centric roadmap to Ethereum. Yet also what we are doing is merely decoupling data from execution into two different blocks. Block two different spaces in a block. And then what happens as a result of that decoupling is that it just so happens that rollups are cheaper. But simply doing the act of decoupling is all that 4844 is doing. We are just separating data from execution. It just so happens that that makes roll ups cheaper. And so I think there is an argument that is actually decently credibly neutral. Like, we are not picking winners and losers here. We are not saying that rollups are the winners, even though, again, it is kind of like why we're doing this. We are just merely decoupling data from execution and letting the free market build on top of this permissionless protocol. And it just so happens that decoupling data from execution is more efficient and allows for more total net economic activity to be produced through crypto economics. Do you like that approach, Dom?
Speaker C: Yeah, it's fine. It's a good analogy. We don't really pick and choose the winners and losers. It's really. Here's the resources available by layer one go make use of them as efficiently as you want. And that's what the market is going to converge onto. The roll up that has the most efficient usage blobspace will offer the cheapest fees and thus attracts the more users. And on the long run. I know something Justin Drake likes to ponder about is enshrining a roll up onto layer one, at which point we're using the full tank sharding and 4844 infrastructure to scale up layer one execution. But that's kind of in the far future where we entrance DK VM and now we can have much higher gas limits and have a roll up at layer one that that is going to scale up.
Speaker A: That's.
Speaker C: And the company Dom is that.
Speaker A: That's 2019, isn't it? Are we full circle to 2019 and we have like, sharded execution? We're finally going to be able to.
Speaker C: Do that with an entrain roll up. Yes, that would be effectively be scaling up execution, but in a much, much better way than we initially planned. So we're going full circle with roll ups and then getting that into layer one in the end. But that's kind of in the far future in crypto time, we're just going to have roll ups doing the full.
Speaker B: Circle and returning to execution. Charting and the distant future of Ethereum's roadmap is one of the weirdest, most fascinating things about this universe's timeline. Yeah, books will be written about.
Speaker A: I agree. This would make 2019 me so happy to hear that this is actually happening.
Speaker B: We found the more natural route to.
Speaker C: Execution charting, and it's a much better outcome, and it's a much better way to get there, too, because we at layer one, we don't have to do as much upgrades and as much research because we can just let the market innovate and find the best designs, the most secure ZKVM and stuff like that. And then we just enshrine it, we just yoink it from layer two that developed it, innovated, competed for it, and then assuming it's open source, of course, and we enshrine it into layer one and reap the benefits. And then we get back the coveted execution. Scaling.
Speaker A: You sly dogs, you. I don't know how you made this happen, but it's pretty clever.
Speaker B: This is just the beauty of the harmony of Ethereum that I think really exemplifies why and so many others are just attracted to the protocol. It's just like a nice, it's a beautiful harmony between the free market and the protocol, right? Like top down rules mixed with bottom up markets. And then that is Ethereum. And we got there in a roundabout way. Dom, this has been. Thank you for guiding us. Sorry.
Speaker C: So it's the philosophy of addition by subtraction, like whatever the community can do, we just delegate and there's no top down approach.
Speaker B: Really beautiful, beautiful. So just to really drive home the roadmap on this, we have 4844. Optimistically, you may be coming November, pessimistically some coming in sometime 2024, full dank sharding sometime after that. And then you just kind of left us with what will definitely be future content, which is enshrining a roll up into layer one. What does come next? Say we're posting full dank sharding. Is that it when it comes to data availability? Or what are the next steps after this?
Speaker C: It's not really a sequential roadmap, as you know, so it really depends where we'll be with the other items like enshrined PBS and stuff by the time we have full denk sharding. So it'll be definitely one more step towards the end game as described by Vitalik.
Speaker A: So let's just talk through that blob space and how this kind of pans out with respect to the roll up. So we'll have EIP 4844 and at that point in time, blob space and layer two data availability and transaction costs will drop towards zero. Then over the months we know apps will take off. If you have a really cheap resource like a blob space, guaranteed it gets used. If we have cheap block space, it gets used on Ethereum. That is the rule here. By the time that starts to ramp up, maybe we are consuming those three, four, five blobs per block, we start to get congestion. And so rather than the 2022 people complaining about expensive block space, everyone's complaining about this blob space is too damn high, Dom. And then we have another ace up the sleeve, which is full dank sharding. And again, we don't know the exact timeline here, but once the blob space fills up, full dank sharding gives us even more blob space availability. Right? And if things work out from a timing perspective, maybe we meet the market need at the right time when things are just starting to get congested again. Is that roughly the idea here?
Speaker C: Yeah, that sounds about right. Will be a crazy increase in bob space with full time charting, but we definitely have some things to figure out before we get there with regards to networking and proper sampling techniques and everything. So this is why 4844 is the perfect stepping stone for rollups. Because they don't have to wait as long. They can just use this, the blob space today and then not even have to upgrade once we have full dent sharding, it'll just be done behind the scenes for that.
Speaker A: Wow. Well, this has been an epic episode in a tour de force. I think the Blob Space 101 of everything we needed to know. And I was trying to keep track, David, of how many times we said the word blob on this podcast.
Speaker B: I don't know, but you definitely failed.
Speaker A: Well, you know what? I think some thankless listener will be able to tell us how many times the word blob was said on this podcast. And if you dm us, you know, maybe there'll be something special at the end of that trip.
Speaker B: To any number. I'd be like, wow, you can't believe you actually counted.
Speaker C: Just control f the transcript.
Speaker A: Dom, thank you so much for walking us through this today. It's been.
Speaker C: Thanks for having me.
Speaker A: Fantastic. Thank you. There is an article as well, if you like, written material that domothy wrote about blobspace. We'll include a link to that in the show notes. Gotta let you know, as always, crypto is risky. You could lose what you put in. But we are headed west towards Blob space, the frontier. It's not for everyone, but we're glad you're with us on the bankless journey. Thanks a lot.
