Speaker A: Bankless nation. Welcome to a conversation with Ansgar Dietrich from the Ethereum Foundation. Ansgar is an EF researcher who we've had a bunch of EF researchers on the show today. And whenever I talk to Anzgar, he's always extremely energetic and animated and has got a lot of perspectives to share. The last time I had a conversation with him, it was all about the EVM and the EVM as it relates to layer twos. He's part of a coordinating force around setting some new standards for layer twos that are using the EVM to get some extra bells and whistles, some extra features into the EVM, at least on the layer two level. So we start there. That's where we begin the conversation. I ask about, like, are we ever going to meaningfully upgrade the EVM at the layer one, and why or why not? And do we even desire that outcome? We also get into the conversation of verkle trees, and we kind of do a quick, deep dive into what a vertical tree is and why it's important for Ethereum and what it unlocks. Uh, and then we also quickly get into what I think is actually kind of the highlight of the conversation, which is Onsgar opening up a massive can of worms, talking about the eTH monetary policy, talking about changing the issuance curve for ether. You know, uh, previously the Ethereum monetary policy, when we were in a proof of work era, was we would issue five ether per block, and that was, that was the monetary policy. Then we changed it and we changed it to three eth per block, and then we changed it again to two eth per block. And then we went proof of stake and actually had a adaptive issuance curve for proof of stake Ethereum. In the back of my mind, I always kind of would have thought it's like, well, it's not like we got it right on the first try of proof of stake Ethereum. We're probably gonna have to open up that can of worms again and find the perfect, more finely tuned monetary issuance curve for ether, the asset. It's not like we got it right in the first try. Uh, and I guess that conversation, the day for that conversation has finally come. Cause this is what Ansgar ends this podcast with, uh, about 45 minutes of a conversation of why we need to change the issuance curve for ether. And what he proposes that he, we change it to two different proposals. Um, but definitely one very clear north star of how, uh, Onsgar proposes that we change the issuance curve. So all about Ethereum. All about the future of Ethereum. Uh, definitely, uh, illuminating some parts of the future of Ethereum and bringing them forward to today on this podcast. So, bankless nation, I hope you enjoy this conversation with Onsgar. But first, a moment to talk about some of these fantastic sponsors that make this show possible. Hey, Ansagar, how's it going?
Speaker B: Hey, I'm good. How are you?
Speaker A: Good. Good question for you. Will we ever upgrade the EVM at the layer one?
Speaker B: That's a great question. Of course, if you look at the history of Ethereum's upgrades and the path that has gotten us this far, we do occasionally make smaller upgrades to the EVM and the feature set. If you remember, two, three, four years ago, there was quite a bit of research into what we call execution environments. So the idea was to actually move beyond the EVM.
Speaker A: This is when we wanted to do ewasm or something like this. Right?
Speaker B: Exactly, exactly. And that was actually funny enough, that was kind of basically when I started to join, when I joined as a researcher as well. So my initial work was on execution environments and how to make them communicate synchronously and all these kind of things. But we basically, over time, figured out that the EVM itself just has enough of a Lindy effect. It has too much traction. It's just good enough, basically. And so now it's more about improving the VM on the base layer, but actually, in terms of to what extent we will keep doing that and basically, how ambitious we will be, I think that's a little bit, still remains to be seen. And specifically, it's actually a really interesting kind of topic in general. So, I guess, good that you brought it up. But so, basically, as I see it, right, with layer two s, a lot of them, so file have chosen to go this path of EVM equivalence, right? And so basically, they all take the base chain EVM, and then they look into extending it in interesting ways, like how arbitrum does it with stylus and these things. But basically, the base virtual machine itself is just one to one main net EVM, or at least as close to one to one as they can get, usually. And that was not something that necessarily, when we moved away from execution environments, that wasn't necessarily foreseen. So the idea was just, okay, let's just open it up. Rollups can build any virtual machine that they want. And now, then we were basically a little bit surprised by to what extent people were flocking to the VM, which, of course, I think it's interesting to see it's good to see. But now, basically, I think there are basically two paths that I see. Forward path a would be that we embrace that coupling and we basically say base chain. And these layer twos will basically remain on the same virtual machine for the foreseeable future, maybe forever. Then the nice thing about that is in the long term, we get these really nice payoffs in terms of. I think you've had Justin on here already talk about this idea of at some point we could have enshrined ZKE VM. So you basically get an opcode or a pre compile where you can just verify like an entire layer two kind of state transition all at once on layer one. So it's going to be really kind of efficient. That comes with a couple of challenges. Right. So, like, of course, the end game. I always kind of talk a little bit about it in the context of Solana because it's just a really interesting comparison to make. And basically, at some point, layer twos have to get to similar throughput levels, right? Like, you have to get to the 10,000, 10,0000 TPS chains. You can't do that with today's EVM. Right. And so the question is, if we never want to do big deviations between layer one and layer two, it would basically mean that we'd have to look into alternative architecture. So, for example, there's this really interesting super chain idea. And also. Sorry I'm talking so much, so feel.
Speaker A: Free to make my job so easy.
Speaker B: Okay, okay. Okay.
Speaker A: Keep going right through this.
Speaker B: Great, great. So you know how like this and super chain, I mean, that's of course, the optimism flavor. So there's basically a lot of these kind of big l two ecosystems have this right now. So I'm only using optimism here as an example. And the idea basically, is that you have many smaller roll ups in your head. You can imagine you have many EVM systems that are all on their own, throughput wise, comparable to l one. You can certainly push them further. You can push them probably much further in terms of throughput, but you won't push any single one of those to 10,000 dp's. But what you can do is you can basically tie many of them together. And what you get, in effect, is basically something that looks almost like one big chain, like super chain or whatever you want to call it, right. It has the throughput level, certainly of like a really big chain, like all of them basically added together, but it basically uses every single individual roll up or individual EVM chain as basically, in a way, like a single thread inside of a multi threaded system. You can think of it that way, right? It comes with some trade offs. Like, it's basically a bit more rigid, because now each individual EVM chain, kind of like you have to kind of chunk the load, the kind of the processing that's happening homogeneously across these kind of, these chunks, like, you can't have. There's still individual chains underneath. So you can't just do very, very fancy interoperability techniques. But the nice thing is you basically get to really high throughput levels almost with out of the box technology. If we go to this path and we end up in that world, then basically that means that layer ones and layer twos will basically forever share a virtual machine, at least in that sense. What that also means for layer one is that now in our EVM design, we have to basically keep taking that into account. We have to basically really, we have to have some sort of governance structure or process that really ends up with the best virtual machine for this shared use case. In that scenario, almost, I would assume that means more innovation on the layer one as well, because it has to keep enabling the layer two innovation, the alternative approach. And I think that's something that maybe we haven't really investigated enough yet and that I'm personally also very interested in, would be basically to say it made sense that we started with EVM equivalence because it's really hard to bootstrap a chain. And we have seen that in the past that there were some really interesting and ambitious l two projects that just didn't really go anywhere because no developers wanted to learn how to use a slightly different kind of virtual machine. And users also just wanted to use metamask and these kind of things. But now we have that traction in particular. For example, a colleague of mine, Carl Beekhuis, and I, we recently started what we call the roll call process. The idea there is to basically bring many layer twos, ideally all of them basically to a table, and to standardize features that they all want, basically to give them a way to move away from the constraint layer one feature set, but in a way where no individual roll up has to be afraid that they might be stranded with a feature that no one else supports. There are a lot of features that all of them would want to ship, but they haven't so far, because they just don't want to be the one roll up that then none of the tooling supports. But if many rollups would signal that they all want to do this together, then of course, all of a sudden, because that's where most of the users are these days already. And especially in the future, as we keep growing, more and more of the relevant user base will be over in layer two. So as a unit they'll have enough importance that now it really makes sense for all the tooling, the clients to support that kind of, you could imagine as basically some sort of common core feature set like we call them rips, roll up improvement proposals. That was Tim's idea for a name. And basically you could imagine that there could be some common core of ips that extend the layer one evm, then layer twos still of course do their own thing on top of that. It's not about making, not all of them have the exact same system because they still want to compete and have their competitive edges and whatnot. But on the features that they all want, we basically create some sort of way for them to slowly start departing from the constraints of layer one avian, right?
Speaker A: You're talking about enshrining features, but at the layer two level, not the layer one level. Starting with the layer two.
Speaker B: Exactly, exactly, exactly. We have one candidate now we have this, the SEGP 256 r1 curve, which is this cryptographic curve. I don't know how much you're familiar with that, but basically on Ethereum we use the k one curve, which is a very similar elliptic curve. But it turned out that in the real world outside of Ethereum, the much more popular one is the r one curve. And so for example, when you use your phone, you have your secure enclave where whenever you do a face id check on Android or something, what it does under the hood, it actually uses the r one curve to create signatures and whatnot. But today if you want to say, build a smart contract wallet and it wants to use face id natively, you have to just basically manually do that verification in just normal EVM code. And it's really, really expensive versus just the EoA, the metamask kind of check. And so there's this in IP to ship that curve specifically. And a lot of, we recently finalized that as the first kind of finalized IP. And now a lot of layer twos have already signaled that they are going to ship it and are in the process of shipping it. We might bring it to mainnet as well. This might actually be a bad example. It might still come to mainnet afterwards, but it's illustrating where we could go basically. I really see this as basically two potential paths into the future.
Speaker A: Sure. I think the spirit of the question is can we get everything that we want into the EVM on the layer one, which is the hardest place to get anything at all? And so you're talking about just innovation at the EVM on the layer two level, and some of that will filter down and maybe actually get merged into the layer one level. And I'm trying to figure out how far can we really take that if we were really motivated to take for example, Monad, who is a layer one blockchain that is working to parallelize, the EVM will one day open source their code. As soon as you do that, you will can only imagine that somebody is going to deploy that as a layer two, maybe merge it as a module in the op stack. But then we're going to have the Monad EVM, uh, inside of like the Ethereum ecosystem, and then like say for example, like all the EF researchers out there like did the sniff test of the Monad evM, and everyone's like, oh, it's pretty good, thumbs up, we like it. How could we one day get that into the Ethereum layer one and like start to have a parallelized ethereum layer one for whatever reason? Like just to, because we could, like why don't we want to optimize execution if it doesn't cost us anything? And like, have I thrown some flags in that process? Is that too ambitious? Is it not even necessary? What are your reflections on some goal like that?
Speaker B: Yeah, so that's actually a really good question, and I think because I feel like I didn't really highlight that part of it yet. So in principle we could, there's nothing stopping us to do that. In the past also, there has been some work already on kind of like transaction parallelization in the kind of main net context as well. The main problem really is that with layer two, SDHE, or depending on whether Monad themselves want to become a layer two, I think that's still a little up in the air from what I understand, but I haven't talked with them in a while or whether they want to be an l one, but it doesn't really matter. Then someone else might just spin up an l two l two s have the luxury that they can in principle run. Basically the meme is that these high throughput chains basically require server grade nodes. Basically you can't just run as a normal user at home. You just can't run a phone node because it's too much throughput, too much compute. The nice thing about layer twos is that they basically, in a way get the best of both worlds, because they are anchored to this very lightweight layer one, they still get the full security of basically this everyone can verify at home, but then they can still scale through throughputs where you need a server node to process it. It's perfectly fine if I'm on a layer two, and the only full nodes for that layer two have to be run in data centers. There's nothing inherently, especially if it's a Zk roll up. But even in an optimistic role, as long as the honest assumption is, again, a single honest party. Whereas on the base chain, a single honest party doesn't help you, right? If there's one honest person in the entire ethereum ecosystem, it doesn't help Ethereum as a chain, right? So on the base chain, you need, like, basically a lot of honest people. So that's why you need kind of this very, very low barrier to entry. There are two data center nodes just fine. And so the amount of, basically the extent to which you can push beyond the current EVM just met differs a lot between those two cases. So, yes, in principle, you could, say, bring parallelization to the layer one EVM. And there's some benefits to be had. For example, sometimes while you execute a block, you have to catch some state. You have to load it from disk. You're just waiting. In that time, you could execute something else. There's still efficiency. You could get out of a node that runs just on your little raspberry PI or your laptop or maybe in the future your phone or something. But most of the benefits you get from, say, the Monad approach, you only get if you design it with the data center node in mind. And so this is one of these examples if we want to go the path of keeping layer one and layer two virtual machines coupled forever. In that case, something that would only have limited payoff, right. This is much more an innovation that I would be really excited about in the alternative view, the alternative world, the world where we have really lightweight layer one and then heavy, heavy optimized layer twos.
Speaker A: Right? And this is what Ethereum has been optimizing for in the first place. It kind of is, like, nonsensical to take a highly execution oriented virtual machine and try to get it down to the layer one, where we still have resource constrained nodes by design. Uh, and so what you're saying is like, it's, it's just, it's oxymoronic to have a very good execution environment, a highly optimized execution environment built on top of a layer that is meant for consensus uh, and like maybe there you're saying, sure, there are some perks, there are some efficiency gains, but really it's been the spirit of Ethereum to push execution to where we can at the layer twos. And just like, if we're going to have an execution optimized, Vmdezen put it on a data center that's also hardware, that's execution optimized. And then Ethereum layer one will just like stay like relatively constrained. And so like, what I'm getting is that, what I'm sensing is that like, even while many people say like, oh, the EVM, it's like such like obsolete technology, it's so bad. I think what you're saying is like, maybe even if that is right, the detriments, the negatives of the EVM for whatever they are, aren't expressed at the layer one level, and where they could become, where all of the benefits of an execution environment at the layer two level can also be mitigated in different ways. It sounds like this is what you're saying.
Speaker B: Yeah, I think that's basically a fair summary. I think there's no need for us to be complacent in terms of pushing on the layer one as well. If we can get free gains by just having a more efficient architecture, why not? That's definitely not something where we would want to, for any ideological reasons, stop innovating. But I think basically these separate perspectives that you just highlighted are very kind of important, I think, to the way at least I'm thinking about it. Of course, we don't necessarily have full agreement. For example, again, when you talk to Justin and Vitalik, I think they are, for example, a bit more excited about this potential future of just having this enshrined. I mean, I don't know, I don't want to speak for them, but you've definitely heard this mentioned before, this enshrined ZKVM on the base layer and whatnot. And that definitely is more beneficial if you kind of keep this coupling. But basically still, in any of these cases, you should think of, if you hear EVM, it's almost like it doesn't really make sense, right? You should say layer one EVM, because even if we keep stay coupled over time, the layer two equivalent won't be a single EVM, it will be the super chain or the whatever. Pick your favorite ecosystem and the equivalent of the super chain. But basically, just comparing a single threaded base layer EVM to whatever all these other kind of user focused, application focused systems do, just doesn't make sense, because that's not where the ethereum ecosystem will end up in. We'll either get the Monad style high throughput vms that maybe are based on EVM or not, or we get some sort of super chain style threaded EVM where the individual blocks are still look similar to the mainnet, but in aggregate they still effectively create something much, much higher throughput.
Speaker A: Interesting. Inside of the Vitalik roadmap graphic, there's end game EVM as a category at the layer one level. Is there a defined direction for what the end game of the EVM for the ethereum layer one looks like? What are some properties that are currently missing from the layer one EVM that you think ought to be in there? And we're talking five to 15 long year time horizons here, right?
Speaker B: So of course the roadmap is always a bit of a living thing. They're basically features that we already know that we want, and then there's just features that basically are bit up in the air and depending on where exactly how this entire architecture, this modular kind of stack and modular architecture plays out, right? Like that might or might not end up there. And for example, I think one thing I'm personally very excited about is verkle trees, I guess, has come up already quite a bit. It's now going to be most likely looks like the next big focus on the execution layer side of Ethereum for the next little while. And the nice thing is that that gets us to this world of statelessness. And I feel like statelessness has been one of these topics that has been around for a long time. And I feel like because it's so old now and we've never really done anything with it, by now, it's just no longer exciting. I haven't really heard anyone talk about statelessness in a long time, at least not in an excited context. But it's actually really, really interesting. What that basically gets you is in principle you could imagine intuition wise, it basically gets you this super, super lightweight node that basically doesn't need to store any data locally. In a way, I always like to call it the root of trust. The role of your node is really just to expose the current state root of the Ethereum system, right? And then you can, you can have like all the layer twos and everything, they can just have proofs on top of it, right? The nice thing is once you have this base hash and you just trust that that is the correct hash and, you know, make sure it is, you can, you can do a lot of things with it, but it's basically, it's super lightweight. And then every 12 seconds, basically new block comes in. But the block just comes with all the data you need to execute it. Right. So instead of going to having to go to disk, you just look at the data that comes with a block. And in principle we could do this today, but with the existing kind of tree system structure that we use, the merkel tree, merkel Patricia tree, it's just way too inefficient. You basically have to send megabytes worth of data with every single block. It's just way too much. So with worker trees, we move to a state commitment scheme that is specifically optimized for this, so that we call them the witnesses, become much more lightweight. And then you can really run these super, super tiny footprint nodes. I think it'll take a couple of years because you have to build out the entire stack around this. Like, what does this look like? But I think this is going to be really, really exciting.
Speaker A: Okay, so we actually just did this Ethereum roadmap episode with Dom and Mike. And during the verge part of the conversation, we were going through some of this, and the way that I understood it was that verkle trees are just like a technically upgraded merkle tree. This is a better version of a Merkle tree. But I think what you're saying, it's actually such a strong upgrade that it unlocks this whole property that we call statelessness. But I actually didn't totally follow about, like, I kind of get it at a high level, but I'm going to need you to explain it one more time about just like, what property do we get from this that enables like a flourishing of new types of activity or new types of just like, chains.
Speaker B: Now this is great. So, so basically the way to think about it, right, is that, like, I don't know how much we want to go into detail, but basically. Merkel, Merkel Patricia tree, right? The way Ethereum does it today, is that basically you have this tree structure where you, where all the state on Ethereum, like your account balances, but also the current price at Uniswap or whatnot, that all lives under one big Merkle tree. So basically you have a root branch, and then at every layer you have 16 different nodes. So basically root branch, and then it branches out into 16 children, and then the 16 children each have 16 children and 16 children, until at some point you go all the way down to the individual entries.
Speaker A: And that's a database that's basically. Well, it's a database structure, you kind.
Speaker B: Of have to distinguish between the actual database, the real kind of actual database in the client that runs on your computer, which is usually just like a flat kind of key value mapping or depending on the client, it's slightly different. And then what we call the state commitment scheme. So basically that's just about, at the end of the day, each Ethereum block kind of says after executing all the transactions, the new state root is this hash. Basically how do you get from all the balances and all the kind of state, the uniswap prices and whatnot, how do you get to the state route? Basically, how do you commit to all of this kind of very very large state with just a single hash? And that's basically what we do with this tree structure. But in a way the tree, you can cache that in a client, so it's more efficient to update that. But basically it doesn't really exist in that way. It's just like a construction to create this commitment. But because of the structure, basically right now if you want to do this kind of create a stateless client, then every block has to not come with all the data. Basically all the, you could imagine it this way, you just take a block in a normal client, you execute it and then you look where on disk did I have to load data from? Then you just take all that data and instead next time you send the block to the next person, you just send all the data with the block because you already know that that person executing the block will have to access the exact same data. So now you just send it for them so they don't have to look at their own disk. Problem is today I don't just want to trust you, right? Because you could just say, look, before the block I had an infinite balance. Oh great, great for me. And so it would have to come with a proof. And today this proof has to be like a merkle proof, which means that you basically you go through the entire tree, you go down to all the levels you need to go to, and then you always say out of the 16 branches you're only interested in one branch, you still have to provide all the 15 other branches because otherwise I can't verify the proof. And so that's just really blowing up the size of the proof. So the majority of all the kind of the data that comes with it is, is this kind of this proof. And then also kind of verifying this proof is relatively slow. Now in verkle trees are basically just optimized for the size of this proof. So basically, the intuition is instead of 16, we make it even wider, but we do it in a way where cryptographically, you don't have to send all the other ones, you only send the one that you only reveal the one that you're interested in. And then you just send a proof that it belonged to the parent node. But you don't do that by sending all the siblings. And that way, basically, the proof size shrinks down to almost nothing. Basically, at that point, it's really just the data you have to send.
Speaker A: So with a merkle tree, it's like you said, it's a tree structure with siblings spawning out of siblings spawning out of siblings, and eventually it has all the data to be able to create the proof. Hopefully that was right. And then we have a vertical tree, which is a new, same sort of tree structure, different kind of topology, but same sort of pattern, but with a different type of compression algorithm. And when you were explaining, it felt like sharding in a sense, where you have this tree on the merkle tree side of things, and it's just like a monolithic tree. You need the whole thing. In order for the next person to run their computation, you have to pass on all of the data. Even if they're only concerned about one single node on this one tree, they need all of it to comprise the data. And what you're saying with the vertical trees is that you actually only need a specific slice of the data to get to your data that you're concerned with, and you can throw away the rest of the data. So we're like sharding a tree. We're taking one single path down the tree, throwing out the rest, because we're not concerned. Maybe it's not like all the people who are listening to this were much more technical than me are. It's not sharding, but it kind of feels like sharding. Einsgar kind of feels like it.
Speaker B: I think maybe the intuition kind of is indeed similar. I like just maybe one, one small addition. There is that even also in the Merkle tree, you don't actually, of course, you don't have to send the full tree. The full tree is like hundreds of gigs, or like tens of gigabytes, but you have to do at least basically, you can cut off each branch. You don't need basically at the next level. So basically you only have to send the siblings. But then for the siblings, if you don't actually care about the data that lives under those siblings, you don't have to send their children. Right. Children. Note, you basically only have to send the hash of the sibling, but it still means that if you're only interested in a single branch at any given point, you still have 16 x, the overhead at that level, and then the next level, again 16 x, but you don't have to send the full tree.
Speaker A: Right. Okay, so with verkle trees and with merkle trees, we're having this exponentially increasing data. The more depth you go down in the tree. And with a verkle tree, you can just prune a lot more of unconcerned, there's a lot of just more unconcerned data in a vertical tree while you can still get to your conclusion, correct?
Speaker B: Yeah, you can basically think of it as like in a merkle tree, most of the data you end up sending on the network is just the proof. It's not actually the data you need to execute the block. It's just all these kind of sibling nodes are everywhere that you need to make sure that that data actually belonged to the tree. Whereas in vertical we make this proof part of it super, super efficient in terms of the data size. So that basically all the data that you have to download is really just the data you need for the execution. And that way, I mean, obviously you can't go like, unless we at some point get to go to a ZKVM system on layer one, you can't go further down than that. You need to download the data you actually need for execution. And so basically we kind of feel close to the theoretical limit. There's still a little bit of a proof size, but the proof is really much, much, much smaller than the data, right?
Speaker A: Anytime somebody says the words theoretical limit, I'm like, oh, nice. Okay, so like, and so this is what, like, it gets, it's such an order of magnitude in such a large order of magnitude increase over Merkle trees, where it gets to the point of just, we can make the assumption that any general consumer hardware is able to run the computation required.
Speaker B: So it's interesting, actually, it's not so much about the computation for now, it's mostly about the bandwidth.
Speaker A: The bandwidth.
Speaker B: Bandwidth, yes. So basically what that means is now your cpu requirements basically for your node is still the same as it was before we shipped worker, but you can basically just get rid of all the local state. So basically your disks disk requirements go down orders of magnitude, and we avoid that. That comes at a big cost. For extra bandwidth you need, it's still going to be a bit of extra bandwidth because you need to download that extra data you need for execution. But it's not this basically completely infeasible amount of bandwidth you would need under Merkel.
Speaker A: Okay, so if we think of the Internet as this thing that's just passing around data packets from device to device to device all over the earth, uh, and we are simultaneously going to like what it means to be passing around an Ethereum block to add to the chain, and the block becomes a lot easier to pass around like we have. Just like, I think one of the bull cases for the Verge that this innovation that we're talking about is like, the Internet can just be passing around a lot more Ethereum blocks all the time. Like, blocks are smaller, they're easier to get around the Internet just because everything is so constrained and then therefore, also, like, more devices can be receiving and also passing blocks. So, like, as this Internet object, Ethereum just gets a lot more lightweight. It's like a very lightweight object that the Internet itself can just manage a lot easier.
Speaker B: Right? And I think it really comes back, you know, how, like, in the beginning of Ethereum, there was always this meme of, like, you're supposed to run your own full node, and then at this, at the current point, you basically, like, you have a few hardcore enthusiasts that, I mean, some people run their own node for staking, but even those people, usually whenever they interact with a smart contract, they don't do that by connecting to their own node or something. They still connect via metamask and Infuria and whatnot. Right? And of course, there are some hardcore enthusiasts, and so I think it's still better than not being able to do this at all. But of course, in practice, it's a somewhat limited benefit that we get, right? And, like, the whole point, of course, the ethos of blockchain is you're supposed to not have to trust anyone, right? And so I think in the very long term, like, I think it's good that we, I think the way we kind of handling it today, where we don't focus on this overly, like, it's definitely more important for now to just get scaling out of the way, to get the roll up centric roadmap, to basically make sure that we actually have a product that's interesting for users now, instead of building forever on this kind of minimum footprint system that no one can use, and hopefully at some point will be good enough. So I think we have the right order of priorities, but at some point down the road, we have to get to a system where my interactions actually are end to end fully trustless, just like how oftentimes when you have a new social network or messaging up or something, they start without end to end encryption, because it's more important that it works than that it works trustlessly. But at some point, you want to add end to end encryption because everyone being able to read your messages forever, it's just not a good solution, I would think about it. Similarly, in ethereum right now, we're just in the stage where we just try to make the system work the way we think it should work in the long term. But then at some point, basically, we are at the stage where we actually want to add back the end to end encryption. So basically add back the full trustlessness. We want to really get to the system where all the computational devices in the world that interact with the chain do that by running something locally that doesn't have to be even a stateless full node that's still computationally, relatively heavily, at least before we get to zero. So there could be some sort of lite client or something. But basically, at some point, we want to have this normalized state where you basically have this root of trust, this kind of, this base hash of the entire system that you have locally, as like in some trust minimized way. And then any interaction you have on top always comes with proofs. And for example, even today, inferior already offers that. Or like alchemy or whatnot. I'm not sure how active, but in principle, that functionality is there, that every interaction comes with a proof and you can locally verify. It's just that because no one runs their own load locally, it's useless. You can't check that proof against anything. And so usually they don't send it at all, but you could request it in the future. If you run your own very, very lightweight node locally, then no one can lie to you anymore. If someone just says, that's your balance, it will automatically just be checked. You could even imagine a browser extension where you just can, and it has a little checkmark because you don't just trust that you. Exactly. It's been verified. And even in the future, there are all these use cases with AI and whatnot, generative AI, there's all these ideas about proof of provenance and these kind of things. You could imagine that literally just any piece of data that you come across, you just have a little check mark because you have your local, very, very minimal footprint client that is the base oracle for the current state of the world. And then everything comes with proofs. That's kind of the long term vision for this, right?
Speaker A: So as soon as we get the vertical trees and the verge and we get statelessness, then the burden of responsibility kind of moves higher up in the stack to the abstraction layer. And so maybe listeners have their, what do you call it, a VPN? And they pop open their little brow, their dock on their Mac or whatever, and they just click their VPN and the VPN turns on. And then all of a sudden they have a VPN and they have some level of autonomy and sovereignty now that they, whenever they navigate the Internet. And so maybe I think what you're saying is like, we get to that level of hopefully like UI Ux, where just like you run your, you like have like this like download thing, you downloaded it. And it's the, it's an ethereum node and it's just processing Ethereum in the background because Ethereum has always been optimized for consumer level hardware, so you're never going to see it suck up bandwidth or computation or memory or anything. Or like you're saying as a browser extension or maybe even like embedded in chrome or firefox or whatever. So you have your little like lock signal for your HTTPs, and then you also have like a little eth signal saying, yeah, all of this is natively verified. And who's doing the verification? Not inferior.
Speaker B: You.
Speaker A: Your browser just did it on your behalf. And this kind of goes back to what you were saying, like the whole ethos of crypto and like what we're doing here in the first place. And we're doing it because we've made it easy, not because we've like hit people over the head with a cudgel about how you shall run your own node.
Speaker B: Yeah, because I think, by the way, I think what you said at the very end there, I think that's really, really important, right? Because as crypto grows, like, of course you started out, when you go all the way back to 20 10, 20 11, 20 12, you have these crazy ideologues, like in the best sense possible, right? I mean, I found bitcoin in 2013 or something. And I was back then I went to all those european students for Liberty conferences and I was like this hardcore libertarian, those kind of people, right? And I didn't even get involved at that point. So the people that actually ran the show back then, those were people that were super committed to the cause. And then you had, over the next few years, you have more and more waves of people that are still drawn in. Then you get to, you, me, people that joined a little later that still share the same values, but didn't necessarily weren't the kind of people that just completely just left their previous lives and just jumped all in when it was still crazy. And then over time, you bring more and more people, and that might still find the technology very interesting, but are just not that, that don't care so much about the values. And then at some point, you just have the consumers that just don't care about the values at all. They just want to use the tech. Basically, the more we scale, the more we have adoption, the less we just get buy in by default, just because people care really about the values, at some point, you just basically have to just have the best and most seamless product. I think it is really, really important that we design the system so that we don't compromise on the values, but we do it in a way where we don't. We don't only succeed because the users have to basically buy into those values to that level of commitment. We have to make it completely seamless. And then we should still explain to people why this is actually a good thing and why these values are actually desirable and why this entire system hopefully has all these nice properties that we wanted to have, but it only works if it's seamless. I think that's really important to always keep in mind, right?
Speaker A: Yeah, maybe the goal is that when you download your browser, you would actually have to turn off the native verification rather than having to turn it on. Just have it. Like, I remember there's, like, this old anecdote of, like, Gavin Andreessen, who I think was the first guy that Satoshi reached out to and started talking to about, like, running bitcoin. Um, Gavin Pest passed back this idea to Satoshi about the bitcoin client, which was like, hey, we should just make it, like, automatically start running when people turn on their computer, and, like, they merge that into bitcoin core. And two weeks later, the hash rate on the network was 20 times higher just because it was on by default. I remember there was one of the few times I researched Satoshi. I remember going back and forth and learning that lesson. This is what you're saying. We need this to be on by default, and that's how we get to that level.
Speaker B: Right. But for that, we have to make it seamless on my default. If it's on by default and it's really, really cumbersome and it slows down your computer, that doesn't make sense. Just to make clear listeners understand that when we talk about this this is the end goal here. That's the end game. That's where we hopefully get in ten years. And I mean, people always talk about ten years. I think it's fine to always still keep talking about ten years from now, because it doesn't mean that in those ten years Ethereum and the Ethereum ecosystem won't already be very useful. And maybe later in this conversation we can talk about this a little bit. I'm getting really excited right now about the current moment in time. I think we are literally at the cusp of starting to build products that are useful for people that don't necessarily care about the ideological side of things, but basically to fully get to this completely seamless world where everyone runs everything local, it will be a slow and gradual transition. But basically worker trees are the first step in this gradual transition.
Speaker A: Yeah. Right. Okay. Yeah. So I do want to get into what you are excited about. There's a lot going on in the infra and layer two space, but I don't think that's what you were alluding to. I think you were kind of alluding to something more on the consumer, higher level abstraction part of Ethereum. What is getting you excited these days?
Speaker B: Yeah, I mean, there's maybe two topics that we could talk about, and I mean, you tell me maybe what order we could do that. So maybe on the general side that fits more than what we just said would be that, you know how basically for the last ten years, or like seven years at least, everyone always talks about mass adoption and we are next wave. Next wave, you know, next wave, we'll get like all the billions of users. And I think for a long time that was just a little kind of too naively optimistic in a way. Right. And I mean, I was definitely part of that. So I'm not, but I'm not calling anyone here naive, but it was basically just that the tech stack just wasn't ready. Right. People always compare to the early Internet. And I think to some extent, maybe that analogy is a little kind of too simplistic, but I still think it's useful. Right. And I think also in the early Internet, you had all these years and years where the only things that the Internet was really useful for was just like chat rooms where you could be a cat and you could talk to other cats, you know, and that was fun, but it was, it didn't create, like, real world value yet. Right, but it would have been a mistake to dismiss it. Right, but. But it still had to justify itself. You can't, you can't remain the chat room for cats forever. Right? And I think in ethereum, we kind of, like in blockchains in general, we've always been in this. In this space. Also, if you look at, like, all these. The past, like, bull markets, right? Like, what were they centered around? Like, defi summer and all these things. That's all inside baseball, right? It's just like, people that already are excited about either the technology or just the potential to make money, you know, like, or some mix of both, right? Just getting even more excited about novel ways to make some money by just shuffling around numbers in the system. But it wasn't creating value for anyone's grandma. That's just not what this technology was doing yet. I think clearly we have to change that at some point. I think the criticism of blockchain from people that are maybe skeptical was always that it's this completely nonsensical system. And I think we're getting, finally getting. That's really about to change. And I personally, for example, I'm really, really excited about stable coins. I understand not everyone, like, some kind of more true believers, like, oh, the whole point here is to reject fiat. I still want to out compete fiat, but I want to out compete fiat on equal footing. I don't want to out compete fiat because its tech stack is too old. Right. I want to out compete fiat because we have the better money. I love that first step for that is just bring fiat on chain, right? We want to bring fiat on chain and we want to really make it the best version of itself that it could be, and then we want to beat it, right. And just getting really, really excited about that first step. Right. And I think basically, if you look, for example, you know, Liam from, he used to be the CTO at op Labs, Liam Horn. And he recently had a really, really good blog post where if any listeners that are kind of interested about that general space should really go look it up, where he had, like, a deep dive in the kind of the evolution of stablecoins over the last few years. And it's one point, for example, one really interesting takeaway was that, like, there was basically no dip during the bear market over the last few years, whereas everyone here in Ethereum was just depressed and they stopped using all these systems. Stablecoin adoption just straight line up. Of course, there was a little bit of a dip, I'm oversimplifying, but basically only small dip, still adoption rising because the people that actually start using stable coins these days, that's more and more people in Argentina in Turkey, in parts of the world where basically the local currency is just highly inflationary, it's really hard. Maybe there's government controls about who's allowed to use the currency and how. And there it starts to actually create real world value for users. For example, I have some colleagues that are starting to, for example, pay attention much more to say chains like Tron. No one here. You always on the show, I'm an occasional listener, and you always talk about the latest and greatest networks. And I think that's where all the interesting tech is happening. Solana, and they got Alex Celestia and all these systems. But actually, if you look at adoption, Tron is actually, in some regards, is beating itself.
Speaker A: That is crypto for a lot of people.
Speaker B: Yeah. And for example, I think there was this metric of fee burn in terms of. Actually, I think there was some weeks where Tron was beating Ethereum, even Tron one, Ethereum two and whatnot. And that's a signal. Do I necessarily like. I mean, I don't want to judge other projects, right. But I personally would prefer a world where maybe that's not the base stack for the entire technology, but we have to first acknowledge that that's where the real value created today is and the real adoption. And USDT, of course, I forgot to mention that USDT on TroN, that's where basically all the action is happening. And that was just because they had really cheap transaction fees and it all seamlessly worked. It's integrated with all the exchanges. That's usually when you go to one of these many shops. You just walk down the street in Turkey, you see a bitcoin logo or something. You go into the shop oftentimes, even though it's a bitcoin logo, they don't even give you bitcoin deposits or withdrawals. It's all just Tron. They ask you for your Tron address, and then they send you tether. Right? You give them local currency, they send you tether, or the other way around. And so, of course, on ethereum, we always do things in more principles, right? So it took us, like a little longer to get that scale. But we are really at the point now, especially with 4.44 and kind of the scaling roadmap there, that we are now in a position where we can start to compete with that level of fees and at the same time, hopefully kind of have the better overall stack. Right? And so I'm personally, I really, really think that within the next. I don't know this is a prediction, but I would be shocked if within the next two, three years, one of the biggest impacts of Ethereum or blockchain more general Wilson was not basically bringing a lot of the dollar world on chain. I think that's basically. I think that's really going to be in the future what's starting to kind of drive real world blockchain usage. And I think it's a very healthy thing. Again, you might have, you could have concerns about fiat in general, but it's still, it's actually something that creates value for real people in the real world. Right. And I mean, this sounds so dumb to keep saying that, but, like, it's just not what we have been doing so far in ethereum. We've been preparing for that moment and I think it's all fine. Like, I'm not criticizing anything, but like, we have been preparing for this and now we have to really kind of start embracing that and start focusing on that and start having to build us, start building our systems with that in mind. Yeah, you can see like, there's a little bit of a dip in adoption, but then it's kind of like, it's not. It's not like it went down 80, 90%, right?
Speaker A: Yeah. For the podcast listeners, I'm sharing my screen for the blocks total stable coin supply, and it peaked out in the middle of last bull market at 180 billion, bottomed in the bear market at 130 billion, and now we're already like, we've already up to 142 billion. And so, like, okay, so we're down, like, what? We're down 40? But then I want to remind bankless listeners that, again, who aren't watching it, that if you go from 2020, halfway through 2022, when it peaked at 180 billion, if we go back just two years prior, it was at 13 billion, 14 billion, and I think, like, that was like the first 100 x in total stablecoin supply. And I think, Ansgar, what you're saying is, like, this is about to do another one of these, like, gargantuan run ups and total stable coin supply just because like, it's what we're good at and it's what people want and it is already turning heads and I think it's probably going to turn more heads.
Speaker B: And it's also the one thing you can use a blockchain for without having to immediately buy into all the audio. Logical side. You don't have to believe in bitcoin. You don't have to believe in ether. If you say a company and you have a branch in one country and a branch in another country, you want to send money, either you do it multi day settlement via bank, or you do it via stable coins. It's just a no brainer. It's not a no brainer yet, because you still need to build all the infrastructure around it. It's starting to get to the point where it's a no brainer. I'm definitely the wrong person to talk to about all the details, but I'm not sure if you maybe already had a stablecoin focused episode. It's definitely something you should have where you bring someone on people that really deal with the on the ground reality of that world, because I'm personally super excited about it. And I think basically maybe the broader categories. Of example, a couple of friends of me and I, we recently organized an event in San Francisco called Real World Ethereum, where basically try to, try to get people to embrace that mindset. So we had a bunch of, we had. Liam was speaking there, we also had Jesse from base, Georgios from Ereth, and a bunch of people basically just talking about how can we make sure the tech stack is ready to actually start embracing, start supporting these applications for the real world. And so I'm still excited about the next cycle and whether restaking will fuel the next whatever speculative bubble in crypto. It's fine not saying stop talking about it, stop focusing about it, not you personally, like the listeners. Right. But that's not what I think over time will be important. Right?
Speaker A: Yeah, well, there's like a couple of different ways I see, like, stable coins really growing. One is like, in the third world adoption. Like, there's no. It's no coincidence that all the countries that are meaningfully adopting stable coins are always also the ones that have extremely highly inflating currencies. So you can talk about that, and I'll call it kind of call that like, the bottom up adoption of stable coins. But then you also have, like, you know, Brian Armstrong and Coinbase and their 25% share of circle and base chain are also thinking about, like, using base for USDC payments. And then also we have Pyusc from Paypal. And so we have two large american companies, like, pushing forward payments. And so you also have it coming down from the top down side of things. I don't know which one. Maybe they both equally spark joy with you for different reasons, but this is kind of how I see the landscape unfolding, right?
Speaker B: I mean, I think, of course it's important to get the details right. How are these stablecoins structured? To what extent are they? Maybe centralized. Of course, in the end of the day, all these kind of kind of us dollar and bank account backed ones always have some sort of central entity. But to what extent do they just have manually, maybe whitelist people or are able to freeze assets and these kind of things? So there's definitely details to get right. But in general, I personally think the way blockchain succeeds is by permissionless innovation. So I almost don't want to have an opinion on what the right approach is. I just basically, I think there's something there. And I think the important thing for us on the base layer side is to just make sure the tech is ready, and then we just see whatever ends up making the most sense. Which layer two type architecture is the best for this? What kinds of stable coins are the best for this? What wallets, for example? I'm also really excited about the wallet side, for example, I'm not sure if you saw that the Coinbase wallet, they have a simplified view that really focuses on just stablecoin usage and just hides a lot of the crypto. There's also the clay wallet, there's a dymo wallet, there's the Worldcon people have a stablecoin wallet. I feel like this is just a few that come to mind, but it's really, I think, more about making it trivially easy to use for people that don't want to care about crypto, basically. And then I don't want to be opinionated about what way, what exact path brings us there.
Speaker A: You said two things were exciting. You first were stablecoins. What's the other thing?
Speaker B: Yeah, so as I guess the stable coins, that real world Ethereum side, that's much more something in my spare time I just think about, and I get excited about right back to maybe my day job and the research side. So basically I've been over the last few months working on two topics. So one of them is kind of the more EVM standardization and extension on the layer two side. The other one that I've been working on recently, and by the time this episode airs, we'll also have a big of write up or two write ups. Actually out on this topic is the long term staking economics for Ethereum. And I know that, yes, that's a topic that you've covered extensively as well, both from the general roadmap side, also from the MBI and ultrasound side. I think our approach to this was less immediately focused on the monetary aspects. I think they are actually really important and I'm also excited about them separately kind of how our exploration there started is basically that we just looked at. And you can see that kind of graph basically, if you look at just like amount of total eth staked, right. It's basically another of these up only graphs we basically started. I don't know if you remember, but like when the beacon chain first launched, there was like a minimum threshold before the beacon chain would actually launch. And there was all this worry, would we even hit the level? Exactly. And it was tiny. I don't remember the exact number, but.
Speaker A: It was 150,000 each.
Speaker B: Exactly.
Speaker A: Very low.
Speaker B: I'm not sure, I'm not sure if that sounds about right, but something like that. And then since then, basically, initially, of course, there was still, we hadn't done the merge yet, so basically the merge de risked it, and then we had the Shanghai upgrade where withdrawals were enabled. Right. And so once we kind of more and more de risked this entire process, more and more people kind of decided to start staking. And then of course, you also importantly had the kind of emergence of lsts, right? Very much, I think also changed the landscape on the staking side. And so if you look, it's basically just been a constant inflow of staked eth, right.
Speaker A: This is a question that we used to ask a bunch of EF researchers when we had them on the podcast, is like, hey, where do you think the final equilibrium of percentage of the network staked would be? And the answer is we're generally around, we want 20% to 30% staked because that makes Ethereum secure. And then the rest can go be in Defi. And we haven't asked that question to anyone in a while. But I'm automatically going to guess that now the new predicted equilibrium is much higher than that older predictions.
Speaker B: So yes and no. So basically the older, if you ask me what would be the best outcome for the network? And I can go into why? Because I don't think kind of, I think it's important to ground these in actual kind of, you know, arguments. But. But the answer would still be 20, like I would, I think something like 20, 30 million. That sounds about right. That's like a healthy, healthy amount of stake. Okay, the question is now more like what, what part are we on right now? Right? And that answer is very much different. And so, like right now on Ethereum, I think we are basically at like 30 million staked right now. So that's roughly 25% of all the Ethan existence, right. We have more or less 120 million eth in existence. So it's roughly a quarter, but the trend is nowhere near slowing down. Right. And so if anything, it's the opposite. Exactly. Exactly. And so kind of we're going to have two posts, basically one of them is our kind of general, where is the end game here? What does the end game look like if we do nothing? And what can we do instead? Basically that's the main post, and then we'll have a second one. What to do in the short term, basically. And of course, I think the more interesting question is the long term. I guess it's a researcher bias, but the more interesting is always kind of where we headed long term.
Speaker A: I think we've hopped into a presumption that I want to put a pin in and tackle first before we get to the long term. Why is 20% to 30% desirable and why is not like approaching 99%? Why is that undesirable? Why, why do we not want all of the network staked?
Speaker B: Right. And actually that's, that's quite nuanced. And there's definitely arguments like, I've heard people argue for the opposite. Right. And so just to, just to briefly, because I think in order to answer that question, we have to kind of understand why we, why we might be headed there. I mean, 99% of course, a little extreme, but you know, like say 90%, call it 90% or something like something in that order of magnitude could also be 80, 80, 95, whatever that kind of range, why might we be headed there? And then we can talk about why might that be bad? And that's something I think basically in our article, we're trying to make the point that we think that would be not desirable for the network. But this is how research works. We're saying this now and then there can be counterpoints. People can be like, no, for these, and these reasons, it actually might be very desirable. But why do we even think that this plausibly could be where we end up in, and I think the way to think about it, you might remember this issuance curve of how high is the reward for stakers as a function of how many people stake. It's basically a curve that drops off. It starts really high. So in order to incentivize people to stake at all, if there's almost no stake, then we pay quite a high apy. And then as the more and more stickers come in, it gradually drops and it levels off at a certain point. And if you just look at that curve, it almost immediately, it's almost like an obvious point that basically it's designed to make sure we have a minimum amount of stake. So basically on the left it becomes really, really steep, meaning that if we were to ever start losing stakers actively, at some point the reward would shoot up so high that people would basically be incentivized to stay around. And so it does a really good job of basically enforcing some sort of minimum participation. But to the right, it just levels off. Right. It just kind of like reaches some, it keeps kind of going down a little bit, but it really tapers off. But it's basically more of a, we could end up anywhere here. Like this curve kind of still incentivizes people all the way to kind of 100%. It could be lower if the incentive is not high enough, but it's completely unopinionated. Right. It's like it has an opinion on the left side, it has no opinion on the right side. Basically that's kind of the first observation, right. And then in economics terms, you could basically, in a way, if you think about like the network point of view, right? Like people staking, that's kind of like the demand for that that's set by the protocol with, with how much it's willing to pay. The supply side, the supply of stake, that's basically the people deciding whether to stake. If you look at the supply side, of course, initially when all these systems were designed, they were more designed with just like home stakers in mind. Right? And home stakers, not every ETH holder wants to run a home stake, right? So basically you could say that it's a relatively steep supply curve, meaning that some people are just enthusiastic to be solo stakers and they would stake even if it's only paying a little bit. And then as you get further and further down the curve, you have more and more eth holders that really just don't know how to set up their own server, how to run their own node. They don't really incur the, the slashing risk. And so you get more and more into like people hesitant to stake and you'd have to pay them more and more, right. And so it's a pretty steep supply curve and then a shallow demand curve. Steep supply curve. They still kind of intersect somewhere in a reasonable range. Right. Now, of course we have lsts. And I mean in general, of course I always think that kind of innovation like that lsts, that's amazing, right? Like I think it's always great to see how basically kind of blockchain kind of really drives all economic incentives to their kind of logical conclusion. But of course, one side aspect of that is that now all of a sudden for the margin ETH holder, the decision whether to stake becomes much more de risked. Right now basically, sure, maybe I don't want to run my own solo staker, but going to, I don't know, either a centralized exchange or decentralized exchange and just swapping over to steeth or reth or one of those, that's pretty low friction and of course you still incur the additional smart contract risk, the governance risk, the operator risk. But over time they build a track record and a lot of these kind of systems get more proven out. And so basically that supply curve a is just now much flatter because of the lsts. And also that keeps kind of coming further down and down as these systems kind of have more and more of a track record. And so basically observation is just now we have a very flat supply curve and a very flat demand curve. And now we can no longer say anything about the intersection point, right. It could be that they still intersect at like 50% stake and then we just stay there forever. It could be that it slides all the way to the right. And so that's observation. Observation is just now with lsts, it's all of a sudden just open ended how high the state could get if we do nothing, which for now, as an observation that's kind of value neutral. That doesn't mean that that's a bad world, right? And then kind of in the second part, we were then looking at what are the consequences of that? Is that something that we would like or that we would not like? And we really kind of pretty strongly believe that after looking into that we would prefer to basically not to kind of avoid ending up in a world of very high staking participation. And for a couple of reasons, one of them would be just from the protocol point of view itself and the supplying a base money for the system. We always talk about ETH as the money for of course for securing the system, but then also for Defi for layer twos and supplying all that money. And if we go to a world where most of that ETH ends up kind of at stake, then of course what that would mean that basically now we swap out ETH for some sort of LST, either like one network effect, winner takes most kind of LST, or maybe some sort of synthetic basket, smaller lsts or something. But basically everywhere you use native eth normally, now you just use that kind of LST and it's not terrible, but it basically does mean that we just kind of now no longer have this one completely neutral and trustless kind of base money.
Speaker A: Right. You have flavored ETH at best, right. And flavored is one way to put it, or tainted. Right? Like if Lido takes over the world, there's no way to have the pristine vanilla eth. You will always have to accept this, like Lido ethnic. And maybe that's just not what, those aren't the trust conditions that you were on board for yet. Nonetheless, all of the defi integrations are with steeth, not with Eth. And so you no longer have the option of just having completely trustless vanilla ether.
Speaker B: And of course, just by you saying that, of course, again, it's super important to say that this is not, that's not something that's like secretly Lido scheming this or anything, right? Like, I think actually Lido. I know some of the people, people that it's a great team. I think they're very, you know, I don't want to say value aligned because this whole alignment meme is pretty terrible. But. But you know what I mean? It's not about them doing anything nefarious or something. It's just how the incentives happen to fall out. And I still think basically the arguments are that ideally you just don't have this extra level of intermediation. Now, basically all the eth holders also have this extra kind of governance that they kind of rely upon. There's some kind of extractive fee that's taken from everyone. It's just not ideal from that side. So that's argument one. I think we have four in total that we talk about in the article. So argument one is that then argument two, just a very of old school MBI argument, right? Like minimum viable issuance. It's just, you know, the whole point of issuance in the first place is to pay for security. Arguably, we already way past the point of the security level we need. I think there was recently kind of also this academic work looking at cost of attacking networks. I think there's been a lot of people looking into this and the conclusions usually are always, we are already like way past the point of needing the security. Right? So basically, just from the very basic argument of like, hey, paying for more than we need is just unnecessary. So that's argument two, then argument three. And that's actually, I think, one that I'm personally also kind of passionate about is just solo staker viability. And so of course, again, the whole story was the Ethereum network was originally designed for solo stakers. And then it turns out that actually pooling and lsts are very attractive for a lot of people. That's perfectly fine, but it's very important for us to make sure that solo staking remains a viable option. And there are a lot of aspects that just get worse and worse as we go down this participation curve and we reach more and more for one. Of course, most of that new inflow is all lsds, basically because we have this steep supply curve for solo stakers. Most Eth holders that are willing to run their own server already do that today. There might still be some trickling in, but especially because also the net rewards will keep going down. Basically, someone who hasn't staked at 4% yield in the past with their own server probably won't do that at 2% yield. So basically, there's going to be almost no new net inflow of solo stakers at those levels. But more importantly, basically, all the disadvantages that solo stakers have over lsts get worse and worse as there's basically more of lsts. Basically. For one, the big one is the liquidity penalty. If you solo stake, your ETH is just locked up. It's not liquidity. It hopefully in the future maybe you can still participate in restaking and whatnot, but you can't take it and use it in defi, right, you can do that with lsts. And as LSTs become more and more dominant and kind of replace eth, as there's the default money for the entire eth ecosystem, not just on l one, but also in l two s and whatnot, the liquidity, kind of the convenience value of having this liquid lst more and more just approaches just eth itself, right? It's basically at some point just as convenient to hold an LST as it is to hold Eth. And then basically the gap to no liquidity widens more and more. Right. Today, for better or worse, lsts are not quite as useful as Ethiopia, so the gap is smaller, so the gap goes up more and more. You have more economies of scale operating. LST is more capital efficient as you have ten x the amount of ease under management, whereas as a solo staker, you always run your three or five or whatnot, solo stakers, and it's just always the same cost, no matter how much overall participation there's in the network, right? So you have economies of scale. One thing that I'm personally a little worried about, even though it's a bit more speculative, is that as say, there's one dominant LST, which I think, unfortunately, the network effects would tend towards one winner takes most LST. You could imagine that people even start to believe that it's too big to fail. If you have one lsT that has 80% of all the Eth. If they get slashed, well, we're going to save. We're going to bail them out. I'm not saying I personally would probably oppose that, but I'm saying that it's really hard to break that perception. So once you get to that point, then you might as well get to the situation where people start looking at the actual risk of putting their money there. And they just say, well, effectively it's zero. Either nothing bad happens, or if something bad happens, everyone is affected. So then as a solo staker, you still have all that risk. You still have to make sure that you do everything right. You could get slashed. You have to be online to not miss a station. All these kind of risks that you incur at a solo stake. Now on LSD's, it's basically risk free, or at least people might just treat it that way. So basically, there's a lot of subtle points that get worse in terms of solo stacker viability. One important one, actually, and that's also our fourth argument, and that's combining the monetary aspects and the solo staking. So basically, this is one of our graphs that we have in that post that will be up by the time of this episode coming out. And so basically what this shows is you see this kind of faint gray line at the top of the graph. That's kind of your issuance curve on Ethereum as you're used to it. So basically it is the nominal amount of ETH that's paid out as an APY to stake us, depending on the total amount of ETH staked. Now, the observation basically is that as there's more and more people staking, more and more of that ETH is basically just dilution. So basically you could think of it is if everyone in Ethereum would stake, and you pay out 10% of ETH every year, well, everyone's balance goes up 10%, but it's meaningless, right? It's just an accounting kind of change at that point, because it's just equally distributed to everyone. Whereas of course, if almost no one stakes, then basically the total Ethan existence is mostly flat, but you get your rewards, so the rewards are real rewards, basically. That's why. Yeah, so basically this kind of dilution effect becomes more and more relevant as more people stake. The reason why. Basically, people in the past always talked about the nominal issuance curve. Is that in terms of just thinking about how much of an incentive is there to stake? In ethereum, the nominal is the thing you want to look at, because if you don't stake, you just hold raw eth, you get diluted. If you stake, you get diluted, but you also get the nominal yields. The difference is still this full gap. But basically a better way to visualize this is what we do in the graph here, instead of just showing it as this gray curve. Now, basically the green curve, and basically the dashed one is the one you're actually interested in because it includes the MeV gains that you can have as a staker. That's basically just the positive side. And then instead of adding this nominal on top, we now have it below the x axis. It shows you the, the dilution. We call it the real eth yield, the yield of just holding etH. Right. Now, for example, you can see at the current levels, that's the dashed line, the vertical line. At current levels, if you just hold raw eth every year, you get diluted by 0.8% or something. So basically, your eth, if nothing else happens, of course, this also ignores the 1559 burn. But 1559 burn is equal for everyone involved. So you can completely ignore it from this analysis. But basically, if you do nothing, ignoring this 559 burn, you basically lose, your eTH loses 0.8% of value every single year. That keeps growing. What this picture really clearly gives you a really good intuition for is that as we go further and further to the right of the graph, more and more of the incentive to stake is no longer something where you can actually make real money. It doesn't actually generate profits for you. It's really just about avoiding being diluted because everyone is more and more diluted because we have so many stakeholders. Basically, the incentive to stake now is just to escape the dilution, right? So it's still the same incentive. So nothing changes in terms of the analysis of who would stake and who would not stake. But, but in terms of how desirable is this outcome, it means that in the end game, we might end up at like 100 million eth staked. And almost none of that is like, none of the gains you get at that point is actual, real profit every year. Most of it is really just not having to be one of the poor suckers that basically on the, on the, the raw eth holder side that have to pay for that party, right? Basically. Okay.
Speaker A: So I think, I think the point that you're trying to make is, as we get a higher percentage of ETH staked, there's less opinion in the curve. And you, like you said at the very beginning, it's very opinionated, but as we go on, it's less opinionated. And so, like as you go further down the spectrum of a higher percentage of ETH staked, there is a larger incentive to simply just escape dilution, even when that doesn't add anything to Ethereum, because the net incentive to stake doesn't really change much between 60 million ETH staked and 120 million ETH staked. The net incentive to stake is about the same between those two points. But really, as you approach the incentive to get away from dilution flips and people will move away from getting diluted into Ethereum staking, and it will just accidentally, naturally end up in a point where almost the entire network is staked. And that's, and it's an un outcome. Whether it's desirable or not is a further conversation, but is an un outcome. And it's kind of what you're saying is like, it's kind of like a gravitational pull, like you may just end up there.
Speaker B: Well, it's not so much. I feel like this picture is already more on the conversation side of like why might this be bad? Like, basically because you can just look at the nominal yield just to make that prediction, right. Basically, even if you just look at the gray line and you ignore all the colors in this graph and the part below the x axis, you can already come to the conclusion that this might very well go to a very high percent of each state. But what you can look in this picture, what you can basically take away from this picture at today's level, right, at the dashed kind of vertical line to the left, most of the APY that you get every year is actual real APY, right? Like basically you can see the nominal APY is 4%, but like 3.5% or 3.2% or so of that is actual profits that you make, right? Compared to basically, compared to just not being an ETH, but ignoring kind of price fluctuation. But then a small portion of that is already just basically you escaping the kind of the fate of your raw eth, slowly losing value. Now if you go to the right of the graph, at some point this flips and towards the very right, you still have the incentive to stake. Like this is not, this graph is not about the incentive to stake, it's more about how the composition, where does this incentive come from? And so towards the right now, basically most people stake, but they don't stake because they actually gain anything from it they basically just avoid losing something so it's basically just. It makes everyone in the system worse off is basically kind of the argument here right the eth holders now are worse off because they get diluted quite a bit and the stakers are worse off because they now no longer make much net profit actually they're just basically just avoiding the losses right and a.
Speaker A: Decent part of the network also takes a loss because they have to accept the conditions of some LST provider probably like Lido or rocket pool or something some conditions that they really weren't interested in the first place but they were.
Speaker B: Pulled to anyways right and so basically kind of what we were looking at all of this and also by the way you can see that if we go to the right then as a solo staker also at least to the extent that if you're already of course like you're committed to being an ease holder then you don't really care but because then you don't have a choice right, you kind of have to stake but if you're maybe more like you're looking at it more from a business analysis point of view then you might actually still be interested in your profit rate and then you can see that towards the right also more and more of your real returns come from mev. Right and that's also another bad aspect because it just increases the volatility of your returns as a solo staker quite a bit. So now there's another forcing function towards LSD's away from solo staking. So we were looking at all of this and we were basically just coming to the conclusion that we would just much prefer staying in a similar range to where we are today where basically just most of your yield is actual real yield and there's not much dilution happening everyone is basically happy. So we were trying to figure out how can we get there. What you can also find in the write up basically is just our argument for why. The best way of doing that would be with what we call targeting basically stake participation targeting the idea is instead of having an issuance curve that's just unopinionated to the right we choose an issuance curve that is opinionated on both sides. It basically to the left it makes sure that there's always a minimum amount of stake but to the right there's also like. It makes sure there's always a maximum amount of stake basically so basically again this is just about giving you an intuition right.
Speaker A: I feel like I'm in middle school algebra or calculus or something.
Speaker B: Yeah, yeah, but, yeah, again, maybe this is too much detail, but again, I think we're just very passionate about it right now. And so we think it's a really important topic. So, basically, you can see the important curve is, again, the green curve. The green is the real yield from issuance. And then the green dashed is, if you take MeV into account. So, actually, the green dashed is where the net incentive, you can see that basically on the left side, it starts similar to what we did in the past, where basically it starts really high and then comes down. But then basically, instead of just tapering off and getting closer and closer to zero, it actually becomes negative. So over a certain point, you would basically start charging people to be validators. Of course, the point is you would never actually get there, because once we got even close to that range, people would stop. There would no longer be an inflow of stake. But basically, that's only guaranteed by having this extreme point. So basically, we create this extreme point on the right. No further than this, please. And that way, basically, we make sure that we would always have an equilibrium somewhere in the middle range, between 0.2 and 0.3 or something like that. And of course, this is just an example. We could choose whatever curve we want. It's more about picking something that we feel like always guarantees we are in a target range that we are okay with, we like and then just setting it and forgetting it. And also, I think the motivation here is when the beacon chain was launched, a lot of these decisions were just made to get it off the ground. Good enough plausible decisions. And I think we are now at a point in ethereum where we really want. Want to start figuring out the endgame. We want to basically make decisions that can last us a couple of decades. I think there's also always this kind of criticism of ethereum, of like, hey, you guys, if you keep changing everything every few years, then it's just really hard to depend on anything, and especially on, say, on the monetary side in terms of the properties of eTh. It's kind of hard to trust a currency if basically the base kind of monetary and policy behind it changes every few years. I think we're really at this point now where we're looking at it and we're like, okay, we're just not confident yet that this kind of good enough solution to get the beacon chain off the ground, that that is yet kind of this multi decade, stable, long term policy. But we want to jump there now, right? And so that's kind of where this entire conversation is supposed to lead us. And so that's kind of, that's kind of the big proposal. And one maybe side aspect there that I personally I think is really interesting. You know how like I think maybe, I'm not sure if you had John Chop on the podcast, but I definitely know if you've mentioned him before, how basically he has this argument that that protocol starting to basically just natively take user EtH and just by default staking it, right. That is basically an inevitable outcome. And I think he's kind of, in a way almost right where, sorry, the suns. I think he's kind of in the current world. I think basically he has a very strong argument for this where basically it's just, I think we've already seen this. Some protocols kind of starting to look into this. Maybe some of them that are a bit more loose with language call this native eth. Native. Native yield. So basically the idea is you're on a layer two, you still have something that's called eth, but under the hood it actually is all deposited in lido or somewhere where it earns yield just naturally for you. And indeed the whole point is that, as we were saying, there's this continuous pressure to keep increasing, keep having net inflows of stake. And that's just one example of that. In the current world, over time staking will be derisked enough that that will be the rational choice, or at least it could very plausibly become the rational choice. And once a few protocols start doing it, you know how competitive dynamics play out, then other protocols will be under pressure to do it as well. And maybe even if there's still some tail risk, it's always very hard to be the only rational player in the market that doesn't ignore the tail risk. Oftentimes you just get out competed by the more reckless players. So basically it just naturally creates this dynamic towards protocols doing this. Whereas if we do targeting the thing that I'm personally really excited about from monetary point of view as well, that it basically makes, it gives native yield to all the eth, right? Because basically the insight is if we do nothing, we get to this world where everyone stakes anyway, so all the yield is distributed to anyone in the entire ecosystem anyway. So you might as well front run that. We might as well kind of embrace that. And with targeting, basically we only pay the marginal rate to stakers. That by definition, right, by definition, what does it mean to be in the equilibrium at like say a quarter participation? It means that for three quarters of ETh holders, it's not rational to stake, right? Three quarters look at the current yield and are like, ah, that's not enough for me to stake. It doesn't account for the risk, whatever, right? Only for a quarter of the most enthusiastic stakers. Hopefully most of those will be solo stakers, because we've seen that those are the most kind of the earliest stakers, right? That only for a quarter of them, it makes sense to stake. And so basically, if you basically compare, there's barely any yield pass through on that level, because by definition, most people would prefer to be in raw Eth. Now, where does all that yield go to then? Well, basically you just kind of automatically get a redistribution of that yield to just all the ETH holders. But that's not why we're doing it. We're doing it again for these targeting reasons. It just basically falls out as a necessity. You can't pass through too much to the validators otherwise, you just keep having this kind of inflow pressure. If you want to take away the inflow pressure automatically, most of the yields just end up with ETH itself. It's similar to 1559. You could basically imagine that, similar to the burn. Now you have something like an Mev burn or a restaking burn or something. That's just, it falls out of the protocol. Now, basically any ETH holder just gets access to that native yield that otherwise, basically you only get through these indirections that I'm personally very excited about.
Speaker A: I think it's super elegant. And you're doing the thing that I think a lot of, of protocol upgrades, proposed protocol upgrades do, which is like they have this large reservoir of ETH holders holding vanilla ETH, and we are just like dumping a lot of excess into that. And that excess is like mev burn. And now it's also talking about just like the yields from ETH staking. And what would ultimately benefit something that would kind of become pseudo enshrined. Like to say, for example, if Lido took over and SE was 100% of the network, uh, well then everyone, it's, it's the same, it's the same destination where everyone's getting it. But now, without having this pseudo enshrinement from something in the app layer, the Ethereum protocol is kind of just like taking more control over its own destiny with its economics. And the net effect of it is like it goes back to kind of like a user sovereignty thing, user prioritization, where if you are just the random, uh, Joe Schmo holding vanilla eth, you're getting all of the benefits of anything that would have emerged otherwise in this previous equilibrium. It's a very EF themed, which is why somebody from the EF is bringing for it proposal.
Speaker B: One aspect that I did want to mention, because actually I think it's really important, again, it's really important to have this actually be something where we believe that that's actually the right thing to do. And one aspect to mention is that if we do something like targeting, it would also result, as I was saying earlier, kind of by necessity, with lower net yields, just because it has to reach this point where only for a quarter of the network, it's still worth it to stay. And that also means that we run to the same kind of volatility issues that we would run if we do nothing. So basically for a solo staker now, because these big pools, they always smooth the mev, right? But as a solo staker, some blocks just don't give me any mev, and some blocks give me a lot. And basically only occasionally. It's like a lottery system. It's bad if, unless you're into lotteries, but it's bad if more and more of your yield kind of comes from that uncertain source. And so in a way, that's the one aspect of this entire kind of complex of problems of doing nothing that we would not address by targeting. So basically we have other solutions for that in place. I think you've already talked in the past, or had research in the past, talk about Mev smoothing, Mev Burn. That's kind of a technology that would get rid of that problem. We also now very excitingly of research into what's called a tester proposal separation and kind of these execution tickets approaches where basically you get the same thing. You basically also auction off the right to the block. And so you also get kind of smoothing out of that. So it's important to point out that this in itself does not solve the problem of volatility for solo stakers. We still have to address that. And it's important to be upfront about that because this is really important and it's important for us to make sure solo stakers in a good place here, maybe as a last point to wrap this up, then this all kind of, this entire conversation is more. We have this article out now. I feel like we just felt like there hasn't really been a lot of conversation about the long term staking comics since we launched the beacon chain. So we want to bring it back up and present our thesis here, it might take a while to get there. First of all, of course, as a community we have to agree. Do we like this? Do we not like this? Are there counter arguments? We have to hash this all out. Then we have to choose a specific we have to figure out for most, especially if we want to have a curve that potentially goes negative, that actually requires quite a bit of rework of how the entire beacon chain works. So this is not something we can do immediately. Now we are a little bit concerned, or actually moderately, quite a bit concerned that if we do nothing for too long, you know how these timelines are for features. If we do nothing, the next hard fork will be in. We have one hard fork coming up in a couple of weeks, but then the next hard work will be optimistically end of this year. Then maybe the hard work after will be end of next year or something. So there's not that often we have an opportunity to change things. So if we don't do anything in the next hard fork, which will be by the end of the year, that could mean that basically we are still stuck with the current curve for two more years. It's not the end of the world, but it would mean that probably with the continued pressure for more inflow, we would end up with a considerably higher staking ratio until then. It also means that a lot of protocols would already feel pressure to maybe start moving to this staking by default model, which then later on they might have to go away from again. Once we do targeting, it's really bad to move away from the world we want, then move back to right now we are already close to where we would want to end up with, and it adds a lot of friction. It also means that for a while we would basically have to really, really have very low yields to keep pushing people back out. And it's just unfortunate for everyone involved. We looked at this and we weren't happy about it. If it ends up this way, it's not the end of the world, but it's not great. We worked a little bit. Whenever I say these two articles that I mentioned, I did that with a colleague of mine called Caspar, Caspar Schwarzschild, who's also great. And then we have another colleague who actually has been working on this kind of general topic for a long time, Anders. And Anders kind of worked a little bit more on this what can we do now part as well. And he proposed a specific kind of issuance curve. And I can send you this one more as one more picture so you can see here, this is the curve that Anders proposes. It basically is only small tweak to the current curve. So it's very elegant in that way. It can be implemented on the client side with very low effort. So it actually is, it is a feasible candidate for the, for the upcoming electra hard fork in at the end of the year. It does. You look at it and you probably immediately say, wait, but it doesn't do this thing where on the right side it kind of has an opinionated cutoff. Right. And so indeed, it's not necessarily meant as like a is. It in itself does not give us, like, certain targeting. But what it does do is it alleviates a lot of the problems in the meantime. So for one, you can see the faster effect. Yes. So it's an intermediary step. And basically, this is our question to the community. Either we do nothing for a while and then have a painful climb back down, or we kind of accept the extra messiness of having one intermediary step, basically. So that's kind of our question to the community. And the nice thing here, if you look at it, basically, just to briefly walk through kind of the advantages of that over the current curve, you see the red line. That's the kind of the dilution effect on Ethel. You can see that it initially, of course, starts at zero and then kind of goes down, but then it reaches like an equilibrium point at -0.4% or something, and then it just stays there. Right. So basically it means that the total amount of ETH issued as rewards just basically reaches a cap and then doesn't grow any further. And that has the nice effect that even if we keep sliding further to the right on this curve, it's not so bad, because it doesn't. It means that the kind of the part of the incentive that comes from dilution and also the kind of dilution pressure to ETH hold this today is capped, and it's even better than today. We're already at -0.8% so it would already have the dilution that Ethel has experienced today. So that's kind of. .1 it also doesn't change the incentive stake so much at today's level. It would be like a 30% decrease or something so as to kind of make sure we don't slide too much to the right. But even if we keep sliding under this curve, the incentives fall off much, much quicker. So basically, because, again, we don't keep issuing more and more ETH, the total amount of ETH issued per year, it kind of reaches this maximum and then it just distributes across more and more people.
Speaker A: So as you can see, new incoming stakers actually dilute other stakers more aggressively. And that's the main difference between these curves.
Speaker B: And so basically it means that for example, if you go all the way to the right now, for example, let's say at 110 million or somewhere like close to the maximum, now basically the difference in incentive to stake, all of a sudden it's like a three x difference. So it's like only a third. Basically we got rid of two thirds of the incentive to stake at that level. It means that likely it would take much longer for liquid staking to get enough of a network effect to still push us there. And so that means that it really seems like a very good candidate for this kind of intermediary step. Now, Anders is a bit opinionated on this, and he would argue that it could even be good enough. There might be arguments that maybe empirically we see that it's just good enough. It effectively gets us as targeting. I still think that over time we would want to move to this set it and forget it curve where we never have to come back to this ever again in the history of Ethereum. And I think that would be really nice. But this curve I personally, basically our work here has the one we argue doing nothing forever is bad, we want to do this targeting, and two, we think ideally waiting until we can do the full targeting would be bad because it then gives us this painful will walk back down. So we would propose to do this already, then we would basically do this in a year, in the hard fork in a year, and then the full targeting in two years. But again, this is just the conversation starter. But I guess it was a good coincidence. Like we didn't schedule this for the podcast, but literally we are about to publish this tomorrow or the day after or something. And I don't know, that's basically been my main, what my brain has been at for the last few weeks.
Speaker A: This is great. I can't believe we're opening up the conversation of Ethereum monetary policy again. But I always kind of knew in the back of my head, like, you know, we're not just whatever curve, whatever parameters that we set at the genesis of proof of stake was not going to be the final thing. I always knew this day would come where we're going to change the monetary policy for ether one more time. At the very least, I guess, I guess now is the time.
Speaker B: At the very least it's important to have the conversation because look, if people are out there and they think, no, it's actually good, if everyone stakes, then we can have that conversation. There's also independent work on, like, how can I, we say, give solo staker some liquidity for their stake by limiting how much it can be slashed and whatnot. So that is a worthwhile conversation to have. But the thing is, we just have to get to a point where we know that whatever issuance curve we end up with, maybe even the one that we already have today, that we now are confident that that's the one we can stay on forever, that is the point we don't have yet. Maybe the current one is good enough. I argue against it. We argue against it in this post. But we just have to get to the point where people can trust that this is forever the set in stone issuance policy for the Ethereum network. And I think that would really, really be very valuable.
Speaker A: Cool. Anzagar, I love this story arc of this conversation. So I'm really glad you brought it with you here today on the show. So we're going to put all those links in the show notes if listeners want to dive into the blog post or research report and see some of the graphics that we were looking at here on the show today. Ansgar, thank you so much. If people want to learn more, if you want to point toward people, towards a certain destination on the Internet, where should they go? Either read more or learn about you or wherever you want to point them.
Speaker B: Oh, yeah. I mean, I think I really would say, ideally, if you have the time, dive into, especially the long term post, the post on where would we want to go in the long term? Think about the arguments. If you disagree, that's actually good. I think it's really important that we kind of don't have this be. Sometimes people are always like, oh, the Ethereum researchers just set the roadmap and just no one else really kind of chimes in. And I think it's really important that this is actually a community conversation that we have. So go to that blog post. Also, again, I think kind of early in the conversation, separately, again, I'm really excited about saver coins. Go find Liam's post. Read that as well. It's really exciting. If you want to find me, I guess these days, actually the primary place would be farcaster. So find me at Ansgar eth over on Filecaster and let's have some fun.
Speaker A: Over there, and we'll get all those links for the show notes. Ansgar, thank you so much. My man.
Speaker B: Thank you so much, David. Have a good day.
