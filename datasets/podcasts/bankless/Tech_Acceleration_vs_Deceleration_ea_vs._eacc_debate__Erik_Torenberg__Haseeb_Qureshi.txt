Speaker A: If you look at the companies that are actually accelerating, they're mostly eas, which kind of fucks up the narrative, which is like, oh, the eas are ruining everything. They're slowing everyone down. And it's the accelerationists who want to go fast. The accelerationists are mostly VC's. They're mostly not building anything. They're just on the sidelines cheerleading for this imaginary team that they think is in the driver's seat when it's reality. It's fucking EA people who are even thinking that this was possible in the first place, right? It was all the VC's who were not investing in any of this stuff because they thought it was so impractical and it was worth is the eas who believed that AI risk was a real thing because AI was going to become really powerful really quickly. Who created all this shit?
Speaker B: Welcome to bankless, where today we're exploring the frontier of the tech acceleration versus deceleration debate. This is Ryan. Sean Adams. I'm here with David Hoffman, and we're here to help you become more bankless. The question on today's episode, I think a question that is everywhere right now, should we accelerate or decelerate our tech progress? And how about when it comes to something as powerful as AI? This isn't just a debate in tech circles, at least not any longer. This is now a political debate. I think it's poised to define the next decade. We've had leaders from both sides of the debate on the podcast. You might remember our episode with Eliezer Yudkowski, who argues very much from a AI safety perspective, and Beth JSOs, which is almost the complete opposite of that. So you've already heard what they think. Today's episode is more of a synthesis type episode. This is a debate on these two opposing opinions. To help you think through where you might stand on the issue, we have Haseeb Qureshi, who tends towards the EA side of the conversation, which favors more caution, more regulatory intervention around AI. And we have Eric Thornenburg. He tends more towards the IAC side of the issue, which is a bit more tech forward, faster progress, and a lighter touch from our regulators. We talk about the history of the debate, the cultures in Silicon Valley. We talk about Sam Altman. We talk about the regulators, the big bad us government SBF, and what they think is the most sensible position on the acceleration versus deceleration debate.
Speaker C: One of the reasons, Ryan, why I was really motivated to get this episode out here is because I think right now this conversation is relevant to both crypto and AI is about, like, accelerationism. And are we pushing the gas? Do we want to push the gas further? It's now just relevant to these two industries. I think there's more industries that is going to be relevant for beyond just what the scope of what this conversation is. I think this is like a sign of things to come. There are other industries that are also going to present some very hard conversations to humanity and therefore government. And our tech sector is going to be coming out of the tech sector. When we talk about accelerationism, the world is going to look so much more different in 20 years than any other 20 year increment that we've ever had before, and that's only going to increase. So we have biotech and gene editing on the horizon. We have longevity. People like Brian Johnson literally trying to become immortal. And I think AI and crypto are like banded together as kind of these accelerationist movements. But the thing is, I think these are the first of many industries to show up on the accelerationist side. And this conversation about this, like caution versus gas, do we press on the gas pedal or do we hit the brakes? That is going to be a more. It's going to define society more and more and more, I think, as we move forward into the future. So that's the kind of the frame of thought that I hope listeners go into in this conversation.
Speaker B: I completely agree. This is the political debate of the decade, but it's also probably the political debate of the entire next century. So a lot of podcast episode fodder for us in the future. All right, guys, we're getting right to the conversation, but before we do, we want to thank the sponsors that made this episode possible.
Speaker C: Bankless nation, I'm excited to introduce you to Eric Thorenberg, an investor, technologist, and fellow podcaster. He's got a podcast network called Turpentine Media. Lots of excellent podcasts in that network at the intersection of technology, culture, philosophy, social commentary. Eric, welcome to the podcast, my man.
Speaker D: Thanks for having me. Longtime bankless fan, the stoked to be on.
Speaker C: You know, actually, before we introduce your fellow guests here, I actually remember a very early podcast episode that you had Ryan on as a guest, actually, Ryan and Cyrus Yunessi, I believe, which is when some of the formative years around, like ETH is money as a topic.
Speaker D: Amazing. Yeah.
Speaker C: Joining Eric here on the podcast today, bank listeners probably know Haseeb Qureshi, another investor, technologist, podcaster, who we've had on the bank list many, many times before, gp over at Dragonfly, and is also in the effective altruist camp, well before anyone in crypto even knew what that meant. Haseeb, welcome back to bankless.
Speaker A: Thanks for having me, guys. Always fun to be here.
Speaker C: So, just to lay some foundation for this episode, I think what this episode is going to be is kind of is going to evolve on its own. But really, we are watching two camps out evolve in the tech space, and it's also migrated into government as well. But there's also some nuances. This is the accelerationism versus decelerationism camp. There are some other tribes inside of these tribes. There are some adjacent tribes, but overall, there's identities forming in the world of tech and governance and where we are going as a society in the future. And it's causing arguments, it's causing debates. The accelerationists want to just hit on the gas pedal and innovate, innovate, innovate as fast as possible. And the decelerationists are like, ooh, scary. Let's regulate. Let's regulate these things. Now, I don't think we've totally seen perfect identification or identities form around these tribes. I think this is all new for Silicon Valley as a whole, which is why we want to bring you guys here on the podcast today to kind of, like, suss out and, like, figure out this landscape of these growing alignments across tech, across Silicon Valley, across politics. Think we can do that here on the show today?
Speaker A: Let's do it.
Speaker D: Perfect.
Speaker C: So I kind of just, like, laid down a whole bunch of stuff. Different identities, different values. Maybe, haseeb, you can kind of get us started here, because you are an effective altruist. Well before anyone in crypto knew or cared what that meant. And that was one of the earliest call it tribes, both in Silicon Valley, in the venture space, and also just broadly, maybe you can kind of identify about the formations of that whole camp and how it's innovated or just been a part of the whole Silicon Valley tech sector.
Speaker A: Sure. So, I think effective altruism for most people, it came into the popular imagination with SPF, which now it's kind of an irrevocable taint on the movement and on the ideas behind EA. But EA came originally in around 2011, 2012, mostly around Oxford and Cambridge, from a small number of philosophers who were very interested in this idea of trying to apply a more rigorous form of ethics and morality to philanthropy. It originally started with philanthropy and broadened into a number of other ideas. So it starts with folks like Peter Singer and a small number of who's the guy who was affiliated with SBF, there's Will Macaskill. Macaskill. Will Macaskill. That's right. Will Macaskill, Toby Ord, a few guys who all basically came up around that time who started writing about a lot of these ideas in public. And the origination of the EA movement is very close to the rationalist movement, which was also founded by Eliezer Ukowski, who also has a lot of ties to AI dumarism. EA, though, is a little bit different from rationalism. Rationalism is the cohort that really came out of Silicon Valley, whereas actually EA mostly came outside of the tech sector, mostly came from universities. Now, ea, the core ideas behind effective altruism, very simply, are that most people, when they think about doing charity or doing good for the world, they kind of turn their brains off. They're basically guided by their emotions. They kind of donate based on mood affiliations or what makes people look good or what sounds good, and they don't think as rigorously as they do when they are, for example, doing science or technology or politics or, I don't know, maybe not politics, but when they're building businesses, for example. Right. So when you're building a business, you a b test, you look for statistical significance. You're like, very careful in making sure, does this thing actually improve my product? Does this thing actually get the thing that I want? And if not, you are very, very careful to measure the difference and allocate resources accordingly. But when most people do charity, they're just kind of like, well, you know, here for the teaching violin in the inner cities. That sounds good. Sure, let's like, go start a museum. That'll be good for kids learning how to read or whatever kinds of things that they sound really good, but are they actually good, and how good are they? And so Ea kind of came up as this very scientific, engineering heavy approach to thinking about how to do ethical actions in the most rigorous way. And there's a whole framework around it. I don't think we need to go into the details, but one of the things that came out of EA, which is now very, very relevant in the world of Silicon Valley, is for a long time, eas have argued about what is the most important cause area that actually can move the most impact in the world over a short amount of time with a small amount of resources. And many people talk about global poverty, animal welfare. But one of the most popular cause areas for a very long time, like almost ten years now, has been AI risk. The idea being that as people develop more and more powerful, AI's AI's might be so powerful that they could end up disrupting society or even causing mass humanity extinction. And this was before chat DPT. This is before any of the modern acceleration in AI advancement that we've seen over the last ten years. And so people have been working on this, a lot of small groups like Miri and Chai and these very weird kind of people in the back room doing stuff that was very disconnected from Silicon Valley. Only now with the rise of large language models and the sense that everybody has of like, oh shit, this is actually happening. And this is real hash. The discourse around AI risk started to really intersect with society, with tech, with economics, with politics. All these things suddenly matter. Now that's kind of been happening in a backroom for about ten years, that part of EA. So I was aligned with these ideas basically like ten years ago, before any of this stuff was really on a global stage of thinking. Hey, you read these arguments. There's a very famous book by Nick Bostrom called superintelligence where he sketches out a lot of the in principle arguments of how rapid AI advancement might cause some degree of existential risk. And you read the arguments, you're like, yeah, this kind of makes sense. It's not a slam, it's obviously correct. It's not that clearly this will happen, but there's at least a risk that it will happen. And nobody seems to be thinking about that risk besides these small cohort of weirdos in the East Bay or in Oxford. And so people thought at that time, oh yeah, you should donate some money to these people, because this seems like a very underexplored risk that right now society is ignoring. Now, today it's quite different. Society is not ignoring that risk now. It's something that the White House is thinking about, which is kind of crazy, right? Very recently, Paul Cristiano, who was the head of, he's a very, very early ea, he was the head of safety at OpenAI. He left OpenAI. Now he is at NIST. Nist recently hired him as like the head of AI safety at NIst. So now at this point, it's very firmly entrenched in at least the us government. Is this concern that, hey, AI safety is a real thing. And that's not just, it's not just sort of AI bias or AI replacing jobs, it's also catastrophic risks that might arise from the rapid advancement in AI.
Speaker C: Yeah, I think probably listeners are generally familiar with Givewell, which is this charitable organization that does a lot of research for understanding. If you give this one charity, $1. How effective will that $1 actually go into, like, improving people's lives? And then, like, this is where famously, like, they came down to, like, malaria nets. If you actually want to like, put your $1 and save the most number of human lives, you would just buy malaria nets for, like, kids in Africa to prevent them from getting malaria. But this would also, like, this is a very effective and rigorous process of understanding how to capital allocate in the world of charitable giving. But I think as we are working on defining the landscape of all these different, like, tribes and identities about, like, the future arises here. I think, haseeb, what you're also alluding to is that, well, there's like, this, like, risk off, low risk research and pragmatic capital allocation strategy in the world of charity. But then there's also like, okay, well, what about some like, venture bet future high impact things as well? So not just about like, hey, current kids could be impacted by this, but we could also have future generations, and not just next future generation, but literally every single future generation. So this is how the world of effective altruism got interested in AI risk, because that's literally the whole thing. It's all of human society. And so effective altruism is kind of taking not just a pragmatic approach to charitable giving today, but also taking venture bets in charity or venture bets in effective altruism. How would you feel about just that add on right there on that for a second?
Speaker A: No, I think that's exactly right. Most people, when they think about EA or they hear these recommendations among EA, they're immediately turned off. There's a few reasons why people immediately get turned off. One is that, well, who are you to say what's the most effective and what's not? It seems like you guys are kind of these big central planny type people, and naturally people have an aversion to that. One thing to understand about EA is that EA is making recommendations at the margin. EA is not assuming that most people will follow EA, because they don't. And that would be stupid to assume that everyone will do what you tell them to do. So you always want to be thinking, what should more people be doing at the margin? And you'll probably influence 1%, maybe 2% of people at the most. If you're influencing everybody, like, let's say you are the government, then, okay, the way you allocate capital and the way you make recommendations should be very different. But if you are saying, okay, I can basically influence about one to 2% of people, what should one to 2% of more people do, given where dollars are already being allocated? And the answer usually is, well, okay, there's some obvious things like we should have schools, we should have sanitation, we should have. And so somebody might say, well, are you arguing that we should not have schools or not have sanitation? Instead we should all donate to malaria bed nets. The answer is no, obviously not. Given where the majority of dollars are being deployed today, where should the next dollar go? That's the real question that EA is tasked with trying to answer. So given that backdrop now, you might ask the question of like, well, if you donate to an anti malaria bed net, okay, I guess that makes sense. You save somebody's life from dying of malaria, that's a very cheap way to save somebody's life. But donating to AI, risk, maybe AI is not a risk. Maybe there's only a 10% chance that AI is a risk, in which case you'd be wasting that money. And again, the way that EA frames this is that you're thinking more about a portfolio of good as opposed to thinking, well, definitely this good will do this much, and this good will do that much. Most people, when they're thinking about, okay, I want to do one thing. I want to eat lunch. And if you're going to eat lunch, are you going to take a risk on like, maybe this random restaurant is like an amazing restaurant and I should try it out? Or you're just like, look, I know that if I go to, I don't know, Denny's, at least have a decent meal. I don't know, depending how you feel about Denny's. And most people are naturally very risk averse when it comes to their own lives or the things that they do with their time. But the idea is that when you have a large portfolio of giving, of all the giving in the world, you actually want there to be some people who are doing high risk giving because sort of high risk, high reward. If nobody's doing high risk giving and nobody's doing stuff that looks kind of weird or that may not pay off or that may not even matter, but in the times when it does matter, it really, really matters a lot. So the analogy that a lot of EA people give is that this is something like nuclear research. If you were in the 1930s and the 1920s and you were thinking about nuclear research at the time, it was this relatively boring scientific thing that nobody really understood why this mattered until they saw the atom bomb and they saw the destructive power of nuclear energy. There was a time that this was considered to be very speculative, that this was even possible at this kind of scale to have this much destructive damage from this technology. On the off chance that it turned out to be right, that the order of magnitude estimates of this stuff was correct, then you would really want there to be a lot of research into the safety of how to prevent nuclear proliferation, how to prevent mass nuclear scale wars. But people may be under invested in that. And as a result, people in the fifties, sixties, seventies, kind of lived under this specter that maybe there would be maybe one or two more decades of human existence and then the world would just wipe itself out. And for all we know, maybe we weren't that far from that happening. Maybe we were kind of various times on the brink of massive amounts of human damage and catastrophic risk from this proliferation of nukes. The question is, is AI like this? Is AI a similar technology? And the answer might be, well, probably not. Maybe AI only has like a 20% chance of that, 10% chance of that. But even if it's 20 or 10% chance, what an EA would likely say is that, well, that is high enough that you probably want some people, maybe one to 2% of people spending their time on this, because one to 2% of people spending their time on something that might be 10% as bad as nukes. That's a pretty good trade. That's a pretty good investment if you're thinking about the overall portfolio of how people are spending their time and spending their resources. So if you think about EA as, here is what I decree as an EA, as this East Bay rationalist guy that everybody in the world should be doing, then I think you'll come away with the sense that EA or Eas are a bunch of selfish pricks and they think they should run the world. But if you come away with the sense that, okay, eas are advocating about, at the margin, how to alter the portfolio of all the different things that human beings are working on to ameliorate risk and increase the likelihood of human flourishing, then you might say, okay, that sounds like a reasonable investment strategy. Maybe I don't agree with it completely, but it's a lot more reasonable if you think about it that way.
Speaker B: So, Hasiba, I want to bring Eric into this conversation, just kind of reflect on what you said and add, but I have just one question that kind of plagues me. So we started this conversation talking about accelerationism versus accelerationism. The way we started this conversation was talking about EA and the rationale behind how EA came to support and fund a lot of the AI safety initiatives. And it's really, when I think of AI, it's really like utilitarianism or consequentialism as a philosophy kind of applied to giving. That part to me sort of makes sense, but it doesn't necessarily follow that AI must lead to investments in funding of AI safety. Because if you are in the EA community and you believe in this sort of giving, you could just as well make the case that in order to benefit the most amount of like the greatest number of future humans, say trillions of humans, we want to actually invest in AI technology because we believe AI technology will build a utopia and cure diseases, cure illnesses, solve the problem of death in like old age and all of these things. And so you could easily make the case in the EA community under that philosophy that, no, we shouldn't be funding necessarily anything that would slow down AI. We should actually be funding startups, we should be funding OpenAI, we should be funding all the tech that goes into AI. But it seems like the community just forked mainly in one direction, such that EA has become somewhat associated with AI safety. I think, by the way, recently maybe some of the founders of EA, Will MacAskill and others, have kind of walked back from that and be like, hey, that's nothing. But EA is not necessarily AI safety. They're not one of the same. There's kind of some differences. Anyway, I just throw that idea out there. And Eric, what are your reflections on this conversation so far?
Speaker D: Well, first, I just want to add that not all long termism is in the venture bet category. Right. It's also things like pandemic prevention or reducing nuclear proliferation. Things that already exist already happened. And they were talking about pandemic prevention before COVID Right. So they're really onto some things. The leap that long termism can be broadly explained by this idea that EA is about a kind of arbitrage. Right. David, you described how if $1 can make a better difference, bigger difference to people in Africa than it can to people in San Francisco, well, maybe we should put our dollar to people in Africa. And EA says, okay, because a person in Africa's life is worth as much as a person in San Francisco's life. What about a person that doesn't exist yet? What about a future person? That person's life is also worth as much as a current person's life. And guess what? There's going to be trillions of future people, way more than exist today. And thus we should really care about the future. And there's a lot of people who got on board with that. There's a lot of people who didn't get on board with that. And this was a schism in the EA community. Some people are like, hey, I'm all about effective charity, but this long termism stuff, that's a bit too out there for me. And there's some people who go all the way. And now even among the people who go all the way, there's another schism that Ryan just outlined. Some people say, hey, it really depends on what you think about the future of AI. Some people say that the idea that you just said, Ryan, which is if you're concerned about helping the most future people, you want to accelerate AI. And these are people who are effective accelerationists. This is why there is an additional schism. It's people who are not on board with the AI safety agenda. And one thing that's worth noting, EA and IAC are having the strongest conflict. But what did Henry Kissinger say? Something like the tyranny of small differences. AI safety people and IAC people agree on 99% of things. In many cases, they both believe that the people who are worried about misinformation or Dei Orlando AI saying racist things, they're not at the adult table of the important conversation. Those people are misguided. They're not disagreeing with that. In many ways, AI safety people and EA people are tech accelerationists. They agree that nuclear regulation has been really bad. They want to accelerate things like self driving cars or longevity or other things that I ac people get on board with. The one place in which they disagree is this field of AI. They say AI. This time it's different. We're not just introducing a new technology, we're almost introducing a new form of evolution or a new species that one day is going to be cognitively way smarter than us and thus is a threat not just to fake news, it's not like social media, not just to jobs, but to actual humanity, to actually the chance of us being extinct, and thus we should treat it differently. And that is the core of the argument between effective accelerationists and effective altruists as it relates to AI safety.
Speaker B: Okay. Okay. So we're dealing with, like, this is like an argument between, like, Catholics and Protestants, basically. So, like, we're all in the same religion, but, like, you guys are arguing over these specifics here.
Speaker A: I'm gonna dispute that. I'm gonna dispute that a little bit. I'd say a lot of Iacs. So, first of all, I think it's probably worth actually defining what IAC is what it means as a movement, because I think it's also been, in a sense, co opted by what people want it to mean, which is accelerationism. This idea that, oh, all technology is good and all technology should be faster, we should be super libertarian. We should let all the guardrails off and just let it vroom, vroom, vroom. That's not actually what Yak says. If you read the IAC manifesto from Beth Jasos, who's the guy who invented it, as this Google engineer, works in quantum physics, it's actually pretty fucking crazy. Crazy like what he actually literally says. I don't know, Eric. It's been a while since I've actually looked at that manifesto. But there's stuff in there about how, yes, if AI ends up totally conquering humanity, that's good. We actually want to feed entropy. We actually worship entropy. Entropy is the core value from which all good flows. It's actually very weirdly cult like and kind of anti humanist. I think most people haven't actually read it, or they just kind of take it as a vibe, and they're like, well, the vibe that I want it to be is like basically Mark Andreessen. I think that is what I imagine IAC is. But if you actually read the IAC manifesto, it's a bunch of other weird shit that's adjacent to the Marc Andreessen philosophy, but it's not quite the same thing. So I think it's sort of been typecast into this is like Silicon Valley, pro tech, libertarian energy. But that's not literally what ex says. I don't know, Eric, if you would agree or disagree with that.
Speaker D: I do agree with it, and I should say that that is my tribe in some sense, is people like Mike Solana, people like Balaji, people like Mark Andreessen. And so that is where I come from. And so I've been very sympathetic to this IAC movement, which to some degree says, hey, over the last decade, there have been a lot of anti tech movements and sentiments and people trying to use arguments to thus regulate tech in ways that have been very bad from whether it's energy or social media or a wide variety of things.
Speaker B: How about crypto? Let's add that to the list.
Speaker D: Yes, exactly. And we're not going to let that happen here, and we're finally going to stand up for ourselves because you saw Zuckerberg and others apologize, and that didn't really get them anywhere. It only conceded their sort of lack of moral sort of credibility and that they were wrong and blah, blah, blah. And that's all these bad things. And so this IAC is coinciding with the desire for tech to stand up for itself, to say, hey, we are actually good. We are actually going to push back and we are going to defend ourselves. Now, I think, as he brings up a really interesting point, because, yeah, Mike Solana Balaji and Mark Adresin are not saying that we're going to be pro human or, sorry, post human in some sense. They're saying that technology is going to make human life way better for decades to come. And I think what Beth was doing, in some ways, IAC is critiqued as being about vibes. And the benefit of being about vibes is you can't really be pinned down, you can't really be critiqued on your philosophy. And I think that's partially because of what Haseeb just just identified, which is if you take it to its logical conclusion, it's not something that a lot of people want to get on board with if you take sort of advancing technology to its logical conclusion. Like, what is the role for humans once we have super intelligence? What is like, when we're like cyborgs stuff and then different speciation as like. So it is very weird. But also, I think in some ways it's intellectually honest of like, that's where things are going. And I couldn't pin down Solana or Balaji or mark on, like, what do they think about that? But I think there is some truth to that. But I also think there's some, like, I do think it's hard to be a pro tech on a long enough time scale and not think that we're going to be think that humans are the last sort of form of intelligence that is going to rule the world. So I think it's intellectually honest. I also think it is very weird.
Speaker B: So, hazebuild, what's the problem with IAC plus some humanism? It's a more moderated version.
Speaker A: To be clear, there's nothing wrong with IAC plus some humanism. I think it's much preferable to raw IAC as it was actually articulated in its genesis.
Speaker D: I think it's evolved that way.
Speaker A: The thing that's weird. Yeah, well, I don't know that it's evolved. So much has been co opted. I think it's more like it's been co opted. Right? It's been co opted. Like, the reality is that any. Anytime you get a culture war, you basically get the superimposition of what was once like a pretty you know, esoteric argument about, like, will AI's take over the world? Is it risky? Is it not risky? Is it good to have more? Is it good to slow down? This is like a pretty conceptual theoretical argument that's been going on in very obscure circles for a very long time. And then basically in the span of about a year and a half, it has now become a culture war. And once it becomes a culture war, then all of a sudden people get typecast, right? So like, okay, if you're an EA, you're like the decelerationist. That means that you hate technology. And if you're an IAC, you love technology and you love Silicon Valley and you love Mark Andreessen and you love Peter Thiel. And it's like, well, you know, not necessarily like in reality, there's a panoply of views. There's people who arrive at different parts of the spectrum from many different places, and there's people who say, well, look, I think the probability of AI is going badly is 5%, but the likelihood that actually it's better if it's open source is like 25% or some people say like, well, look, I think it's like 10%. But I actually think that decelerationism is a bad idea and it ends up just politically backfiring. And so we kind of have to live with the acceleration. In reality, there are Eas and there are ex quote unquote who have views anywhere along the spectrum. But once you become a culture war, all that shit goes away and it's just like, oh, you're IAC, oh, you're decel or your EA or whatever the fuck. And people ultimately want a one dimensional or sort of one bit answer to how you feel about what is probably one of the most important questions right now facing technologists or technology regulation in America. So if you ask Eas, what do you think we should do regulatorily about AI's? You'll get a lot of different answers. You won't get, well, we should shut it all down and fucking control all the GPU's. Eleazer says that, but Eliezer is like all the way on the spectrum. He's like the platonic ideal of a decelerationist in that sense. But most people on EA are very, there's like a multi dimensional spectrum that people sit on different places in that spectrum. So for myself, I was donating to AI risk in 2014, 2015, way before large language model. I mean, not way before large language model, but before any of this stuff was in the popular imagination. And it was not very obvious at that time. What were the specific policy prescriptions that were going to be made, or how was this going to interact with Silicon Valley at that time? The people who were working on AI risk were also Silicon Valley nerds. It was the same team, it was the same people talking about this stuff. It's only very recently that now there's this sense that, well, build, baby, build, which is the Silicon Valley mantra, is now hitting this wall. And I think it's a little bit of a boogeyman as well, because the reality is that the EA types, they don't really have that much power. They're not actually stopping anyone from doing anything at the moment. The real people who are stopping AI's are the EU or China, who are not eas. That's not what's motivated. It's like data privacy laws or just, you know, deference to the CCP, right? Even the executive order that Biden passed said that, okay, if you're above a certain training run size, you have to report that to the government. That's it. It doesn't say you have to stop, doesn't say you can't do it, doesn't say you have to like, you know, follow these rules or whatever. It just says you have to let us know that it's happening, which to me like that, okay, that's a pretty far cry from, you know, oh, deceleration, blah, blah, blah. It's really just like an information gathering exercise, which, I don't know, probably they wouldn't have that much trouble gathering that information anyway given the amount of GPU's, would have to be cornered by anybody running a training run of that size. So I think there is a lot of reactivity right now, and that's what's causing people to believe, oh, you're way on this side and I'm way on this side, and you kind of can't take any nuance view in the middle. That's what happens in a culture war.
Speaker D: First, let me just say that Eliza has also tweeted things like abortion. We should be able to abort babies that are up to 18 months, or you should be able to leave your partner if you find your wife or husband if you find someone 25% better. You mentioned the platonic ideal. He takes utilitarianism to the extreme in the same way that best manifesto takes that idea to the extreme. There are people in EA, SBF, of course, one of them, who take some of the ideas within EA to the extremes so far that there are no longer sort of palatable. I would take the other side a little bit. I do think EA has a lot of power because I think that a lot of people who make up OpenAI, who make up anthropic, who make up some of these biggest labs were influenced by EA or themselves.
Speaker A: They were Eas.
Speaker D: Yes, exactly. The ideas they came up in the EA soup. Right now, there's not a brace of regulations, as you mentioned, people are still allowed, but we're at a time where these regulations are about to be defined. The regulatory era of AI over the next year, et cetera. And the people who have, they're inviting people into the conversation who have EA influence, EA ideas. And so I do think the AI safety community will have an impact into the regulatory.
Speaker A: But, okay, but you raised a good point, which is kind of ironic, right? Which is that if you look at OpenAI, if you look at anthropic, if you look at the companies that are actually accelerating, they're mostly eas, which kind of fucks up the narrative, which is like, oh, the eas are ruining everything. They're slowing everyone down. And it's the accelerationists who want to go fast. The accelerationists are mostly VC's. They're mostly not building anything. They're just like on the sidelines, cheerleading for this imaginary team that they think is in the driver's seat when it's reality. It's fucking EA people who are even thinking that this was possible in the first place. Right? It was all the VC's who were not investing in any of this stuff because they thought it was so impractical and it was worth is the eas who believed that AI risk was a real thing because AI was going to become really powerful really quickly. Who created all this shit? So that's why I think, again, this culture war typecasting that has happened is just ahistorical. It just doesn't make sense, given who's actually the players involved, most of the people who are regulating AI. Like, yes, we just talked about Paul Cristiano, who is now somewhat influential, presumably at NIST, which is again, a relatively minor kind of scientific organization. He's not a congressperson. Right? That's who actually writes the laws, is congresspeople. If you listen to congresspeople, what do they care about? They care about jobs. They care about bias. They care about, oh, you're squelching conservative voices. That's what is very likely to end up on the regulatory regime or the legislative regime. I'd love if Paul Cristiano got something in edgewise into that conversation, and maybe he will. But so far, it doesn't seem like that's predominantly what we're hearing from the people who actually make the laws.
Speaker C: There's one thing that we're familiar with in the crypto space, it's tribes and certain cultural leaders in respective tribes becoming the hardliners for that tribe. They teach the tribe who vibe associate with the cultural leader how to be a hardliner. And then, like, the people that listen to, like, the high priests of this particular alignment camp absorb some, not all of the message, right? Like, oh, yeah, most of this stuff feels good to me, right? We see this in literally every single crypto tribe that has ever been birthed, and we all have, like, one, at least one respective hardliner. And that's what, like, kind of more or less creates the tribe. And then also you smash the particle of social media into that and, like, everything gets. Just gets juiced. And especially this last cycle as, like, bitcoin ETF's have gotten approved and, like, blackrock is starting to tokenize securities on Ethereum. I've very quickly realized that, like, the realm of crypto Twitter is actually a significant minority of, like, what actually matters in the space. And, like, if you're in crypto Twitter, you think crypto Twitter is massive. You think it's a really big deal because you are in it, and maybe it actually is. But, like, I remember hearing this, like, line from Sam Harris when he was critiquing Elon Musk. Takeover of Twitter is like Sam Harris was saying, like, Elon Musk is disconnected from reality because he's on Twitter all of the time. He thinks the universe is Twitter. And this is where these tribes fight, right? This is where the hardliners chant their chant and post out their tweets, get their tribe on board, and then everyone else thinks that this is the universe. And so I guess zooming out. Eric, you talk to all the technologists of Silicon Valley. Can you help us understand the scope of this? Is this a broad, encompassing blanket that is over all of Silicon Valley and government, and everyone is focused on this conversation, or is this kind of a side quest from the VC's and the builders? It's actually not really defining the game, the landscape that much. How far are we on these continuums?
Speaker D: I think the answer is both. Similarly, people say Twitter is not the real world. Most people in the world are not on Twitter. That said, the people who are influential in the world, who influence other people's views, whether it's journalists or politicians or people on tv or anyone who's got a mouthpiece to the rest of the world are on Twitter. Not only are they on Twitter, they get their ideas from Twitter. They battle it out in the marketplace of ideas or the culture war. And so I disagree with Sam Harris. I do think Twitter is the source where a lot of ideas come from, and a lot of the sort of ideas get litigated on Twitter. And similarly, most of Silicon Valley is just doing their jobs, not on Twitter, not focused on this. But where do things get litigated often on Twitter? Aziz was just saying how EA has not had a big impact yet. We alluded to this off camera, but there's a world where Sam Altman did get fired and there was an EA coup, and maybe there's overblown, but there seems to be some EA concerns from the board, and maybe OpenAI would have taken a much more sort of decelerationist stance. There was this movement to pause AI about a year ago that it didn't seem like people followed, but maybe they would have go for it.
Speaker A: Yeah, I want to push back against this because this, again, was the framing of the OpenAI coup. Basically, in the couple days when it was all fog of war and nobody knew what was actually going on, there's now been a ton of reporting on what actually happened in the OpenAI coup. And it looks like just a regular old Sam Altman was kind of telling people different things, and he was raising money for stuff on the side and not telling the board and blah, blah, blah. There's just actually, there was not. Oh, there's Q star, and it's a secret algorithm that's going to AI risk. And people saw that, like, oh, my God, Sam Alvin wants to unleash it on the world, and he's not being safe enough. That was not what happened. We know now that's not what happened. What happened was just ordinary board loses trust with CEO, CEO and board fight. Board is very amateurish. Oust the CEO, hire somebody. It was all just terrible board management.
Speaker C: But really, because to this day, I have always thought that it was an AI safety.
Speaker A: No, there was no. There's been so much reporting on me right now. No, no. Go read. The New York Times has done a ton of exposes about this now. Like, there was almost nothing to do with AI safety in the reason why OpenAI. Because at the end of the day, the whole nonprofit structure of OpenAI was not the reason why they fired Sam. Right. The reason why they fired Sam was that the board members lost trust with Sam. Sam reportedly was telling different people on the board different things. The board was getting back together and they were very uncomfortable with the way that Sam was managing things. And he was kind of controlling people. And he had, people talked a lot about the fact that Sam is a very manipulative guy and he's very, very effective at what he does. Part of what makes him a very good leader is that he has this reality distortion field and he makes people feel how he wants them to feel when he's around them. That's the real story. If you try to tell the story about, well, OpenAI was an EA coup, then you have a lot more explaining to do about, okay, what was the EA thing that happened that precipitated all this?
Speaker D: Perception is reality. Right. And it was framed perception.
Speaker A: Yeah, exactly. No, it's true. It's true. It was framed in that way by like the chorus, you know, just the people on Twitter, the talking is who wanted this to be the story. It's the same thing with like, Ea versus Iac. Right? People want this to be the story, but the story is almost always more complicated. And the reality of OpenAI was that it was more complicated than that.
Speaker D: Yes, but if it had been completed, this coup that never was, it would be framed as an EA coup, right?
Speaker A: Yes, yes.
Speaker D: So let me refer it a different way. You're saying, hey, the people who actually write the laws, they don't care about AI safety. They're not AI safety people. They care about jobs and they care about misinformation. And I agree with that. And this gets to the sort of baptist bootleggers sort of dichotomy that Mark Andreessen often brings up, which is this idea that there are people who are purists, they actually believe in things like AI safety, and they really care about the issues. And there are bootleggers who are sort of either grifters or opportunists who notice something that is in their self interest to take the same position as the Baptists. And they use the language of the Baptists to justify their own advancement of their own self interest. And similarly, in tech right now, there's a schism within tech. Like there was a survey that went out to 10,000 researchers or thousands of researchers, and I think 10,000 MLIR researchers who work at these big labs expressed some concern over or sympathy with AI safety. And so if we as an industry don't present a united front in our belief of these technologies, it gives ammo to people who want to regulate them. Right? And in the same way that within crypto, if you have half the community talking about why crypto is bad and regulators see that, they might say, oh, even crypto people think it's bad, or web three people think it's bad. Thus we need to regulate it. There's a concern, in the same way that with social media. Social media people apologize all the time and express their concerns, and it gave more ammo for regulation. They have the concern that the same thing will happen here. The counter to that, which I think is a good counter, is, no, it's not about giving them ammo. It's about self regulating. If we as an industry can self regulate ourselves, maybe that will prevent other people from coming in because we can prevent some of the damages. And crypto talked about this a lot, too, of getting rid of scams early. So that doesn't give the whole industry a bad name.
Speaker A: Well, in reality, that is what's happening. Who is doing the AI safety research? Who's actually trying to be responsible with this stuff? The answer is open a and anthropic. And the guys who are actually leading the front are self regulating because the regulators have no clue what the fuck is going on. They have no idea how to even be productively, how to productively give input into how to guide this technology forward. So the reality is that the concern about regulation, I mean, it's one that I don't know anybody who doesn't share the concern over bad regulation. Bad regulation is bad, right? Clearly. But it's also obviously true. Even among the effective acceleration is that AI is really powerful, and AI is going to massively change the playing field around who controls what. And right now, they're kind of toys like, okay, yeah, you can spam people, or you can spearfish much more efficiently using these tools, or they can tell you how to build a bomb, which, like, okay, if you can't figure out how to google how to build a bomb, good luck using a large language model to get there. But there's a lot more that's coming, and it's pretty obvious to everybody there's a lot more that's coming. You know, whether you agree about whether you think the right way to do this is self regulation or government regulation depends on your confidence in the right laws or right frameworks to arise. And Silicon Valley obviously has a lot of scar tissue in believing that governments fuck it up. Governments are terrible. Everybody hates us. And so they're going to create these draconian laws that are going to be about this power struggle with government and with special interests or whatever. And that's going to be the regulatory regime that we enter into. But nobody goes around saying, like, yeah, all the stuff that opening eye is doing around safety, this research that anthropic is doing, it's stupid, it's worthless. It's slowing us down. Let's throw it away. If effective acceleration has said that, then I would say, okay, it sounds like you guys are very consistent in believing that AI is an unalloyed good. But they don't say that. They don't say, like, well, why are these people slowing themselves down? Instead they say, well, government bad, these people good, which, like, okay, I understand where that's coming from, and I don't think eas are pro government. Right. Which I think is, again, another way that this dichotomy kind of breaks down. Eas in general think that governments are terrible allocators of goods. Right? I mean, you can just see it historically, governments are incredibly inefficient. And so I don't think eas look at that and say, well, great, government is the natural person to provide this regulatory regime because they're all brilliant. And we love what everything that governments do. Most eas are pretty libertarian on almost every single front. They generally believe that governments are incredibly wasteful. But when it comes to AI, the concern is that, well, is self regulation enough when you have a tool that is incredibly powerful, right? So if you have self regulation around nuclear weapons, how well is that going to do in an environment where nuclear weapons are extremely powerful and extremely damaging? Well, in practice, we've kind of learned through experience the only way to really control nuclear weapons is with the monopoly on violence. It was basically saying, yo, Iran, if you develop nuclear weapons, we're going to come at you. North Korea, if you develop nuclear weapons, we're going to come at you because we don't want these plethora. We don't want this to just be a tool in everybody's hands. We actually think that's a worse equality, to have everybody be really powerful. But this is kind of what IAC implies, is that the best equilibrium for AI is that everyone has one and AI's keep other AI's in check. And it's sort of like we all have nukes. Maybe that's true, but there's at least a real possibility that that's not true, that it's more like nukes. And it's not like, you know, I don't know, you know, books, right. Which is a thing, okay. More education is good for everyone. So anyway, sorry, Eric, I know you want to jump in.
Speaker D: You are correct in saying that we are accelerating and EAC people are happy with the status quo. They're just worried that things will change because as you mentioned, the irony, people who are sympathetic with AI safety are the ones driving the acceleration. People work at these labs, but there are people who are even further on the side of AI safety who are noticing that contradiction saying, hey, you claim to be about AI safety, but you're actually accelerating. People like Eliza, people like Laurent, very smart, brilliant people who are saying, hey, we actually need to slow down, we need to pause AI, we need to put an off switch, we need to have some sort of more extreme intervention. And I think the IAC people are responding to that concern. Not that the status quo is a problem, but that a very significant change could be a problem. So that's what they're worried about.
Speaker B: Yeah. Yeah. I think that draws something that we see on both the extremes and something that Eliezerkowski and like Beth Jasos might also agree. It's like there's another more cynical interpretation of the stuff that the EA people, like the Sam Altman's are doing and the reason they're spending time in DC. And that interpretation is regulatory capture. So they actually want to hit the accelerator on AI in general, but they're also reasonable people and they're happy to work with the government in order to come up with sensible regulation and partner with them because we're the educated ones, we're building the models. And so, yeah, let's team up with the regulators and the governments around the world in order to kind of create the right rules for this new and emerging industry. And, oh, by the way, this would be, I think, the Marc Andreessen critique, by the way, we are the recipients of value at the end of, we have cemented ourselves, we've created a moat for ourselves and our own businesses. And so there's a criticism that comes from Elie Yser in doing this. He's saying, well, this is hypocritical. If you're really worried about AI safety, you would, like, cease immediately. You would stop doing this until you do a Manhattan project and go figure it out. And then there's a criticism from the Beth Jasos side of things with Sam Altman. Beth would probably be someone who says, like, this is clearly regulatory capture. They're doing it in broad daylight. Right. The only solution for this is decentralized AI. And everyone needs like a model and an AI genie in their own home, running on their own hardware. Right. So this is a critique that's coming from both sides. And I'm wondering, Haseeb, what about that more cynical interpretation of what's going on? In fact, wouldn't that be, like, actually the game theory of what companies want to do? They want to go build monopolies. And so, I mean, that's a sensible business strategy. Sam Altman's a smart individual. He's great at this. Right?
Speaker A: Yeah. Okay, so first of all, I'd say I find this a little ironic because, again, during the whole AI ousting or OpenAI ousting drama, Sam Ullman was typecast as, like, the promethean, you know, like the EAC guy who was trying to make AI go faster. And now all of a sudden, oh, no, no, actually, nevermind. He's not the savior. He's actually the, he's the boogeyman. He's actually going out there and trying to, you know, he's the JP Morgan trying to corner the steel trade or whatever. Or. I don't know. To me, this is not a critique. This is an innuendo. Right. What is Sam Altman? Okay, let's step away from AI. Let's talk about crypto, because I think people on this podcast understand crypto easier. Let's pretend that instead of Sam Albin, it's Brian Armstrong. He goes to DC and he says, hey, I'm going to engage with DC and try to come up with sensible legislation. Would you be like, oh, my God, he's trying to cap. He's regulatory capture. What is he doing? Why is he consorting?
Speaker B: It depends. So not, not Brian Armstrong, but Spf. That's exactly what he was doing.
Speaker A: Yes. Yes. Okay, so put SPF aside. Let's say Brian Armstrong. Brian Armstrong does go to DC. He does do all those things. Do you say the same thing is happening now it's just Brian Armstrong instead of SPF?
Speaker B: No, but I would be cautious generally.
Speaker C: I'm sorry.
Speaker D: Why not?
Speaker A: Why not?
Speaker B: Just because, I mean, for me, is.
Speaker A: This a be cautious? What do you mean by be cautious? What do you mean be cautious?
Speaker B: I think.
Speaker C: I think it's both. Both. It's. He's both fighting for regulatory arbitrage in the way, maybe implicitly, but he's also fighting for the immigration. Yes. Regulatory capture. Yes. But I think he's motivated by all the right reasons. And then that effect of that is regulatory capture.
Speaker A: Sure. Okay. So it sounds like what you're saying is, hold up a sec. Let's see what he actually does and then judge him based on the consequences of the legislation that's actually proposed. Yes.
Speaker B: Yes.
Speaker A: Okay. That sounds very reasonable, right? That sounds very reasonable. Okay. What is the legislation that Sam Altman has proposed? Speaker one?
Speaker B: I actually have.
Speaker A: Nobody fucking knows. We don't know. The answer is we don't know. So it's all innuendo, right? This is people just making up a fucking story because it sounds good. The reality is, if you're Sam Almond and you're not going to DC and you're ignoring people in the White House, you're a fucking idiot. You were an idiot. Whether you're fighting for the industry or whether you're fighting for regulatory capture, like, either way, if you just ignore them, you're a fucking moron. So until we see what is the actual legislation that he's advocating, we don't know. No. And if you were Sam Altman and you'd literally just ignore DC, you would be definitely doing the worst thing possible. Right? I would much rather say mom and be there than whoever else is going to be there instead, you know, some guy from, I don't know, fucking Google or something. I would much rather Sam Alvin be there.
Speaker B: I think this is a fair point. I want to hear what Eric says on this, but it is a fair point, actually. It was kind of like a critique that I had of, like, Brian Armstrong and Coinbase, of, like, not engaging in DC earlier and sort of letting a vacuum be present so that people like SBF could actually engage the moment. I actually like bankless in general. So David and I kind of, like, started. Our spidey senses started tingling with SBF was when the draft of some proposed legislation that he was responsible for actually came out. And it felt very much like a rug pull ladder pull up on all defi front end interfaces, and, like, brought him to task and said, hey, SBF, this is not like crypto values. Like, what are you doing to. You're essentially forcing it or creating some legislation that would force kind of compliance on this space and promote your centralized exchange. And so a critique of Brian during that time was like, I wish he had been there. Because if you're saying Sam Altman could be good, we don't know. It's kind of like Schrodinger's Sam Altman. We'll see what the legislation that pops out on the other side of. I guess the case for that would be if he doesn't do it, then some more nefarious version of a Sam Altman would be in DC engaging those regulators, engaging regulatory capture. So I suppose we don't know until the legislation is out. What do you think about this, Eric?
Speaker D: I would agree with Haseeb's framing that Sam has to get involved in. Yeah, let's wait and see what the regulation looks like. I would like to see AI safety people more strongly disown the AI ethics people because they often get conflated because they have a sort of a coincidence of wants motivated by disparate desires.
Speaker B: We're the AI ethics people. I don't think we've defined that well enough, Eric.
Speaker D: AI ethics people are people who are worried about misinformation, who are worried about sort of inequality, about certain people or certain voices being represented by the AI.
Speaker C: Is this like AI woke people?
Speaker D: Yes, that's exactly it. And these are the concerns that, along with jobs and other things that congresspeople or regulators are more sympathetic to because they don't quite understand the AI safety arguments just yet, or it's a bit too abstract. And by not disowning those sort of reasons, they sort of implicitly accept them or give them more power. And I would like to see that. And this is why we see things like Google Gemini being super woke, or even chat GPT earlier. And these things matter. And so that's something that I would like to see. One thing I want to return to a little bit earlier is Azeeb was framing the culture war. And when we have a culture war, things get kind of dumbed down. And people typically associate culture war with Badlandhouse. And when they think of a lack of culture war, they think of, oh, a great marketplace of ideas where people are being very sophisticated and presenting good arguments. And this, to ease credit, is often what EA is. In many ways, it's very intelligent conversation, trying to get the truth in some ways. Now, I would also posit that the lack of a culture war is not just this great marketplace of ideas. It's also something like North Korea. It's something where there's a lack of. Of moral or intellectual diversity. It's just like, you just have to have, this is defense that some people give a poll.
Speaker A: Is this a pro culture war argument?
Speaker D: Yes.
Speaker A: Yes, it's a we should have more culture wars.
Speaker D: It's a pro polarization argument. I think in, and even in EA, I saw this blog post, we can link to it where someone was saying that EA needs more polarization and it actually affects their giving. Like EA gave, or, you know, Gible gave like $200 million to criminal reform or something, and this guy was arguing that actually made the problem worse. And why didn't people come in and measure this? Because they felt it was too controversial to go against it. And this is a critique that some people give a VA, which is as much as they want to claim to be rational first principles, you would think that that would lead to a diversity of perspectives, and in some cases it does on issues like long termism and specific implementation details. But it's worth noting that EA is democrat, that there are very few Trump supporting eas. Now, why is that? Is it possible that everyone in EA from their first principles is just like Trump, bad Biden, better? That seems very unlikely. It seems that EA is also prone to tribal thinking in ways that other communities are. And that's not a knock on EA, but the reason why culture war in this case might be good. I'll also posit that IAC is a way for Silicon Valley to implicitly be right wing, implicitly be conservative. And you notice that IAC is not only talking about issues like advancing AI or technology, but also wading into other cultural it's much more masculine, it's much more alpha, it's concerned with birth rates. Right. And so I think it's good for there to be, in Silicon Valley more polarization and more diversity as it relates to. As it relates to intellectual issues. It relates to politics, because for so long we've had a lack of that. And that's prevented really important conversations on things like, you know, criminal justice, for example, or things where there are different, there are taboos. And EA rationale is supposed to get rid of taboos. And in some ways it hasn't done that. So I'm curious how you would react to the characterization of EA as Democrat affiliated or left wing and not having enough sort of intellectual or moral diversity.
Speaker A: I think you raised good points, and I should probably reiterate that I've been affiliated with the EA philosophy for almost ten years now. I wrote a post fairly recently critiquing eas, I think I titled the post the unreasonable ineffectiveness of effective altruists. And I think what I've found over time, coming into the crypto space, I kind of thought that there would be. When I first got into EA, EA was all these kids who were really, really smart and made a lot of money, and they seemed to be extremely smart and fastidious and hardworking. And I thought, wow, these people are going to be super successful. I'm a nobody. I hope I can be successful too, someday. And then I found my way to crypto and started building stuff. And I knew some eas who'd gotten into crypto. One of those eas was this kid named Sam, who I first met at EA Global, which is this big EA conference in San Francisco in 2015. He was still working at Jane street at that time, and I met him in a hallway, and we chatted for, like, five minutes. Just about blah, blah, blah, ea whatever and whatever. I didn't think much of it. He was just pasty guy shorts. Little did I know. Little did I know. I'd come across him again much later when we were both doing very different things in the crypto industry. Neither of us were in crypto at that time. I kind of assumed that there would be a lot of people who were really successful in Crypto who were eas. And what I would later discover many years after being at VC was that that was super not true. There was basically only one other person who was a real EA or like, a central EA, who was very successful in crypto, and that was Sam. And I didn't know Sam super well. We chatted maybe three or four times over the years that we'd interacted with each other, but obviously, we all know the story of what happened with Sam, and he single handedly kind of redefined EA for most people as being this kind of really corrupted, kind of corrosive group thinky, kind of go along with the flow. Very left leaning tribe, essentially. That's what most people think of ye as well.
Speaker C: Now, just like to add a little bit more onto that. Just like sociopathic comes to mind because it was like utilitarianism to its nth degree, not considering about any local humans, emotions or anything like that.
Speaker A: Right, right. And so these famous stories about would you bet the entire universe on a 51 49 coin flip? And he's like, yes, I do it infinitely many times. And so this almost, like autistic chasing of. Go ahead, Eric.
Speaker D: It's worth noting he said that in a podcast, and it wasn't like there was this, you know, everyone coming down on him, criticizing him. He was still a guy. And so it's crazy, the difference between how SPF was treated before we found out he was doing the fraud, even though he was saying some of these things. He wasn't saying fraud, but he was saying extreme utilitarian ideas in broad daylight. And so it's just worth noting.
Speaker A: Yeah, exactly. And so it's something that we actually talked about. We alluded to briefly earlier about extinction risk. Right. And like, you know, what is the importance of avoiding extinction risk and the reason why SBF's example is wrong. So a classic thing in EA is this idea of risk neutrality, which is this idea that, okay, most people are risk averse. They don't want to take risks. And so it's like, okay, if I can definitely save a life by going to, I don't know, a food drive and donating some food, and I will definitely save a life by doing that, but it's very, very expensive. Or I can maybe probabilistically save a life by investing into water filtration systems, and maybe somewhere down the road that's going to result in saving a life. Most people are like, well, I want to know for sure that I save to life. And EA kind of advocates this risk neutrality, which is the idea that look in expectation because there's millions of people doing millions and millions of things trying to save lives. All you really care about is, on average, how many lives do you save? And in the portfolio of all the things people are doing to save lives, you should just increase the expected value and not worry so much about whether your coin flip turns out heads or tails. SPF kind of took this idea to an extreme and said, okay, we should gamble the whole fucking world. World on plus ev, one life. Great. Okay, flip the coin as many times as you want. It's free money now. This is wrong. This is actually just wrong. This is wrong. According to this mathematical rule called the Kelly criterion, which basically defines the optimal level of risk taking you should make in order to maximize the growth rate of your bankroll. I used to be a professional poker player before I ever got into tech or crypto or anything. And Kelly Criterion is a concept that everybody in poker knows, is that if you want to make the most money, you follow the credit Kelly criteria. You don't bet your entire bankroll on. Oh, okay. I've got, like, you know, 60% to win this hand if we go all in. So great, I'm gonna bet my entire bankroll on it because that's the maximal expected value. That's wrong. That is wrong to do that. It is wrong to do that. Because when you go to zero, right, if there's a real existential risk, even if the risk is low, like, you can never come back from zero. As long as you still have some bankroll, you can make it all bad back. But if you go to zero, you cannot make it all back anymore. Your growth rate is now zero forever. And so Kelly criterion tells you you should maximize growth rate as opposed to maximize expected value, and you maximize growth rate by following the Kelly criterion. Right? So Sam was wrong in his elocution of that, but people kind of get stuck on this idea that, oh, well, that's what eas believe. Eas believe that you should bet the universe on every tiny little edge and every small piece of expected value. So again, it's like this. It seems like the EA worldview just completely diverges from what normal people think of as common sense morality. And I think that SPF's worldview did. But most eas worldviews, if you explain them clearly enough, it's like, actually, no, that actually does sound pretty reasonable. Yeah. Okay. If you're playing with a technology that might make you go to zero, okay, the one rule of gambling is that you never fuck with going to zero. You never fuck with going brokers broke, you always leave some money on the side that if you end up losing the hand, even if you're all in aces versus kings, you still, there's a chance you lose when you go all in aces against kings. Right. So, okay, that 10% of the time that you wipe out, you want to make sure you got some money to start grinding it back. In some sense, that is the core idea behind, hey, if you're p doom, that's the probability that you think that AI is in some way are an existential risk to humanity. If your p doom is like, like 5%, that means to most people. Okay, well then why worry? Obviously this won't happen and so what's the chance that a one in 20 thing is going to happen? Why are you guys freaking out about this? But if somebody says, I don't know what my p doom is, but it's probably something on the order of 510 percent, which means the modal outcome, nobody ever has to worry about any of this. None of this is going to matter. Most of the time you're going to walk through the world and AI's are just going to be sex robots and it's going to help you with your taxes and whatever, but none of them is going to, none of them are really going to try to kill humanity. Most people who are talking about Ea believe that. They believe that the modal outcome is that nothing bad will happen or nothing bad on a catastrophic scale will happen. I should clarify, but they say even if that's true, that 5% of the time you don't risk that. You don't risk that. So even if you think, yes, there are going to be trillions of humans in the future, and the IAC worldview is that, well, all those trillions of humans, I agree. There will be trillions of humans in the future, and we should care about their wellbeing just as much as we care about present humans. And so therefore, if we wait even a moment longer to let AI's arrive into the world, we are doing a massive amount of harm to all those humans. The shape of the argument is kind of similar, but the Kelly criterion, if you take it seriously, what it implies, is that, hey, waiting another couple of years to get to the same place we're eventually going to get to, but with basically minimizing the chance that your bankroll goes to zero is worth it. If you're just thinking about Kelly Criterion.
Speaker C: And also with the role of human incentives, I think the gap between the Kelly criterion and what value, like maximizing EV ended up being SPF's, like Bahaman Penthouse and all of his political donations and everything else.
Speaker D: One thing I want to speculate with you guys is why is EA this convenient punching bag? And it's punching back on the on contradictory statements. The AI ethics people for years have been calling EA transhumanists or saying, hey, you're way too tech oriented. And of course the EI people called them doomers. EA has a branding problem. It's been having a banning problem, and it was a punching bag after SBF. It was a punching bag for OpenAI. It's basically something that everybody else can rally around of why they dislike it. And it continues to have this trouble. And I would pause, it's because EA has beta energy. EA is nerdy beta energy, and it doesn't stand up for itself. It doesn't say, you know, like after the SPF thing, it goes, it apologizes, it goes into hiding after opening eye thing. Also, like, why didn't they say, no, this was totally fake. You're making this up. Actually. Like, EA has done some amazing things, right? EA has had incredible accomplishments, and yet they're not shouting them off of the rooftops in the way that Mike Solana is shouting off alpha, aggressively saying why tech is good. And this is where I pause it when I was somewhat joking about culture. We're good. But I think ea and Iac need each other. I want an ecosystem where IAC and EA are duking it out because I think we need both. I think we need a movement that is explicitly unadulterated pro tech alpha energy willing to fight back against people who are trying to unfairly critique it or get our CEO's fired or put in dumb regulations. But I also want another movement that keeps that movement in check and says, hey, AI safety is actually this really important thing. We should really look into it. We should self regulate and have that fight internally as opposed to externally. When EA gets too powerful, it becomes this massive punching bag. At the same time, IAC, when it gets too powerful, it becomes really cringe. Right? Like, IC is at its best when it's duking it out with EA, when it's trying to be this overarching thing, it can't really. EaCT is like an underdog movement. They're both like underdog movements that should be in productive tension with each other for us to figure out. And maybe the status quo is actually pretty good. We have labs who are accelerating, and yet they're also working on alignment research. They're also trying to keep each other in check. And if we could just keep this productive tension, or what I'm calling productive attention, maybe we can get to a better solution here as opposed to the worst of these extremes. You talked about the manifesto and how extreme that is, and we talked about some of the more extreme players in AI safety and how bad that is. So that's something I would posit. Why EA gets beaten up so much and why it needs ex support to help defend the parts of it that they agree with, which is 98%, in my belief. And then also why the tension is helpful.
Speaker A: I agree with you that EA is a punching bag. It's weird to me, in a way. So I was in EA before EA got really on the global stage. Most people didn't know what an EA was when I was first in EA and then really kind of in large part due to SBI EA suddenly became very high status during SBF's reign. EA, it was almost crazy the amount of positive coverage that EA was getting. Despite the fact that EA is objectively really weird and very unapproachable for most people. It kind of got lumped into this general asceticism and sagely, saintly energy that SBF was trying to portray himself as being like. In a way, two things I'd say. One is that that made me very uncomfortable. This idea that, like, wow, EA is, like, so good, and everything about you is good, and everything that is EA is good. And now EA is actually really cringe. Right? It's like, not cool. You go to Silicon Valley party and you tell someone you're in EA and they're like, oh, okay, well, interesting. And it's definitely not something that people continue to self affiliate with, which is part of the reason why I also like it more, because I feel like okay, if you're EA now, then you actually, it requires some balls, right. You're basically taking a hit publicly EA to say that you're an EA. Right, exactly. It's an EA bear market. And I think that is kind of the valley of darkness that EA needs in order to really define its identity and not just be like this kind of milquetoast, like, oh, I'm smart and cool and good and isn't that wonderful that I'm all those things at the same time? And so that's one thing I'll say. I think that it has been good for EA in a certain sense to kind of get stress tested by becoming very politically unpopular, popular. The second thing I'll say is, okay, why is EA such a punching bag in a way that, for example, tech was not post theranos or post or post, I don't know, crypto. Crypto obviously was a punching bag, post SPF blowing up, but EA as well. And I think a big part of the reason for that is that EA is just not designed to be politically capable. It's almost premise on this idea that politics is bad and politics is the mind killer. And if you believe this and everything that you preach is about nuance and careful thinking and avoiding sloganism and not being lazy and being suspicious of power structures, and then you're like, great, now let's go fight in the biggest power game of all, which is politics. You're just going to lose because you're going to suck at it. So I think that's largely the answer is that the one person who was good at politics in EA was SPF. And everybody else in EA just fucking sucks at politics. And that's why they were the saintly, quiet, nerdy people who are just in the background doing stuff and donating to bed nets and all this doing the AI safety research at a time when nobody else cared. And now it's like, oh my God, you guys are actually in the way of progress. And you guys are all terrible people. And they're like, no, no, we're really not. Let's do a two hour debate to discuss why. And they just don't have the ability to get out there and fight with slogans, which is the EC people, very politically capable. Mark Andreessen and Mike Solana and all those guys, they live and breathe politics, whereas the EA people, they're a bunch of philosophy professors that was the leader of their movement. These are verbose, extremely particular people and they suck at politics.
Speaker D: They might not be interested in politics, but politics is a.
Speaker B: That's just the point. Maybe we could bring into that part of the conversation because these nerd battles that we thought we were having insular to kind of nerd culture in Silicon Valley are now like political battles. And there seem to be some party lines kind of drawing and using these platforms for maybe a different use case than when they were originally created. And there seems to be this dichotomy forming behind kind of a political party for decelerationism, like check on Silicon Valley power and technological progress without constraints. Maybe there's some anti anti capitalism or anti billionaire type stuff tied up in that. And then there's this pro tech accelerationism. Remove the constraints. Let's let America be kind of the grandmaster of AI. Let's win this battle at a very curious time with respect to at least american politics, because we're sort of in between order. We had the new Deal order for a very long time, FDR until the 1970s. And now we're in this neoliberal order and have been for the last 40 years. And I feel like we very much are on the exit ramp out of that neoliberal liberal order. And the US is trying to find out what's the next political order that we're going to establish ourselves with. And I think the neoliberal order that kind of rubber stamped the Internet in the 1990s and helped Silicon Valley prosper, they were very tech forward. They love technology, right? It's going to bring jobs like the economy. It's not a certain thing that the next order political order that dominates America will be as tech forward as the order has been. At this point in time. You could start to see some battle lines being drawn. I think people like Balaji and Marc Andreessen might say there's like the blue trial that is tending towards tech decelerationism. Like lots of restraint, lots of regulatory apparatus slow down on this whole tech thing in general. And then there's maybe team red. They haven't quite chosen an alliance yet, but maybe they are going on the tech forward side of things and tech accelerationism side of things. And it seems like, I don't know, you have to pick a tribe at this point in time. From our conversation up to this point, Haseeb and Eric, I feel like both of you guys believe in a p doom for AI greater than zero, right? So you're not maximalists on either side, the ex side, you know, or the. Or the EA side of this debate. You're probably somewhere in the middle. But it's hard to really pick a side politically, right? When it seems like if you choose the decelerationism side, then you're consigned to, like, shitty regulation and they're going to shut things down and cement and some of the leaders, or if you go on kind of the other side, well, then maybe you don't have the constraints that you actually need for the space and you're not properly regulating. Kind of like the danger of this. Anyway. How do you think about the politics of this whole conversation that has exploded beyond nerd culture and is now, like, Biden has executive orders about this stuff, all right? And it's just starting. Like, AI is the thing that the nation that the world is talking about at this point. So, Haseeb, what's your take on this?
Speaker A: So what I resist the most strongly is the politicization of this question, right. Because this is one of the most important questions of our time, which also means that it's really fucking important to get it right. And the one way you will guarantee that you don't get it right is by making it into politics. Right? Like, if you think about masks, like, you know, how we dealt with the pandemic was probably one of the most important questions of this decade, and we fucking failed because we turned it into politics instead of turning it into a just raw scientific question of what is the answer of how this fucking disease is spreading and disrupting society and, like, you know, killing millions of people around the world. And so if you let that happen to AI, I think you've already lost the game, right? If you. If you concede this idea that, well, there's this tribe over here and this tribe over there, and you pick one side and then, you know, end, like, no, that's fucking retarded. Okay? That's just absolutely wrong. That's not the way you should be thinking about this. The way you should be thinking about this is that these are really hard questions. And the answers require nuance. They require careful study, they require lots of time and energy. And the only way you will guarantee that you get it wrong is by saying that there are only two sides. Right? In reality, it's a multidimensional question. And the idea that, okay, well, there's the d cells and there's the xls and the blue tribe likes this and red tribe likes that, and blah, blah, blah, blah, it's like, no, no, no, that's not true. Okay? One thing that we know about politics, when people actually go and study this stuff, as opposed to reading media, which will give you a very distorted view of what people actually think is that most people are not die hard leftists or die hard rightists. Most people are somewhere in between. They have a mishmash of ideas. They don't fit neatly into either of these categories and they don't like either of the leading candidates right now. Right. That is what we know. We know that by actually doing polling. If you go look at people, they don't fit neatly into these two categories and they don't on AI risk either. Right? Some people say, well, I think Peter is this, but I actually think we should do that. Or I think, hey, gpu constraints are not smart, but I think we should be putting more effort into using the government to fund safety research because maybe it's undersupplied by the private market. There are a lot of median positions you can take that are not shut it all down or build baby build. There's tons of room in the middle. And if you lose that room, then you're definitely going to fuck up.
Speaker D: I think you'll fuck up to some degree. But I would argue that it could actually be worse. And so I think it would be not the worst situation if one side was build baby boom and the other side was shut it all down. Because my worry is that both sides are going to say shut it all down like we have Tucker Carlson going on Joe Rogan literally saying shut it all down. He sounds like a dumber version of Eliza not to put down Tucker deserve. And Tucker is one of the most popular people on the right. If he ran for president, there wasn't Trump, maybe he'd be a candidate. So I could see shut it all down being a bipartisan thing. Like, you know, we're at a time of extreme populists, as seed mentioned. You know, people don't like biden, people don't like Trump. And we're also at a side of an era of horseshoe theory on anti tech. Right? Like when you think about someone like Bernie Sanders, you think about someone like Trump. These are populists from both sides who actually have a lot in common or more than we might have think. So my hope is that the AI safety people, I don't want to call them decelerationists because they don't call themselves decelerationists. And we should use the terms that people use for each other for themselves. My hope is that the AI safety people and the EAC people can publicly state, hey, we have 95% agreement. We're all pro tech. Tech as an industry is pro tech. We just agree on this little issue. Of course, it's of mega importance, utmost importance of AI safety, because that would signal to the rest of the world that there's alignment, whereas my concern is that both sides will be both red and blue in a populous site will be anti tech, and we'll be sort of missing the forest from the trees when we need to actually defend tech.
Speaker B: I share that concern. And actually, that might be kind of like the default state of the pendulum swing, because I would say the neoliberal order of the last 40 years has been very much like pro globalization, pro many things, but definitely pro tech. Okay. And I'm seeing whatever the new political order that's replacing it, to not be so favorable to tech as. As they previously were. And microcosm for this is like, is crypto for me, right. Is basically like, it's not a sure thing that team red or team blue will be team crypto. It's just totally not a sure thing. We have no idea where they're actually going to fall on this. And actually, to Eric's point, does make me, as somebody who is an advocate for crypto technology, decentralization, private keys in the hands of individual, all of these things start to skew a bit more extreme on the. More extreme than I otherwise would be in trying to make the case for, like, why crypto is good. Right? It's like I have to do that in order to push the agenda and to contrast myself against these movements from both sides of politics that would just shut it all down. They don't see the use case for proof of work mining in bitcoin, so just shut down all the mining facilities. They don't understand why people need to run a validator in their own home or why defi interfaces, you know, don't require AML KYC and all of these things. Right. So, Haseeb, what do you say to that? Like, Eric's making the case that maybe we, we should all band together, not necessarily on the far extreme IAC type of case, but we should definitely make a pro tech case, like, or else. Or else we'll fall prey to the political parties that just don't want any of this technology, just don't care about it.
Speaker A: So the EA tribe is often affiliated with being doomers about the technology, but I feel like the IAC side is, in a way, a doomer on the politics, where they basically believe that if we don't have this extremely rigorous defense of technology, that it's all. It's all going to go KABLOOEY and we're going to turn into this socialist state that we're all going to be enslaved by Elizabeth Warren and nobody's going to have any freedom. And the reality, just look, right now, you say, oh, the ex are in control, or, sorry, the eas are in control, the safetyists are running the asylum. If you look right now around the world, where are the AI restrictions actually happening? The answer is in the EU. That's where the most constraining stuff is happening. And then in China, people are saying on the ex side, they're like, oh, no, if we don't. EA is so anti tech. If we let the democrats, we have a democratic president right now. The Democrats, if they control things, they're going to shut down all the technology and they're going to make it so I can't use the Internet, I can't use social media, I can't use whatever. Go look at China where they have, people are worried, oh, China's going to race ahead of us. They're going to unlock the technology. They're the real accelerationists. Go read the AI regulations in China. In China, you cannot release an AI application without it being explicitly approved by the CCP. There are only 40 applications that have ever been approved by the CCP. You are liable in China if your large language model says something that's nothing true, you are liable in China. That's the law that they pass in China. Okay. That's fucking insane. That's absolutely okay. That is what you're up against in America. The reality is that, yes, there's a cacophony of voices, many people saying many different things. The ex are saying one thing, the eas are saying one thing. The AI anti bias people are saying some other things. And the reality, what did we actually have in the US? The answer is nothing. There is no laws, there is no regulations. There's nothing. Right now, there's an executive order that says, you have to let us know if you run a training run at least this size. That's it. That's the only regulation that we actually have. And regulation is a strong word for that. The reality is that there's no regulatory regime at the moment in the US. And the reason why that is, is because that cacophony of voices continues pulling things in different directions. We say, man, it really sucks that Gensler is doing this and the fincen is doing this and blah, blah, blah. Okay, what are the laws of crypto in the US? The answer is that there are none. There are no laws in the US. There's no regulatory regime. There's nothing. There's just a bunch of people fighting. And, you know, if you look at the founding fathers, the way that they described government is that they wanted that to be the case. They wanted there to be continual fighting between the different branches of government to prevent any centralized control in, for example, the president. They didn't want the president to be able to say, well, guess what? I'm president. So here are the laws now. Well, the laws can get shut down by judges. They can get replaced by legislators. And the reality is that that cacophony of voices keeps things in check. So anybody who's claiming, oh my God, there are eas in the government now, therefore, it's all going to go to hell, and we're now all going to be talking on our Google controlled smartphones and nobody's going to have open source AI anymore. It's all going to get banned. They're fucking wrong. It's never happened. It's never been true because of our system of government. So that's why there's a political doomerism that I also object to, which I feel like is part of what motivates the energy in demonizing the EA type. So I think what they have to say is a very good point that deserves to be heard. But do I want eas running the government? No, I want the government to be the way it is, which is a contest of different forces. There are people who are economically minded who have a good point. That's not being pro culture war, it's being pro democracy. Democracy, yes.
Speaker D: I'm just kidding. But you're pro this productive tension, and it plays out in it.
Speaker A: Yes. Yes. I'm very pro productive tension, 100%.
Speaker C: I actually really like this productive tension framing. Kind of like, as we get to the close of this conversation here, I kind of want to zoom out because I actually see the same acceleration. I know deceleration isn't actually a tribe here, but it's like a kind of a talking point. There's like, acceleration is first, deceleration is debate. But it's also not just like Silicon Valley. It's not just AI, it's not just crypto, whereas these things are generally all on the accelerationist front. Like, we're also seeing experiments going on in other technology verticals as well. Like Brian Armstrong or, excuse me, not Brian Armstrong. Brian Johnson is trying to experiment himself into, like, becoming a new class of humans. This is the. This is the guy who's taking any risk, or not any risk, but any sort of, like, experiment to try and, like, add, you know, five more minutes to his life every single day. Right? Like, people like Aubrey de Grey are literally trying to create, like, immortality. Even in the crypto space. There's, like, Vita Dao out there who's, like, trying to combine crypto and immortality to make it happen faster. And, like, this is. We haven't even, like, opened up the conversation about, like, gene editing, which is, like, not far away. Like, there's other technologies out there that's really going to, like, if you think AI and crypto is going to make the future weird, well, like, there's, like, three more technologies out there who's also going make the future weird. And the thing is, like, this has always been the case of, like, technology. Technology just accelerates. It's always accelerated. As soon as we develop tools, we can use those tools to develop other tools. And all of a sudden, like, we are an accelerationist in an accelerating state. And this is just the only difference between, like, now and any other time in history, is that it seems to be where we are now versus where we are at the end of the lifetimes. Might be, like, multiple orders of magnitude different. And that has never been true before in history. Like, we've only ever had, like, 20%, 30% gains at most. Usually over the span of human history, we have, like, 2% gains. And, like, for thousands of years, we actually literally had 0% gains. And we're now on this, like, parabola of innovation. And so if we're talking about this tension between governments, like, stakeholders in the government, it does make sense that when you have crypto and AI, and I like biotech and all of these other different, like, sectors of technology, which are all, like, making monumental gains, that all of government is like, okay, everyone break. Because who else is saying that? Who else is saying, like, yo, calm down. Like, that's maybe, like, kind of, like, my commentary on, like, why I think the government will trend towards breaks, not gas, but, like, maybe, Eric, maybe you can, like, check my reasoning on this.
Speaker D: No, I agree. That's part of a. The broader productive tension. And I want to bring in. You guys had this amazing episode with Vitalik where he identified and expanded upon his diac philosophy. And I think that's worth noting, too. And I think that piece had a great synthesis of the problem and sort of the common alignment between EA and IAC and tried to develop this sort of thesis and synthesis approach. But I think, unfortunately, I think it's a bit too neat. I think it's tough to just focus on technology that are defensive. Things that tend to be defensive can be offensive, and vice versa. Fire can be used for good, it can be used for evil. And if we're going to continue to develop these models, I don't see ways in which it can only be used for defensive ways. That's my commentary on that. And then to kind of your point, I think we are going to continue to need the AI safety people to develop and further develop our alignment capabilities, and we're going to need people who are pushing the brakes. It's my goal in this conversation to rehab or improve easy AI safety's reputation, but to also keep it in its box. As haseeb said, we don't want them running the government. They're not close to it yet. But the regulatory regime team is going to be up for grabs over the next few years. And that's why these fights are really important. Even if they only influence a small amount of people right now, they have potential to influence a large amount of people. And so I think we do need the right combination of brakes versus gas. I would rather see it come from within the industry, from people who know what they're talking about, than outside of it prematurely. Yes, we are going to see governments come in with some breaks. Ideally, they can be informed of by some of our people and some of the productive tension that we're having first.
Speaker A: The one thing I would add to that is that we've talked a lot about, oh, my God, what if the eas are running things, and what if they have total control? Won't they shut it all down? The reality is a much more politically powerful cohort is not the AI safety people, but the AI bias people. These people.
Speaker B: Much more people that Eric was calling eiadous ethics earlier in the conversation.
Speaker A: Yes, ethics, exactly. So these are people who are like, okay, what if the models are biased? What if the models are more liberal? What if they're more republican? What if they say things that they shouldn't be saying, or they give people instructions on how to cause harm or whatever? Now, the interesting thing about AI ethics versus AI safety is that the core problems that you need to solve are actually kind of the same, which is, how do you control these? And the answer is, we don't really know. We don't have good techniques to control these models. Now, how do you control a model? Well, that's called AI alignment, which is the core problem behind AI safety, which is, okay, right now, the thing that the model is doing that you don't want is making biased representations of people in certain stereotypical careers or something. But eventually it might be the thing you don't want is that gathering a bunch of resources or hacking people in order to make a bunch of money in order to fulfill some tasks that somebody who's running an AI agent wants it to do. These problems are one and the same. In a sense. One of them is easier to solve than the other. But the core of the problem is that we don't know how to control these models. They are very difficult to control. They're very unintuitive. They're kind of like these space aliens that we've conjured up. People often say that large language models were discovered, not invented, in the sense that we just sort of threw more and more compute. And then it's kind of like when something moves faster and faster, it turns into. From a liquid into a gas. In the same way, when you throw more and more compute at something, at a certain point, it becomes a large language model, and you're just like, whoa, what the fuck? This thing can now, like, talk to me and do my math homework. Like, that's really weird. And that phase transition is still very poorly understood. And I think there's going to be a lot of political fights about it. But if both the ex and the eas don't get their shit together, the AI ethics people are going to win because they have much one. They're much more persuasive. They're much more easy to understand. For most people, the whole Gemini drama had nothing to do with IAC or EA. It was all bias. It was all like, oh, this thing fits neatly into the actual culture war, which is the east, or, sorry, the left right culture war, which is, oh, these things are super woke, and they won't depict any white people. And that is much more powerfully animating among Congress. When you talk about, what are 70 year olds, who are actually the people making the laws and voting on this stuff, what resonates with them? I'll tell you, it's the AI ethics stuff. None of this stuff even rates yet. So everything that Mark Andreessen doing and that Eliezer is doing is important because by default, both tribes are losing.
Speaker D: And maybe this is a great place to end this idea. There's a saying, I'm against my brothers, but me and my brother are against my cousin. And right now, IAC is against ea. But EA and IAc need to team up against AI ethics or AI bias. Bias, because we can all agree that those people are wrong.
Speaker A: Well, hold on. Last thing I want to say I want to make some room for the AI biased people because obviously it's true that Gemini was a horrible model and obviously it's true that the AI models can do pedestrian bad things too. And you want to control that because way before AI's are going to become super intelligent or become so powerful they take over the world, they're going to do pedestrian harms. And you kind of want both, right? You want to also solve the pedestrian harms and that's a real problem. You also want to solve the existential harms. That's a potential real problem too. It's a much bigger problem. And you also want to solve AI over regulation. That's also a big problem. All three of these are problems that a good regulatory regime should find a way to solve. And so, I mean, that is my very lame and narrow attempt at synthesis of these three positions. But I don't think you have to say, well, you know, these two are illegitimate and this one is legitimate. The answer is obviously all three of them have legitimate concerns and you want to find a way to address all three.
Speaker D: That's too much nuance for me. I preferred my.
Speaker A: Fair enough.
Speaker B: Let's close this conversation out and I want to ask you just one final ending question, which is just maybe summarize the conversation for each of you. So when faced with the question, which you surely will. Bankless listener. And then Haseeb and Eric, are you team excel or team decel? Are you team Iac or are you team ea? Pick a side. What's the most sensible posture to that kind of framing of the question? To pick aside Haseeb.
Speaker A: So, I mean, I won't surprise anyone to say that I'm on Team A, but I'll frame that as saying I'm on Team EA because I think at the margin the world needs more of that. I think it's got enough people who are say, protech, no regulation, I want more AI products. And I think there's also enough people who are saying, hey, there's too much bias in these things and they're bad for society. I think there's not enough people saying, hey, these things are really risky and we should invest more into safety.
Speaker D: Also won't surprise people. I'm team accelerationist. I'm Team pro tech, but I think that we should absorb some parts of EA. I think EA is beaten down, they have wounds and we should pay for their hospital bills and bring them into our trial and team up against the AI bias or AI ethics who have some legitimate points. I will concede that, but also some points that I also very much disagree with. And I think my call out to accelerationists is to take AI safety seriously. I think they worry that if you concede that P doom is even a concept or that there are AI safety concerns, that you sort of throw out the baby with the bathwater and you go as extreme as some of the most extreme EA people. But I think we should find a way to have more common ground with ASFT people, acknowledge that there's 95% agreement with most of them and work on alignment and then fight the right enemies, which, as Haseeb mentioned, it's the AI ethics people. AI advise people, regardless of what you think of them, they have a lot more power on a regulatory stage, and that's where the battle is really taking shape. So I'm an acceleration with some humanism, and I think we should extend an olive branch to our most rational and reasonable AI safety people.
Speaker B: There you go. Nothing unites us like a common enemy. And so maybe that common enemy is the EI bias people. The people are trying to censor all these models. Haseeb. Eric, thank you for this fantastic conversation. I think this will leave bankless listeners with a lot to think about on this subject. It's been a pleasure.
Speaker D: Thank you.
Speaker A: Thanks for having me.
Speaker B: Gotta leave you guys with this. Of course, I usually say crypto is risky, but we talked mostly about AI. I think AI is risky as well. This is definitely the frontier. It's not for everyone. But we're glad you're with us on the bankless journey. Thanks a lot.
