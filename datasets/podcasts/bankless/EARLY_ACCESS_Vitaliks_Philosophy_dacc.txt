Speaker A: We are creating financial systems that work without relying on any specific country. We are creating forms of privacy that work without relying on a central actor to hold everyone's information in custody for them. We are creating forms of accounts recovery that don't depend on Google or Twitter having everyone's master keys. And that's happening with social recovery wallets and account abstraction and ERC 4337. We are creating zero knowledge proof technologies that let people prove that they are trustworthy without revealing any more information about themselves beyond that. So we're creating all of these really powerful tools that in a lot of cases are substitutes for more centralized forms of trust.
Speaker B: Welcome to bankless, where we explore the frontier of Internet money and Internet finance. This is Ryan. Sean Adams. I'm here with David Hoffman, and we're here to help you become more bankless. Vitalik Buterin. On the episode today, he's sharing his philosophy with us. He calls it DiAc, and he explains what he means by it in this episode. I think there's really three reasons we wanted to have this conversation on bankless. The first is this. There has been a society wide debate on what to do with technology, specifically AI technology. So we've got the tech accelerationists, that's the EAC community. We've got the tech deaccelerationists, that's the EA community. And the debate is whether we should continue on this path forward towards AI the way we've been doing it, or if we should stop because maybe the robots are going to come kill us. And I know David and I had been hoping to pick Vitalik's brain on this for quite a while, ever since we had our episode with Eliezer Yudkowski, who informed us very politely that we're all going to die. So we asked Vitalik what's his probability of AI doom? The second reason for this episode is, I think the philosophy that Vitalik lays out is something everybody in crypto can align on. It's a way to really unite the tribes. It can help us explain what we're doing here to the world and why it matters. And more important, I think coming out of 2022, when we seem to have lost our way in crypto, it reaffirms why we're here. It's a core part of reestablishing our soul and getting to the bottom of things. And the third reason we're having this conversation, it's Vitalik Buterin. Okay, that's enough. He's always got interesting things to say. David, why was this episode significant to you?
Speaker C: I think even more broadly, outside of crypto, society at large is having a conversation with itself as to how fast it wants to go into the future. I think there's some parts of society which is concerned about the vanguard of Silicon Valley and tech innovation moving faster than what society can really, as a whole, keep up with. And then there's other parts of society was like, guys, you solve problems via technology. The acceleration and the speed of technology will help others catch up as well. And this conversation of how fast we want to go collectively as humanity, is causing tension in society at large and well as podcasters, what do we do? We try and define the landscape, define the contours of the conversation, help people learn the perspectives of other sides, discover what is signal versus noise, discover what is truth. And so I think when we have clarity on these conversations, we can all be in agreement in the direction that we want to go. And I think this is really what Vitalik did with his blog, and what we were hoping to do with his podcast is help define the conversation a little bit more so humanity can get on the same page. And so that's why this episode is significant to me.
Speaker B: All right, guys, we're getting right to the conversation with Vitalik on Diack and his philosophy. But before we do, we want to thank the sponsors that made this episode possible, including our number one recommended, crypto exchange. That's Kraken. Go create an account.
Speaker C: Bankless nation. I'm extremely excited to introduce you to Vitalik Buterin. You know him as the creator of Ethereum and Ethereum researcher, but today he comes to us wearing the hat of philosopher, which is, I think, what we are increasingly need, at least in the sector of tech, as we are having conversations around the world of technology acceleration, as we are approaching new frontiers, both inside of crypto and outside of crypto that are defining society at large. Vitalik recently wrote this post, my techno optimism, which has made the waves in the tech space about what to do about this increasing pace of technology. And that is the subject here on today's episode of Bankless. Vitalik, welcome back.
Speaker A: Thank you, guys. That's good to be here.
Speaker C: This post that you wrote, Vitalik, which subtitled my own current perspective on the recent debates around techno optimism, AI risks and ways to avoid extreme centralization in the 21st century, made the rounds inside of crypto and just immediately outside of crypto as well. And this has been, I think, a continuation of a larger conversation that much of the world is having at large with its relationship to the globe's technology sector. So can we set up the conversation and kind of set the table, if we will? Because it's happening society wide. Two camps are forming. There's what is known as now recently, the accelerationists, the pro tech, and then the decelerationists, the anti tech. I'm not sure if anti is fair. How would you set the table of this global conversation that's being had?
Speaker A: I mean, I'm not even sure if two camps is the right way to describe it because I think what I honestly see in the world and some of the discussions that have been happening in the world is like people being confronted with these completely new issues made by completely new people that they are not used to paying attention to, with completely new memes and, you know, weird vocabulary like Shoggoth and P doom and timelines and all of these things. And, like, it's almost like an awakening in that for the first time, people are actually thinking, because, like, their existing camps don't really tell them how to think, right? If you think about the even most recent IAC versus effective altruist debate, to kind of give very approximate and crude labels to the two campsite, for example, like, this is not red versus blue, this is not us versus China, this is not Europe versus Russia. This is not woke versus anti woke, right? This is basically a debate that's happening between two groups of people who even two or three years ago, considered themselves to totally be part of the same tribe, basically, the San Francisco centered, tech forward, AI leaning, grey tribe, people who went and in many cases, continue to go to the same dinner parties or in the same social circles, and suddenly there's this issue that has basically pulled them in varied different directions. If you're coming at it from the outside, then existing tribal mirrors don't really tell you much about it, right? Because if you're the type of person who, for example, thinks that tech people are bad and these are rich, white, male dominated fields that are totally out of touch with the rest of reality, then, well, guess what? Both of these camps have a very large number of people that fit both of those descriptions, right? And both of those camps have, I think, a large number that do not, that also gets. That also gets underappreciated, right? Like, there's a huge international audience for a lot of these AI focused things because similarly to how within the crypto space, things like ZKE, evms and ERC 4337 are kind of resetting a playing field and giving opportunities to people from regions that haven't really been historically well represented in ethereum, I think a lot of people around the world do also see AI as that kind of opportunity. And then if you're a left leaning person who is very skeptical of corporations and who is pro governments, to put it crudely, but most skeptical of governments when that government is being influenced by corporations, then, like, well, guess what? The EIAC and EA camps are both doing a lot of influencing of governments and there's lots of money on both sides. I think the way that this conflict really doesn't map to these existing tribal mirrors is like, it does create this interesting property in that people actually have to figure out for the first time what actually is their own perspective on the particular issue and how do they actually think about this totally new thing that was not even on most people's radars, like, even two years ago. Right. And I think, like, within crypto, it's the same thing, right. In the sense that I feel like crypto has operated in a bit of this bubble where, like, there hasn't really been too much dialogue between, like, what's happening in the space and some of the discussions, like, both technological and political, that are happening outside of it. Right. And I feel like there is to some degree, this kind of extent to which big parts of the space were born in this 2008 era context where, I mean, we're talking about, like, the second, the chancellor being on the brink for the second bailout for banks, as the bitcoin Genesis block says, you know, like, literally. Yeah. As part of the block body, we. Yeah. Discussions around, like, ending the Fed, creating an alternative to central banks and all of these things. And a lot of the big discussions in 2023 are just totally not related to that at all. Right? It's like, I'm sorry, but the Israel Gaza situation is not going to be better if those lands were ran on sound money. Right? And same with AI. Sorry. Sound money is not going to make p two go down. It is possible to kind of overstate this, I think, for example, with the recent election of Argentina wherever, I feel so far totally unqualified to give a kind of grand, this is good, this is bad perspective on it. But what I have noticed from the sidelines that fascinates me is that millet is actually talking about economics and argentinian people do actually care about economics. There definitely is an extent to which the US and the rich countries in general, we have, I think, like, as Balaji has pointed out, moved from caring about the economic access to caring about the cultural access. Right. It's like, like the issue that emotionally arouses people in the, in the US these days, like, it's not, you know, like pensions and healthcare and, like, savings. Right. Or at least like, that's not what the newspapers report about. But, you know, in places like Argentina, it. It still is. And there's a refreshing, sort of grounded in reality aspects to it, but it's important to recognize the ongoing importance of that, but also, at the same time, recognize this growing, rapid emergence of these conversations that have just nothing to do with any of those questions. And there's this big question of how does crypto actually relate to these topics? And I think a lot of people who come into the space come into the space because they have ideals and values and goals and dreams that extend beyond fairly narrow details of this is what the structure of the money is going to look like. I think it's important for the space to try to engage in some of those other topics as well. Right? Yeah. So I think for a lot of those reasons, like, I've been thinking about some of these other technological topics as well. And one of the things that I noticed, especially this past year in 2023, is that I have a lot of beliefs about blockchains and cryptocurrency, NCK snarks. I also have a lot of beliefs about the importance of longevity research. I also have my beliefs about geopolitics. I also have my beliefs about AI, and also have my beliefs about effective altruism. But, like, these sets of beliefs were not really talking to each other enough in a lot of cases. Right? And, like, asking the question of, like, what? Really? Yeah, look, what is it? Or your actual take on, like, how crypto fits into this larger picture of the world? And, like, does that perspective, do the different parts of that perspective really actually make sense in the context of the other parts of the perspective? One of the questions that we totally should ask, for example, is, if AI is so important, then why not drop everything and start working on it? I think there are good answers to that question, but it is a question that actually needs to be asked. And so, yeah, like the. So, like, I was thinking about a lot of these topics, and then, of course, about a month and a half ago, Mark Andreessen's techno optimist manifesto came out. And then, of course, an entire spectrum of replies to the technoptimus manifesto came out. And then I started at least thinking about how I would write this kind of piece. And then, you know, Exoo connect and Devconnect, deleted a bunch. And then finally, yeah, you know, the OpenAI situation and just kind of like blasted basically the exact same topic into focus in a lot of people's minds again. And so I decided, like, I'd actually need to, like, get this document out there. And here we are.
Speaker B: Yeah, that's what I very much saw in your post. Vitalik is kind of like, it accomplished this goal of maybe creating a unified philosophy for crypto in the broader context of the societal conversation. And I want to set that up because, you know, this year is, I think, the year that this societal conversation between this acceleration view, which you called IAC, which stands for effective accelerationism for folks that are not familiar, and this more kind of anti tech type view, effective altruism, maybe, like, to your point.
Speaker A: VitaliK well, I mean, just like, just to kind of insert a ten second parenthetical, I think. Just remember that, like, as little as two years ago, the main criticism of effective altruism is that these are tech people who believe that, like, quantitative and technological solutionist stuff is the problem to everything and ignore the non technical and immeasurable side of life. Right. And so fast forward two years later, and now it's just interesting to note that they're basically being criticized from the exact opposite direction now. Right?
Speaker B: It is fascinating. And there does seem to be like, to your point, this is all the same tribe that has maybe like in crypto, we'd call this a fork, maybe a social fork, right? Where we've got now, these accelerationists coming out and saying, whoa, whoa, whoa, we don't subscribe to kind of some of the anti tech philosophy of some in the AI safety movement. I would say that bankless listeners, maybe, and from David and myself's perspective, were first exposed to this through the AI conversation, the AI safety debate. So we had an episode back in February of 2023 with Eliezer Yukowski. That is what I would say imprinted on my soul. Vitalik okay, so it was basically the first time I was exposed in depth to someone who's very intelligent and had been thinking about a concept for decades. Now here it was with chat, GPT that the AI posed actual existential threat to humanity. And so we went on a quick sidequest from our regularly scheduled crypto channel to explore that a bit, and we uncovered that this is not just a question that bank lists and people in crypto are facing, but it's a societal level question, and it feels like the rest of society is, you know, the reason David put this into two camps. I realized that there's a lot of subtlety and granularity between the two camps, but it's almost like society's being asked to choose. What do you think about technology? Do you want to pedal to the floor and fully accelerate, or do you want to just stop, slow it down, and just, like, be cautious? And so I'm wondering, Vitalik, if you could give a quick definition for folks that are unfamiliar. What is the accelerationist view? I think you have a meme of this that opens your article and it says, dangers behind Utopia ahead. That's the accelerationist view with respect to technology. And the anti tech view is there's safety behind and dystopia ahead. So we're journeying into this dangerous frontier, I guess I would say, is the anti tech view. Could you just illustrate the core viewpoints of both camps to ground us in this episode?
Speaker A: Sure. So the way that I would think about the effective accelerationist perspective is basically it's all about recognizing the invisible graveyard, right? The invisible graveyard is a phrase that I think either Alex Tabarak or someone else came up with in the context of talking about the harm that the FDA causes in the US just by delaying the extent to which it approves certain drugs, right? Basically, yeah. That if a life saving medicine gets delayed even by a month because of regulatory hurdles, then that's something that can easily kill tens of thousands of people. And if you do the math, then the amount of people killed by these things potentially goes way higher. And if you kind of zoom out even a bit more broadly, like in the meme there, right, you have a utopia head, and then behind you, you have a bear. And the biggest bear of them all is probably aging, which is a condition that kills about 60 million, literally a world war two scale number of people every year. And if technology doesn't massively accelerate, the base cases that literally all of us are, including everyone listening to this podcast right now, is going to die. The gains that come from technology are just massive, right? Like, if you just think about the difference between the kind of life that we have now and the life that we have 1000 years ago or even 50 years ago, there's just a whole number of both measurable and immeasurable things that have massively improved. And, like, these improvements are incredibly large. And these improvements can even overshadow some of the worst things that happen, even if you can blame some of those things on technology, right? So if you look at the chart, for example, right, like, we can understand what some of those dips are, right? So, like, for example, if you look, there's like a whole bunch of correlated dips in the 1910s and obviously that's World War one. In the case where there is a double dip, the second dip is the spanish flu.
Speaker B: We're looking, by the way, bank listeners, at a life expectancy chart over the past 120 years or so.
Speaker A: Yeah. And it has, I think, about eight lines for various countries. You know, us, Europe, Asia. So there's some dips in the 1910s, where there's double dips. It's world War one and spanish flu in the 1940s. There's dips, which is obviously World War two. Then China's got a dip in 1960, which is the great leap forward. And so there's, like, very visible disasters along here. But if you just zoom out even a bit, all of these dips ultimately are overshadowed by, just, like, the incredibly large gains from medical technology. Right? And so growth in technology does a lot, and even growth in wealth does a lot, for example, right? Because if you imagine a world where everyone is wealthy, then that's a world where if you suddenly have to leave your home and pack up and go to another country, then you're not going to starve. You're going to basically walk into a much more stable situation than you would otherwise. And so there's just all of these big positives that come from technological gains, and there's, like an unrelenting history of thousands of years of technology repeatedly doing good despite lots and lots of people screeching and complaining that things could end up going in the opposite direction. So that's the accelerationist case, anti tech case. I think it's important to separate the kind of old school anti tech case versus the AI Doomer case, more specifically. Right. So the old school anti tech case, it's one that I admittedly am, on balance, very not sympathetic to. And I've written about this in the article, but I think if I had to illustrate kind of the really biggest and most important, like, the strongest parts of that case, obviously, the environment and climate change are a really big aspect of this. Right? So there's this chart of how temperatures are suddenly rising in a way that's, like, totally unprecedented in any historical natural situation, except possibly for, like, asteroids that, like, fell, like, many tens of millions of years ago or something similar, right? Yep. There's. There's the graph. Right? It just over the last century and a half, it's just gone vertical. There's graphs of species extinction that are pretty bad. There's graphs of even populations of particular animals that are pretty bad. And then another kind of aspect of this that starts bleeding into the AI discussion is like, the possibility of technology getting misused by authoritarian governments. But I feel like even, like, that's not a risk of super intelligent AI, but that is a. I mean, super intelligent AI would make the risk worse, but that even is a risk of, like, present day AI and, like, present day surveillance technology. Then there is just the fact that easier communication creates greater economies of scale, and that creates greater centralization, which creates opportunities for political conflict of a scale that totally did not exist before. So you could try to make that argument, though. I think at the same time, Genghis Khan would be a pretty big counterargument to that. You know, the guy who genocided about as many people as Hitler back in the 13th century, but then for some reason, it's, like, totally socially acceptable to sing songs about him today. But. Oh, my God. Yeah. So, you know, there's that. But the. And then I think on the climate side, like, my counterarguments to that case is basically that, like, there have been, there is a history of lots of specific environmental issues that once they became bad, we actually did get together and solve them. And improvements in air quality in cities are a big one. Right? Like, I remember seeing the tail end of this myself and the first time I visited in Beijing back in 2014. Just remember how incredibly smoggy it is. And that aspect of China improved massively and very visibly over the six years that I went to visit over and over again since then. There's ozone and some reforestation in some areas and so forth. That would be the counter to the counter. But then the anti tech argument that is more compelling to me personally is like this. What? This, like, very specific one about super intelligent AI, right? And super intelligent AI, to me, is like, one way to think about it is to think about it as being something that's in the category of technology. So I think about it as being the same kind of thing as smartphones, the Internet, contraception, the printing press, the wheel, guns, the steam engine. And these are technologies that in many cases really were socially disruptive and in many cases definitely did harm people who depended on the incumbents. But at the same time, if you just look at it from the eye of long term history and you realize that there's massive good that came out of most of them. I mean, guns are, I think, more controversial, right? Because military technology is the one branch of technology where it's, I think, much less clear that improvements are good, though. I mean, actually, even there, right there, there's an interesting argument that some historians make that guns are sort of more democratizing than the previous wave of military technology, which was bose, which, you know, required, like, ten years of training to be able to use well. And so I kind of enabled more centralized forms of government. But even still, right, like, military technology is, like the other big exception in general, right? But aside from military technology, like, on average, technology has been crazy good. And so if you think of AI as being technology, then, like, your first instinct is going to be AI is going to be crazy good. And maybe you would worry about AI military applications, but if you instead think about AI as not creating a tool, but as creating a new type of mind and creating a new type of mind that is far more intelligent and powerful than the human mind, then this puts us in a totally different category. If you think about humans, right, humans have been able to take over and utterly dominate the world and even accidentally genocide all kinds of species of animals, in most cases without even intending or even realizing that that's what we're doing. And humans got into this position of power entirely because of our minds, right? Like, our minds enabled us to create tools, technology, better work together collectively and cooperate and do all of these things. And then now imagine an AI that beats humans on that exact same metric, right, by a factor of 10,000. Then the question is like, well, what's going to happen to humans? And the big risk here is the comes from this argument of that I call, well, two arguments. One argument is the difficulty of alignment, and the second argument is instrumental convergence. So the difficulty of alignment is basically the difficulty of just making a thing that has the same kinds of goals that we have. This is a surprisingly hard problem that we just have no idea how to do, right. And there have been plenty of these myths and legends in history that kind of talk about the alignment problem. So there's King Midas, who famously got the touch that turned everything into gold. And then, of course, he ended up dying of starvation. Then there was, I forget who, but the greek mythical figure who wished for immortality but forgot to wish for eternal youth. And so there's the problem that if you don't perfectly specify what you want, then there's lots of ways for that to be just satisfied in a slightly different way.
Speaker B: Do you guys remember that? I think it was an Edgar Allan Poe short story called the monkey's Paw, or something like this, where the character receives a monkey's paw that gives him three wishes, and so he would wish for things. One of his wishes was he wished for money. And what ended up happening is the monkeys paw granted the wish but his son died later that day in a tragic factory accident, and so he received some proceeds from the workers compensation policy. That's how the wishes were fulfilled. It reminds me of that as well.
Speaker A: Mm. Mm hmm. Yeah. Now, I mean, I think one kind of rebuttal to this that, like lots of people make and that I've made is that, like, if you think about if you just interact with existing AI's, like even chat GPT just a little bit. Like, these are not hyper autistic robots that have no idea how to understand, like, context and I unexpressed intentions and subtext. Right? Like, JGBT will totally be able understand that. You know, if a human says, I want anything I touch to turn into gold, he has an exception in mind for things like food and water and medicine. Right. But there is this by itself doesn't save us, and it gets a little bit more tricky to sort of explain why. Right. But but I can try, basically. Yeah. The best analogy, unfortunately, is kind of like looking at some of the previous generation of AI that we've had. Remember back in the mid 2010s when people were starting to work with deep learning AI's for the first time and they were starting to get kind of good at making pretty pictures. And there was a thing that you can do where you can run the model forwards. And if you run the more model forwards, you pass in an image and it tells you like, is this thing a cat or is this thing a dog? But then you can also run the model backwards, right? And you can pass in the input that, like, I want the ultimate essence of a dog. Like, I want the thing that really is going to be 100% dog. It's going to maximize the extent to which this classifier is going to tell me that this thing is definitely a dog and it's not something else. And then you run it through and it generates the image that maximizes that. And it turns out that what you get is totally not a dog, right? What you get is like some insanely crazy contraption that definitely, like, has doggy aesthetics. But it's also got, I mean, like maybe twelve eyes, maybe 48 eyes, maybe there's like a whole bunch of dogs that have merged bodies. It just like, looks like some totally crazy thing.
Speaker C: And I'm getting visuals of a kaleidoscope.
Speaker A: Of a dog, right, exactly. Right? And like, this is the thing that, like, maximizes the dog parameter, right? And I think another example is like, us humans have sort of hacked evolution in the same way, right? So, like, if you think about what evolution is, evolution is like this agent that has a goal, right? And its goal is to, like, maximize the reproductive fitness so, you know, survival times, how many children you have of whatever agents it's operating on in their environments, right? And in order to fulfill this objective, evolution gave humans a lot of desires, right? Like there is the desire to have food, the desire to have delicious food, right? And what is delicious and what isn't delicious? Well, those are things that were fine tuned based on, like, what is nutritious in the, in the natural environment, right? And then there's the. A lot of desires associated with reproduction. There's desires associated with survival and all kinds of things, right? But then look at how modern humans have dealt with these desires, right? And one is we've created a lot of food that's, like, hyper optimized for deliciousness, that in a lot of cases doesn't do well at all on nutritional value, right? We've invented at least, like, five separate different types of technology that let people have sex without getting pregnant, right? We've invented all kinds of things that, like, satisfy the proxies that evolution has created in our minds for evolution's goals, but that do not actually satisfy evolution's goals at all, right? And, you know, the results of this is that, like, lots of people are eating unhealthy food, and there's increasingly a depopulation crisis with lots of countries having fertility rates that are below one. And so it's now, one thing you might ask is, well, surely we as humans know that we are not following the goals of Mother nature. And the answer is yes, we know very well that we're not following the goals of Mother Nature. But guess what? We are humans. Our goal, maximizing reproductive fitness is the goal that Mother Nature had. It is not the goal that we have. And we know that Mother Nature was not able to perfectly sort of copy its goal into us, but we don't care. We have our goals and we follow what our goals are, right? And so in the case of AI, what might happen is, like, we tell the AI, as Alex Friedman would say to, like, I'm going to bring more love and peace into the world. And then the AI would discover that, like, okay, here's a bunch of things that look like love and peace, and these are things that we would all recognize as being love and peace. But then at some point, it would discover, like, wait, if I create this 47 dimensional squiggly that looks in this particular way, then, like, it's going to be even better at max. Like, satisfying its own internal conception of love and peace, which is going to be slightly off because anything that is created by any finite process is going to be slightly off. And this 47 dimensional squiggly is going to look incredibly lovely and peaceful to the AI. But it's like nothing that any of us would recognize as being love and peace in any sense. And then we go and tell the AI, bring more love and peace into the world, and then the AI just kills all of us and replaces us all with 47 dimensional squigglys. Right? This is kind of the AI safety case, right? I mean, this is basically the same as what I remember Eliza Ryudkowski told you guys, and it's the same thing as what a lot of people have said. So this is like one of my points of concern regarding AI. But in my post I also talked about two other points of concern regarding AI, where one of them is just this question of, like, well, even if everything goes well, like, is this actually a world that we would want to live in? Right? And it turns out that, like, if you examine the Sci-FI worlds that people have tried to come up with that show humans and bots living in harmony, then, like, either the world is like insanely unrealistic and it's just unstable and it's just obviously going to collapse into AI's dominating everything in what, you know, like in another one to ten years, or it's like a world that actually really feels quite deeply unsatisfying from most people's perspectives today, right? Like, we're basically talking about a world where we all become pets of the superbot. And a kind of human agency doesn't really play any part in determining which way the universe goes from there.
Speaker B: And by the way, Vitalik, for those that maybe doubt that a bunch of machines or a bunch of computers could actually wrest control over humanity, when I was reading your article, I was in kind of a serious mood and I was drinking some coffee. I literally spat out my coffee, laughing at this turn of phrase you used. You said this to see why the machines could wrest control over humanity. Imagine that you are legally a literal slave of an eight year old child. If you could talk with that child for long enough time, do you think you could convince the child to sign a piece of paper setting you free? I have not run this experiment, but my instinctive answer is a strong yes. And so all in all, humans becoming pets seems like an attractor that is very hard to escape. I was just visualizing in my mind trying to convince an eight year old child to set me free and what that process would look like. And that's kind of what we're dealing with when we're dealing with a super intelligence. To them, we would be that eight year old child. It would have probably no trouble convincing us to do whatever it wants to fulfill whatever outcome and set of goals that it had.
Speaker A: Yeah. And then the other thing to keep in mind is that, like, to the extent that there is any notion of competition in this world of the future, like, whoever really gives up control of the reins to the AI is going to outperform the people that don't, because that's what happened in chess, that's what happens in go. That's just what eventually happens anywhere. So that, and then the third risk that I outlined is basically accentualization and surveillance. And this isn't even just a risk of super intelligent AI, it's a risk of, like, basically AI of the type that exists already. And this is actually, in some cases, situations that happen already. Right. So one of the things that's been happening in Russia for the past while actually, yeah. Even quite a bit before the recent war started, is that you'd have protests. Right. And unfortunately, the authoritarians discovered this one weird trick, which is you let the protest happen and you send the police out and you do the usual protest versus police thing a little bit, but, like, you don't aim to crush it right then and there, but you have the cameras out and then you identify who all of the key people in the protest are. And then at some point later, at 02:00 a.m. they get a knock on the door and, you know, like, repeat and rinse about 100 times. And five years later, suddenly you have, like, basically, yeah, almost no one left to lead the protests. Right.
Speaker C: And meanwhile, every other single country is like, oh, this is a normal, functioning, you know, society who's expressing their desires and they are free to express their desires.
Speaker A: Exactly. Yeah. It's like, much less visible, which makes it much more difficult to coordinate against, and it makes it much more difficult to even create outrage against it internationally. It's a big problem. And I think it's a big part of the reason why it feels like protests against authoritarian regimes have been getting less and less effective for the past. While I. And if you just extrapolate this trend even further, then the risk basically, is that there isn't a place to hide anymore, there isn't a place where any kind of credible opposition movements to a government could even start, because as soon as it starts the surveillance can detect it. And I mean, you don't even need, like, physical police. At some point, the AI soldiers could just, I could go and shut it down. And this gets even worse when we think about wars, because, like, the need to get the population on your side has historically, like, actually been a pretty significant break, at least slowing down people's desires to go to war. But then if your entire army is a bunch of robots, then, like, you know, the dictator gets drunk at 10:00 p.m. they see someone being mean to them on Twitter at 11:00 p.m. and the drones start flying and raiding hellfire on other countries before midnight. Right? And so this kind of natural check and balance that comes from the facts that, like, whatever, ultimately, there are decentralized humans that have to be doing the executing. And if you try to do something really terrible, then, like, those humans are going to be demoralized. And look, they're going to be much less, they're going to be much less willing to go along with your plans. And even if they do, lots of them are going to leak every single detail of your plans to the CAA or to whoever your opposition is, which is actually another thing that fortunately did happen in Russia. Let's say you have AI armies. All of those checks and balances go away. This is my other big concern about AI, basically, is this the ultimate centralization from which at some point, there might not actually be an escape? So those three cases are my kind of big note of caution on artificial super intelligence in particular, and how it's pretty unique and pretty different from all the other technologies that we've dealt with over the past ten millennia.
Speaker B: Okay, Vitalik. So David's going to come and summarize this in a moment. So we're tracking our journey so far through this, and then we want to kind of introduce your philosophy here and what you think the counter is and how that applies to crypto. But I have one follow up question to you specifically that's been kind of burning in me since Eliezer podcast back in February. And that is, what do you think? We talked about three different AI risk scenarios. The first is doom. Basically, we all die. The second is we become pets. And I'm like, at this stage, it's better than one that's not so bad. And then the third is totalitarianism. But I want to go back to the first because that's been giving me an existential crisis all year. What's your personal take on this? You've got a spectrum in your article on what you called earlier, the probability of doom, the p doom ratio. What's your p doom ratio and why? I'm just curious where you weigh in.
Speaker A: Yeah. So the number I gave in my tweet thread is 0.1. So 10% chance super intelligent AI is going to kill us all. And I think the reason for this is I see both sides of the arguments, right? Like, I see the sort of, quote, doomer arguments, which basically. Yeah. Is essentially what I've already outlined to you guys. And then I think if I had to give the counter Doomer argument, I would basically say something like, you know, look at the kinds of AI's that we have now. Look, those kinds of AI's are not even goal seekers. They basically are things that, you know, put on human costumes and sort of play out roles of the type that, you know, cube of, like, whatever type, they sort of pattern match themselves into thinking that they're in at that particular moment. Right? Like, Chedgpt does not act like something that maximizes any particular objective. Like, if you tell it to make the world more doggy, like, it's not going to do anything that looks like maximize the essence of dog, right? It's going to just give you a five paragraph essay that's a pretty normal and human thing that expresses what might it mean to make the world a more doggy place. And it's the sort of thing that's pretty inoffensive and it looks pretty fine, right? So basically, yeah. If you just compare the specific scenarios that people worrying about AI feared would happen with AI at current level of capabilities, and then you compare that to the actual thing that AI at current levels of capabilities does, and it's always quite different. One thing I'm happy about is that I feel like the, the level of harm from deepfakes in particular so far has been much lower than what I think most people expected. And even what I expected at current levels of capabilities. Right? Like, if you explain to someone from, from back in 2015 what the current level of capabilities of AI making deepfakes is like, they're probably going to tell you, like, whoa, we can't trust anything that people say anymore. Like, this is going to totally break elections and it's going to lead to all kinds of horrible consequences. And some bad stuff has happened. But the reality is much less than that fear, which is interesting and surprising. To some extent, it shows the adaptability of mankind. To some extent it shows that mankind is less evil than some people fear. And then, of course, the doomer would sort of counter by saying, well, guess what, with super intelligence, neither of those two things even matter because the AI is going to be doing all the work. But it does still feel true that sort of the way that things keep progressing do sort of go in different directions than what people's existing worst fears have been, and in ways that sometimes feel like we're going further and further away from the kinds of hyper optimizers that people are afraid of. So that's the case against. And if I had to again counter the counter, I would say, well, that's LLMs. And it's looking very possible that LLMs basically tap out at some level of capability. And GPT four, and a little bit better than GPT four is basically what we're going to get. But then there's going to be some next technology which could be combining LLMs and Q learning. It could be something else. And look, we don't even know what property is that next level of technology is going to have. And so it's like a big washout, right? And so I think like there's a big probability chance that the AI doom problem is just like, it turns out that it was never that big a problem to begin with. There is like a big chance that it is a really big problem, and within that chance, there is a really big chance that it is a really big problem. But I mean, with awareness and hard work, we will be able to deal with it and make sure that we don't actually get doomified. So that's where I think the balance comes in. But it's important to keep in mind that a 10% probability of doom is still a big deal. So, for example, one analogy for this is 10% is, I think, greater, bye. Somewhere between a factor of one and three, I forget, but it's only a little bit greater than the probability that any of us is going to die from a non biological cause. And so if you think about accident or something like this, car accident, homicide, suicide, like any nasty thing that's not disease, then if you think about the amount of just like care and effort and thought that you personally put into your physical safety and the amount of like care and concern that you expect, I mean, like governments to put into your physical safety with things like police, then, like roughly that level of care is a reasonable level of care to have about, I don't know, the possibility that something really bad is going to happen out of AI, right? And obviously, and that doesn't mean like overturn the entire world to suddenly care about this problem, but it does definitely mean care about the problem more than we do today.
Speaker C: So, Vitalik, I want to kick the stool and throw us all the way back to the start of this conversation. We just went down this AI rabbit hole to talk about the potential risks of acceleration, which is one of these camps that has kind of emerged in thought as our society is having this conversation. The risks of going forward in time and accelerating our progress with technology, and you've labeled, well, AI, is this exceptional case that we need to consider. The circumstances of some other examples are like climate change. There seems to be a correlation with the increased risk of climate change and the quickening of technology. And this is the decelerationist camp, if you will, and then the accelerationist camp, which I think you have said that you are more resonant with, says that, well, as technology advances, we find these occurrences in history, these wars, these increased capabilities to cause harm, but by and large, they are completely drowned out. Bye. All the other focus, all the other developments of technology, we've kind of, like, set the stage for these different constructs of thought. And like all extremes, they're all relatively blunt, if you will. If you just only focus yourself in one school of thought, it's a blunt, it's a blunt tool for the job. And as we progress forward in your article and in your thought, we start to get a little bit more nuance, a little bit more. We don't have to be so blunt in our thought about the direction of society. We can pick and choose components from different schools of thought and put them all together. We understand that AI has risks. We understand that we have to solve climate change. We understand that technology presents new risks. Yet nonetheless, technology also helps us navigate those risks all the same. So rather than having to pick an extreme, pick a tribe, if you will, a school of thought and be shoehorned in there, how would you propose we think about all of these different things that we've put together here? If we're talking about a unified idea of thought, how would we have a framework for understanding, if we are going to go forward with technology, what is the correct path, the more optimistic, more precise path that we can maximize the good? Because really, all of these conversations are all about how do we maximize human welfare and well being. At the end of the day, how would you proceed in this conversation from here?
Speaker A: Yeah, so I think this is where the idea of DIAC that I came up with in the post really comes out of. Right? So ack is obviously acceleration and d is intentionally standing for primarily for defense, but also for decentralization and democratization. And so the idea of this philosophy is to basically look at the offense versus defense balance of the world, right? So basically look at how easy is it to do things that harm or go against the goals of other people versus how easy it is to protect yourself or your community against that. And think of that as something that is and has always been shaped by technology, but also something that can be shaped by future choices in technology. And to really focus on building defensive technology and especially, and I think this is the angle that really naturally appeals to people in Ethereum and people listening to this, defensive technologies that work by improving defense in the abstract without kind of coming with a built in perspective or a built in enforcer that decides what is good and what is bad across the entire world. World or across an entire ecosystem, right?
Speaker C: Would you say, like it's phrased differently, defensiveness as a platform or just a technology platform that allows defensive technologies to emerge?
Speaker A: The word platform is interesting, right? Because I feel like this is one of those sort of sometimes vc buzzwords that means a lot of different things. The part of platform that appeals to me is like this idea that a lot of this is going to involve creating common infrastructure and both defensive infrastructure and even common infrastructure that enables building many kinds of defensive infrastructure. The part of platforms that I'm kind of like, I will, I want that cautions to me against embracing that word is that a lot of things in the modern world that call themselves platforms are things that do contain this centralized actor that controls the thing and that plays this role of deciding who's good and who's bad, right? Like, we talk about Facebook being a platform. I'm not open AI being a platform, Twitter being a platform. And all of these things have centralized actors that run and control them. They have centralized forms of moderation. They have centralization on all kinds of levels. And this is one of those things that creates a lot of problems. There's this book that was written recently that talks about this concept of weaponized interdependence, basically this idea that the type of technology that we've been building for the past 20 years, and when I say we like, I do mean, you know, the centralized world, like, this is the place where the decentralized world gets to pat itself on the back and say like, yeah, no, we're actually better than that, is that the technology has been networks technology and it's networks technology that creates centralized choke points where the creator of the technology has ongoing power over the users. Right? Like, if you think about just going back to the year 1970 and let's say, pick a random country that has powerful technology that we don't trust, actually, let's just not say anything bad about anyone in the physical world. Let's say you have Mordor. Let's say you have literal Mordor, and it just pops up in the middle of the Atlantic, and it turns out that that's what Atlantis was the whole time. And it's a technological superpower. Imagine you're in the 1970s, and you're buying cars and forks and knives and all kinds of things that are made in Mordor. And we ask a thought experiment of, like, what is the worst that the Mordorians can do to you? And I think probably the worst that they could do is one, they could do false advertising. They could create things that look like they satisfy certain properties, but actually way underperform on durability or on safety, for example. And that is bad. Probably the most evil thing that they could do is what we call the razor blade model. Basically sell you devices, but then those devices end up depending on these attachment parts that have to constantly get renewed. And it turns out that Mordor is the only vendor of those. So that's the closest that Mordor could do to really getting power over you just by being a vendor. Otherwise, it's like, even if you're very anti the Mordor and you have a Sauron must die poster on your wall and you go around wearing free serif on gold t shirts, Mordor can't really do much to hurt you. But then fast forward to 2020. In 2020, we have networks, technology, and if Mordor builds smartphones and you use a smartphone from Mordor, well, the smartphone can spy on you. If you use Internet platforms for Mordor, those platforms can censor some political viewpoints and promote other political viewpoints. It's going to tell you that, hey, yeah, this free seraph on goal thing is actually a bunch of terrorists, and nobody is allowed to support them anymore. It can affect the domestic politics of other countries. It can, at any particular moments, just flip a switch and take away the technology from any subset of its users. The amount of power that you have over users by being the producer, at least if you're building this kind of centralized network, technology has just gone way up now compared to what it was in 1970. Right. And so that's the aspect of platforms that actually. Yeah, it's one of the things that this is even reacting to. Right. It's even one of the things that the, I think crypto space is really reacting to, right? And so the goal here is to build defensive technologies that are not like that, right. Defensive technologies that do not assume that, like, they are being built in America and they are going to be good, because everyone in the world agrees that America is good. Right. Because unfortunately, this kind of consensus does not exist in the world, right. And, like, what we want to do is we want to build technologies where people can trust them, even if they have different opinions on what's good and what's bad. And these are technologies where there exist a lot of really interesting examples of already, right? So I split defensive technology into four different parts, where the first split is the split between the world of bits and the world of atoms. And in the world of atoms, you have defense against big things and defense against small things. And defense against small things is, of course, bio defense. And then in the world of bits, we have what I call cyber defense, which is like defense against things where if you look at them hard enough, it's obvious that they're attackers. And then what I call info defense. And this is like a very specific distinction that I think other people haven't quite made in the same way that basically is about defending against things where there is much less consensus about who the attackers are. Right? I. And the big example here is what we call misinformation, right? Like, people don't want to be misinformed. People wants to know the truth and not know false things. But a lot of the sort of, quote, anti misinformation ideas that have been proposed by the mainstream world, or what we might call the centralized world, they all involve there being a particular actor who understands what's right and what's wrong and basically forces that perspective across an entire ecosystem. The question is, can we build tools that actually avoid having that central point of deciding what's good and what's bad for everyone? And so in the case of the world of atoms, this is kind of somewhat easier. So for macro, for example, I talk about building resilient physical infrastructure. Even the facts that we have solar panels, and the fact that we have such amazing batteries now is amazingly good. And if every household had those kinds of things, then the amount of disruption that would happen to people's lives, even as a result of cyber war or even regular war, would already be significantly lower. If we had much more distributed agriculture, then that would improve things even more. So there's things that are just like, obviously defensive without having to, like, come with an opinion attached of, like, who is the one that you're trusting to do the. To do the defending for you. And then in the bio space, like, there's vaccines, there's other kinds of prophylactics, there's things that boost your immune system. There's like, and like, I basically talk about how there's like this entire set of things that we can do that we totally are not putting enough resources into right now that could totally create a much more airborne pandemic resistance world where we have much less Covid, much less long Covid, and even much less colds and flus, and where lots of diseases would basically stop before they even start because there are zero, would end up even being less than one in this kind of world. But we just need more funding and more effort to actually make this happen. And then in the world of bits, this is where crypto stuff really starts coming in somewhat, right? So one of the things that I began this whole episode with is like asking the question, well, crypto needs to also think about some of these issues that people are really thinking a lot about in 2023, and what is the way that cryptos plays into some of those concerns. And here it is. Basically, we want to create a world that has much more digital hardness baked into it by default, and where digital attacks become much harder and digital defense becomes much easier. What's interesting about the cryptocurrency and blockchain space is that it's great at doing that without relying on a single centralized party. So, you know, we are creating financial systems that work without relying on any specific country. We are creating forms of privacy that work without relying on a central actor to hold everyone's information in custody for them. We are creating forms of accounts recovery that don't depend on, you know, Google or Twitter having everyone's master keys. And that's happening with social recovery wallets and account abstraction and ERCA 4337. We are creating zero knowledge proof technologies that let people prove that they are trustworthy without revealing any more information about themselves beyond that. So we're creating all of these really powerful tools that in a lot of cases, are substitutes for more centralized forms of trust. And one of the arguments that I make in that section is basically that one of the reasons why the Internet has become more centralized and a less free place over the last 15 years is basically because there are threats. And the easiest and laziest responses to threats that you could implement are responses that involve centralization, like require everyone to have a Google account to sign in, and that's your anti Sybil mechanism. The question is, well, how can we actually bring privacy back? How can we actually bring the ability for anons to participate in the Internet back? How can we actually let people do all of the things that they need to do without creating these mechanisms? Wherever, if you're in one of the good countries, then you're trusted, but if you're in one of the untrusted countries, then you're screwed. These are things that actually happen. I love community notes, for example, and I talked about community notes very positively, but I remember there was this one thing about it that, at least when I checked a couple of months ago, in order to join community notes, you needed to have a phone number from a trusted carrier. And I remember seeing a tweet from someone in India basically saying like, hey, guys, you just made, like, one of the major carriers in India that serves hundreds of millions of people untrusted. And, like, there's this big population that's like 5% of the world that's like, locked out of being a community notes participant. And, like, you can see how those kinds of problems just naturally come out of this centralized perspective on trust. And so if we can create better and decentralized alternatives, then this ends up really solving that kind of problem as well. That's cyber defense, basically. All of the stuff that we've been working on, in terms of creating more decentralized and more robust financial systems, in terms of creating these zero knowledge proof systems, don't let us prove that we're good guys without revealing any other information that let us prove computations. Like all of these things, they make a much more defense favoring world. And so it's amazing that our space has been accelerating these technologies so much. And so that's kind of the core of the way in which crypto fits into the DAC vision.
Speaker C: I'm getting a notion, Vitalik, of Daniel Schmachtenberger and his metacrisis concepts wherever I we have the increasing capabilities, increasing capacities of technology to do stuff, good or bad doesn't really matter, just stuff generally neutrally. And some of that stuff sometimes ends up as, like, bad outcomes or just problems that need to be solved. And the concern here is that when technology introduces new problems to society, that society just comes up with centralized solutions to that problem. Or corporations and entrepreneurs can just move quickly and solve problems before humanity can come together to coordinate on a mutually assured platform, a mutually assured defensive technology. To answer this, and I think what I'm hearing from you is that, well, certain elements of cryptography coordination via ethereum allows for a solution space to emerge that isn't merely just some large company slapping on that patch onto society at large by saying, hey, here's our solution. It seems to be like what you are illustrating here is there is a middle ground between the chaotic production of high capacity technology and just centralized companies solving that problem space. Is that a fair illustration?
Speaker A: Yeah, I think the really important piece of this is basically creating these technologies that improve the baseline defensiveness of the world, while at the same time allowing the world to remain and be even more of a pluralistic place. So avoiding the usual trap where you basically have danger all the way up until one group just takes over everything and imposes its will on everyone else.
Speaker B: So, Vitalik, what you're proposing here is maybe a philosophical framework for where. Where crypto fits. Right. And the reason I really like this is because it seems to be like, kind of a big tent, and it's something that I personally resonate with. So you're basically saying, you don't have to pick the effective accelerationists, the techno utopians version of the world, you don't have to pick the techno phobe's vision of the world and kind of the Luddite Dumerism picture. Either this is a third way, and you're calling this defensive or decentralized. The deacons stand for all sorts of different things, or democracy, or differential accelerationism. So this is DiAc, basically. And crypto fits under the defensive technology in kind of the cyber type of use case. And it feels very much like what you're advocating for is technologies that increase and enhance human freedom. And so this can also be a bulwark against maybe your AI risk scenario of a probability three that the AI brings about totalitarian technologies, and now we have this defense against it. The other thing I would say is it seems like it's a very broad tent. It's like, who can't get behind some good old fashioned defense? We're not talking about something that could destroy the world. We're just talking about regular individuals and societies being able to defend against something that can destroy the world. And I want you maybe to talk about this as a philosophical framework. Obviously, people in crypto are hearing this. I'm sure they resonate. Are you telling me that there's a way to express my beliefs about crypto and this defensive and freedom enhancing and decentralized technology? You call it DEAC. Sign me up. There's also, I think, some other camps that could listen to this and be interested. So you've got this section in your essay saying, is DEAc compatible with your existing philosophy? If you're an effective altruist, this is a rebranding of the idea. If you're a libertarian, there's something here for you. If you're a pluralist, if you're a public health advocate, I'm wondering if you talk to the specific camps here and the value that they might find in subscribing to the DIAC belief. Of course, I know you're pushing it. You're not trying to necessarily convert people. But in order to explain it, maybe, can you talk about the wins for these various camps?
Speaker A: Sure. I mean, any specific camp you want me to start with?
Speaker B: I would love you to start with the libertarians, actually, because I think we have more than a few listening, maybe.
Speaker A: Sure. I mean, I think the best way to think about this is it's a pathway to, like, basically preserve monolithic liberty going into the a much more technologically advanced 21st century. Right. And I think the challenge that diac is looking at is basically that there are lots of technologies, including technologies that are being developed by governments or by corporations that are increasingly working with governments. There was that recent a 16 z post that was talking pretty enthusiastically about american dynamism in defense, which of course convenes military tech. And basically looking at how do we create a world that through all of these changes and through all these pressures, doesn't just maneuver itself into being this incredibly centralized place where you've basically got these probably somewhere between one to four of these sort of big super states worldwide that are in full control of their tech ecosystems. And regular people basically have no option except for being stuck inside of one of these with no other real options for getting out of that equilibrium. And so we can talk about, like, the possibility that we'll have much more offensive AI in the hands of governments. You can also just look at existing trends in how the Internet is not going the way that a lot of us hoped. Right? Like the concept of Internet anonymity, for example, which is what was like a big hope of people, I think, like ten or 20 years ago. But then the Internet is obviously becoming an increasingly difficult place to actually be anonymous. And a lot of the reason why is definitely just all of these cybersecurity issues, and people just kind of naturally grappling for the centralized solutions for those problems because they're the easy ones. And diack basically tries to ask the question of, like, well, there are threats, and you can go after a threat either by hunting down all the wolves or by putting armor on the sheep. And putting armor on the sheep is philosophically much better if you can do that. The problem with hunting down all the wolves is like, well, the wolves are some of us. And, like, we have to agree on who the wolves are. And, like, there is a risk that, you know, the government's going to decide one day that you're one of the wolves. Right.
Speaker C: And also, the wolves don't want to be hunted.
Speaker A: Exactly. And if, you know, we instead say, like, let's make the world a more defense favoring place by default, then, like, that is something that is much harder to, you know, like, twist into a narrative for, like, why? Yeah, like, governments should just, like, go after all kinds of, all kinds of people that they don't like.
Speaker B: So Diac has some wins for libertarians here. How about the Solarpunks? How about the Kevin Owaki region type community who are very oriented on collective action and human coordination? Are there wins in Diac for that community?
Speaker A: Absolutely. I think solarpunk, again, is, I think, a school that values human flourishing. It values cooperation. It also values decentralization as well. I think ultimately, there is a punk in Solarpunk. It's not solar monolith. Right. And I think a lot of people in that camp are concerned about the resilience of the world going forward, our ability to survive different kinds of risks, and are probably very cognizant of the fact that all kinds of centralized actors, including both corporations and governments, can be a big problem, making a lot of those risks larger. And what DiAC basically says is, it says, like, here is a set of tools that we can use to just cut down on a lot of those risks just across the board and make the world one that is much more friendly to human flourishing without having to construct any of those kinds of monoliths. Right. So, like, if you think about the idea of, like, let's say, the bio defense side of this, right? We can basically make a world that is much more protected against diseases, natural and artificial pandemics, all kinds of things natively just by having cleaner air. And this is a much more natural solution. And it's a yemenite. You know, it's a. Yes, it's a solution to that problem that really avoids some of the downsides of things like lockdowns that we've seen, which can be justified if there's enough of a health risk in a particular situation, but which also are just kind of massive force changes in the way that people just, like, live their regular lives with their, with their families and their regular relationships and their work. I mean, and so it's something that, like, it's an approach that allows us to be in, in harmony with each other. I would also even say in harmony with nature because I think defense includes protecting the environment. Absolutely. It's an approach that leverages local communities instead of trying to put power into these kind of big super states that decide what is good and what is bad on behalf of the entire world or on behalf of much larger groups of people. It's a world that really empowers the local coordination much more. And so I think there's a lot of technology is within the DIAc umbrella, especially if you look at the kind of info defense category that we didn't go too much into, that, like really talks about improving social technology that can really make society both more defended against attacks and much more of the kind of society that has the kinds of relationships that Solarpunks would want us to have.
Speaker B: The D can also stand for democratic. And yeah, we didn't have a chance to delve into that, but, okay, so we got the libertarians, we got some wins in DAC for libertarians. We got some wins for the Solarpunks. Let's talk about the original group that we started this entire episode with, which is on the one side is someone like Mark Andreessen, who is an IAC. He's effective accelerationism, full throttle pedal to the metal. Let's just do technology. And the other side is somebody who's maybe in the EA community, the effective altruist community. Can you get, can you get Marc Andreessen on board with Diack? And can you get Eliezer Yukowski on board with Diack? Could they both agree about this one narrow subset of technology accelerationism, do you think?
Speaker A: I mean, the post got retweeted by Mark Andreessen and by AI not kill everyoneism meme.
Speaker B: Success.
Speaker A: Yeah, yeah. You know, it's pretty successful already. I mean, I think the really big piece of it here is that, I think for the IAC side, the thing that it brings that a lot of the previous philosophies don't bring is, I mean, one is there's just optimism about technology in general, but then there also is an alternate path forward. Right? So the message is not just pause. The message is like, we proceed, and here are some alternative routes for how to proceed differently. And so if you are a builder, then the perspective does not kind of frame you as being an incorrigible enemy. You can continue being a builder. And there's plenty of amazing roles for builders to do really great things within the DAC context. The final stage of DIAC being successful, if we imagine going out to the year 3000, really does look like a post singularity Kardashev type two super advanced technological society of exactly the type that IAC and transhumanist people have been dreaming about, people who are in the AI safety camp. I mean, the concept of differential technology development is something that a lot of effective altruists have actually been already been talking about. Right? So I included a link to one of those posts, but I think the thing that it adds is this kind of emphasis on a more democratic political approach. And like, this probably is one of the big areas that effective altruists do get criticized for, right? And like, sometimes the criticism is unfounded because, like, if you put the governments in charge of like, distributed public health funding, then, like, realistically it's going to be rich people, countries, governments. And like, there's, I mean, you're going to like, national governments have a huge track record of not even caring about what's going on in Africa, whereas effective altruists actually already have put a lot of money in, right? But at the same time, once effective altruism starts going away from putting money into obviously good, but we just might disagree on how good things and into kind of manipulating, like, big political objects, then you start really needing to care much more about legitimacy. And to me, I feel like both of the big effective altruist related fails, if you can call them over the last two years, where one is the OpenAI situation and the other is f two x ftx. To me, they both have to do with underrating legitimacy. SPF was clearly, he had a. All kinds of massive problems, but one of them is definitely that he just overrated the extent to which he could become a massively negative value actor just by delegitimizing the ideas that he deeply cared about. And then on the OpenAI side, basically what we saw was we saw a seemingly earnest and well intentioned effort to kind of create kind of clamps on the OpenAI effort that could try to kind of reduce its potential to become super harmful by creating this board that could push things in the other direction. But the problem is, like, it tried to do all of this through a completely undemocratic and unaccountable board of five people that saw no need to even try to explain its actions to the wider public, right? And then what happened was like, well, it fired Sam Altman and then basically within three days, well, it was like, in some ways, a pretty unprecedented political fail, because what happened was the employees of the company, who are probably capitalist, libertarian leaning, just like, tech software types in a lot of cases, formed an impromptu union to side with a billionaire CEO against the board. Right. Like, that's like a pretty big fail if you think about it that way, right? Like, congrats. You got software engineers to unionize, except they're standing behind the billionaire. Right. And, you know, if. And so I think the thing that, like, Deac ideas can really bring here is basically bringing back some of these concerns about legitimacy and, like, understanding that, you know, you're not just spending money points, but you're also spending social capital points. And, like, you really need to take that seriously and, like, bringing that in in a way that's not just sort of an adjunct, but that is a really core part of the philosophy. Right. Like, it's a core part of the diag philosophy that we are trying to create a world that is more defense favoring from anyone's perspective, regardless of whether or not you agree with any specific actor that is going to be, that would enforce its own idea of who the wolves are and who the wolves are not.
Speaker B: Vitalia, the last group to ask you about here on compatible philosophy that's near and dear to our hearts is, how about the crypto tribes? We have bitcoiners, we have ethereans. We have people who are into Solana and cosmos, and there's a lot of tribalism. Do you think something like Deac, we can all stack hands on something like Deac and say, hey, these are a common set of core values, defensive decentralization technology that we all agree on. Yeah, we have our differences with respect to implementation, but can we all unite behind something like Deac? Is it that wide of a tent to bring the crypto tribes together?
Speaker A: I think it absolutely could be. And I think this is one of those places where I think it's. It's good to give a positive shout out to some of the positive aspects of the bitcoin community, which is that there definitely is a strong sub community in there that cares about non blockchain decentralization tech. There's bitcoiners who really support things like Noster, there's bitcoiners who support Tor, and things like Internet freedom tech. There's bitcoiners who have supported more secure operating systems. There's. And then there's Ethereum people who have also supported all kinds of things in each of those categories as well. Right. And so I think the idea of viewing the blockchain world as being one part of this somewhat larger thing, which is a decentralization favoring vision of cybersecurity and then seeing that itself as being one picture in a broader vision of decentralization friendly, pushing the offense defense balance strongly toward defense is something that Ethereum people and bitcoin people, and Solana people as well, can absolutely get behind.
Speaker C: Vitalik I'd like to throw a different candidate for what the D means for in Diac mine might be directional acceleration.
Speaker B: I thought you were going to say David.
Speaker A: I was going to say that, too.
Speaker C: That can be a different topic with the Dave Dow. So to me, there's the tribal debates between decelerationism and accelerationism, or effective altruism and accelerationism. And when there is these tribal debates, usually the weaknesses inside of one tribe never really get addressed by that tribe because they only ever really argue in relation to a different tribe. The way I hear this blog post and hear you speaking about is like, hey, we're going to move forward in scientific progress. We're going to have technology that has higher capacities. And now it's really just a matter of picking our direction, our priorities in where we want to go and what technologies we want to prioritize first. Because I think generally most people accept that technology has helped the world over the grand arc of time, and now it's really more about choosing which direction we go in, rather than blindly saying, yes, it's forward. It's more about saying, yes, forward. And over here in this particular direction, how would you feel about directional accelerationism?
Speaker A: Yeah, I mean, I think with the caveat that it's totally possible to create a directional acceleration story that I totally disagree with. But, you know, as long as we.
Speaker C: Still have to debate in which direction.
Speaker A: Exactly. As long as we understand which direction is. I mean, you think? Yes, absolutely.
Speaker B: Vitalik, as we maybe conclude this episode, you've given us a fantastic tour of this whole debate, the societal debate in this context, in crypto, and a great definition of Diac, which is, I think, just a fantastic philosophy that I think bankless listeners will probably take some time to mull overdose. But your article concludes with this. It includes with some optimism for human potential here you say human beings are deeply good. And I got to confess, in my darkest moments, of course, I sometimes doubt whether that is actually true. And if you look at even crypto in 2022, I feel like we all came out of that collectively as an industry, pretty beat up, pretty doubtful that human beings are deeply good. But you say this I love technology because technology expands human potential. We are the brightest star. 10,000 years ago, we could build some hand tools, change which plants grow on a small patch of land, and build basic houses. Today, we can build 800 meters tall towers, store the entirety of recorded human knowledge in a device that we can hold in our hands, communicate instantly across the globe, double our lifespan, and live happy and fulfilling lives without fear of our best friends regularly dropping dead of disease, zooming out. We have come quite a ways, haven't we? What grounds your belief that human beings are deeply good? Vitalik.
Speaker A: I mean, I think just like, what other thing even remotely compares to us, right? This is the question to ask, right? You know, the universe is a very lifeless and unforgiving place, right? Where we've had in like 1st 9 billion or seven to 9 billion years of the just stars and planets crashing into each other and randomly creating supernovas that would just completely wipe away everything within light years without thinking about it. And then we had 4 billion years of life. But life that was very nasty, very brutish, very short, and that basically involved predators constantly running around and eating prey, and everyone being on the brink of the dropping dead of disease and starvation. And, you know, there is not a single example of a cat that modifies its eating behavior because of a principled stand that killing mice is wrong, right? Like, that is just not a thing that happens. Whereas with humans, there are plenty of humans that have written entire screens on why this is the case and that have made huge personal sacrifices to protect the people or animals or plants that they care about. And I think to the extent that this happens, it's incredibly amazing and incredibly beautiful. And I think if humanity continues on a positive trajectory, then the amount of good that we can do just multiplies even further exponentially from there. Right? Like in the 21st century, I think there is a big chance that we're finally going to turn the corner on factory farmed animals. And probably the biggest moral catastrophe that you still can blame humans for is something that we will actually end up moving beyond. And then 1 billion years from now, the sun is scheduled to get so bright that life on earth is not going to be possible anymore, right? And does the sun think about the moral consequences of this act that it's going to make? Well, no, it does nothing. But humans, well, what can we do? Well, you know, we can sprinkle calcium carbonate or sulfur into the air and compensate and reduce the amount of light that reaches the surface. We can build giant mirrors in space to reflect the light. We can go and terraform Mars, we can do all kinds of things. And so if, you know, the beauty of earthly light, life is still going to shine 2 billion years from now, it will be because of us, right? And so I think we carry the torch of this just enormous potential that is unparalleled in any other thing in the universe that we currently have evidence of the yet existence of. And it's our job to do a good job of carrying that torch and carry the torch forward.
Speaker B: What a fantastic way to end it. I think that's what the DIAC movement it sounds like is all about. And I could say bankless is certainly part of that movement and I'm hopeful with crypto we can help carry that torch further. So I'll leave maybe bankless listeners with this line from your article. We are the brightest star. There's a lot of good that can come from ongoing human progress into the stars and beyond, but there are big forks in the road and we need to choose carefully, accelerate, but accelerate carefully and well. Vitalik Bueniran, thank you so much for joining us today. Some action items for you bankless nation. Link to the tweet thread that we discussed today. My techno optimism. That's a Vitalik's article. We'll include a link to that and also the article itself. Risks and disclaimers, of course. Crypto is risky. So is philosophy, so is technology. So many of the things we've talked about. Yeah, I think so. You could definitely lose what you put in. But we are headed west. This is the frontier. It's not for everyone. But we're glad you're with us on the bankless journey. Thanks a lot.
