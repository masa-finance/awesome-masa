Speaker A: Welcome to Bankless, where we explore the frontier of Internet money and Internet finance. And today on this episode of our Zoozalu series, we are exploring some new frontiers. New frontiers and new technologies, all of which are poised to completely revolutionize the world and change everything about the operating system that society is currently running. On this episode of our Zuzalu series, we're exploring the frontier of cryptography, which is maybe not as new of a frontier as some of the other ones that we've explored yet. Nonetheless, the cryptography enabled future is poised to change the landscape as all the other technologies that we've talked about. I will say that the ZK week at Zuzalu was one of the weeks that I attended the fewest talks and workshops on, because, I mean, come on, what am I gonna do there? Which is why I pulled in a very familiar, friendly face, Justin Drake. To summarize, the entire ZK week at Zuzalu in a 45 minutes episode. Turns out there's this cool new frontier of crypto called Nova. Nova is Ek Nova, which has to do with folding numbers recursively to make cryptography harder. I don't know how to explain it, but that's what Justin is for. But first, before we get to the familiar territory of Justin Drake, we're going to talk to Daniel Shore, who's working at a startup in the ZKML landscape, which has gotten a ton of hype and attention lately. And if you listen to the interview with Daniel, you'll understand why. The thesis is that there's going to be a cambrian explosion of AI models out there. And simply verifying the model itself on chain using Ethereum and a ZK proof can give consumers and users of these models assurances of the authenticity and the outputs of that model. The fact that the input actually create went through the correct model and the output is actually verified by the model that you want. Why this is important and what this unlocks, Daniel will explain in the show bankless nation. This one is a doozy, but Daniel and Justin do a great job of dumbing it down for us in this episode. So let's go ahead and get right into it. But first, a moment to talk about some of these fantastic sponsors that make the show possible. Bankless nation. It is ZK week here at Zuzalu, and I am talking to Daniel from Modulus Labs. Daniel, welcome to the show.
Speaker B: Thank you for having me.
Speaker A: Okay, so there is growing hype and attention around this world of ZKML, and so we're getting some hype y adjectives, not adjectives. Hyperdeze.
Speaker B: Letters. Consonants.
Speaker A: Yes, exactly. Letters. Yeah. Can you explain the world of ZK and ML to the best of your ability and as short as possible? And then we'll get to why these things are currently getting married in this point in time.
Speaker B: Yeah, I think, I mean, let's just start with the letters. ZK is the first 20. Knowledge is often known as an accountability or integrity technology. So it basically tells you that some compute was done correctly. So that's exciting. And actually has this odd property where verifying that compute is a lot less expensive than doing the compute naively. So, it's often been used to compress information, and then in the case of blockchain, bring that information on chain while retaining the security standard.
Speaker A: The metaphor we like to use a lot is that it's hard to complete a sudoku puzzle. But once a sudoku puzzle is completed, it is easy to verify that it was done correctly.
Speaker B: Exactly.
Speaker A: And that's just like ZK is essence.
Speaker B: Yes, yes, precisely. That's kind of the most powerful property of Zkhdem in the blockchain context. Kind of. What's next then, at least for us, was, okay, excuse me. If this technology can be used to scale blockchains, what is the most kind of intense or almost irresponsible kind of compute we can throw at this kind of technology? And in terms of the size of compute, it doesn't get much larger than machine learning models.
Speaker A: Irresponsible is just in like the magnitude of the compute.
Speaker B: Yes, exactly.
Speaker A: So you're stress testing the whole paradigm of ZK.
Speaker B: That's right.
Speaker A: And that's how you got to ML.
Speaker B: Mm hmm. That's right.
Speaker A: Why is. Okay, what is ML, and why is it so intensive?
Speaker B: Yes. So machine learning, or just artificial intelligence in general, is a process of using an algorithm to approximate human like decision making. So think, like, high semantic output space. Right. So I want to look at an image and decide if it's a cat or a dog or I want to kind of predict what might happen to prices in the future by looking at a lot of data from before. Traditionally, this has been seen as kind of a task for human beings, but ML is this kind of wild regime where algorithms can take the place of human decision making.
Speaker A: And specifically these algorithms, there's a software and a hardware component. Right. And the hardware is kind of the clock speed, how fast this thing can think. And you can throw a lot of hardware at this thing, and that will get the result faster. But the point is, is that blockchains, ethereum, is not something that you go to to do a lot of compute quickly because that is what gas is. That is what gas fees are.
Speaker B: That's right.
Speaker A: Okay, so like where does this intersection occur at? If you said that you want to have this irresponsible use of a ZK proof and what that means is just like throwing a lot of compute at this thing. That's fun. Why is this a real utility? Why is this actually useful?
Speaker B: Well, kind of the magic is, I guess the security of the blockchain, right, and the ability to bring compute of any size, but especially in this case really large compute up to that same security so that you can ingest AI decision making into your smart contracts or into your dapps and on chain services that we think is a really powerful paradigm and something that's now uniquely enabled by the fact that ZK has improved so much, and especially, at least in the case of modulus, with a focus towards machine learning, compute.
Speaker A: Okay, so apps can use AI. Can you just unpack that a little bit?
Speaker B: For sure.
Speaker A: It's a very general statement. Can you make it a little bit more just defined and illustrative?
Speaker B: Of course. Let's take some kind of defi service which has liquidity pools. Perhaps they want to rebalance these pools using a really advanced algorithm, maybe something even akin to an AI algorithm. Currently, all that compute needs to happen off chain because it's just too expensive to run on chain. But with Zkai or ZKML, you can imagine that off chain compute getting that Zhay's seal of approval. It's like, ok, this compute has been pre committed, bless you, of course. And using zero knowledge, we can prove that that compute was done correctly, thereby upgrading it up to the security standard of the chain. And it's as though that entire process, in this case of rebalancing the pool ran naively just on chain.
Speaker A: Why do you need to verify? Why do you need to verify it? Why can't you just run it without actually having to verify it?
Speaker B: Yeah, and I guess I'll start by saying a lot of services currently do just run compute off chain. It's certainly a lot less expensive, but I think a big part of why we're in the blockchain ecosystem at all, and certainly a big part of what attracts Modulus is the security standards that's established by this decentralized network of nodes and having the social consensus and all the wonderful things we get in Ethereum and some other chains as well. And so we want to make sure that the services that write on these chains that buy into the security ethos of the chain gets to participate in that fully, even as they take on bigger statements of work when it comes to compute. So if I'm a dapp or I'm an on chain service, and I want to use AI, maybe I don't have to give up on the security that my customer, and maybe even myself, I expect out of my own service.
Speaker A: Okay, so I think the reason why this whole area of focus is getting so much attention, ML machine learning chat, GPT has just elevated that into the stratosphere. ZK on the crypto side of things, the theme of ZK week was how fast and compressed and capable some of these ZK circuits are becoming. And so on one side of things we have the growing cambrian explosion of AIH getting large in capability. And then on the other side, we have ZK decreasing, which was what ZK does, decreasing complexity and simplifying compute.
Speaker B: Yes.
Speaker A: And so, like at a high level, at just like broad stroke, you can see how these things kind of would work together. I'm guessing, without knowing too much about this kind of naively, that the idea of putting an AI on chain implies that there's going to be tons of AI models, like generalizable, any kind of model that you can think of that an AI would be able to produce for, I don't know, DeFi liquidity comes to mind, but like, literally anything.
Speaker C: Sure.
Speaker A: And then the idea is that with ZKML, you get to verify the actual model, the external model that I'm assuming many, many people are going to work on, like individual models. And we want to make sure that that is the actual model that we are running on chain. And that's the ZK component.
Speaker B: Yeah, you got it. I mean, this is, I mean, sure it's Defi and certain market mechanisms, potentially, or maybe moving users funds, of course, or crypto across different chains to optimize yield farming, or even nfts. Right? Let's say I want a machine algorithm to output pixel art or any type of generative art, but I want to know that that piece of art actually came from the algorithm that might be really valuable. So ZKML is a way of extending that kind of cryptographic promise of authenticity to all kinds of AI output specifically. So I guess the possibilities are substantial, so long as we can get the tech right. And obviously all the acceleration on the ZK side really helps with that.
Speaker A: So I'm just going to list off a bunch of models that I can think of. We got chat. GPT is the big one, GPT four mid journey. What are some other ones that are out there? We got bard. There's all of these models, older models.
Speaker B: Like Bert recommender models, generative models that output pixowr gans, generative adversarial networks, as well as maybe more subtle models, like anything from a game that uses an agent, an AI agent to simulate NPC's to something more kind of outlandish like an LLM that maybe predicts investment decisions or things like that.
Speaker A: So perhaps one model is, in the gaming world is something that is generatively producing a landscape.
Speaker B: Sure. Yeah.
Speaker A: That is a model, yes. And maybe this applies in the world for people that are familiar with dark forest.
Speaker B: Sure, totally.
Speaker A: We need to make sure that we're playing a game that has a specific model to define the landscape of the game that we are playing.
Speaker B: Exactly.
Speaker A: This is an example of a model.
Speaker B: Yeah, that's great. The counterparty risk here is bigger than just swapping out the models. It's swapping out the output entirely. So let's say you have an AI responsible for the landscape or the weather or the in game economy, or it's like almost like a God role in your game. Right. If I, as the operator of the game or the developer is biased to, let's say, penalize David's camp or David's planet. Right. We're going to nuke your planet from orbit via the decree of the galactic government. And there might be enormous financial stakes to these in game economies. That's like a devastating result. And so in the same way that.
Speaker A: It'S a rug pull.
Speaker B: It's a rug pull, yes. And so in the same way that blockchains kind of extend this veil of security or this promise of security rather, to everything that's on chain, we want to make sure that keeps being the case even as we want to bring AI kind of feature sets on chain.
Speaker A: Right. Okay. And so, like, just going further, like we got the AI generative landscape and you also have like the models of AI NPC's.
Speaker B: Sure.
Speaker A: And like, I could just think of any. This is kind of fun to think of, like what AI models could become. But I think that's kind of the point is that it's so generalizable. Right. And so AI's. We're about to get into AI week. Lots of AI talks happening right now. There's a talk of generalizable AI, like artificial general intelligence. But then there's also the topic of like narrow AI. These are both models. Right? Like Alphazero is a narrow AI about chess. And then we have more generalizable AI's that we think are coming, or word are coming, also a model. And so the landscape of models, which is one of those words that you don't really understand until you see how generalized it is. The growing world of AI is generalized models for stuff. And then the ZK element allows it to take the soul of that model, of a particular model, and place it on chain to become an on chain resource.
Speaker B: That's right.
Speaker A: For the rest of the blockchain to ecosystem to use.
Speaker B: Exactly. And it has all those properties that we love so much about things being on chain, including, it's composable, it's obviously high security. It's in some sense, really attestable and referenceable, all these things that makes the chain wonderful environment. You imbued that AI compute, which previously was just in a black box somewhere.
Speaker A: And so I'm assuming, yeah, the composable part gets really, really cool, because then you can create some sort of system that creates a world of models. And I don't know where is the utility coming from first. It's exciting to see all this possibility. What's the lowest hanging fruit? Where it's going to get built first?
Speaker B: Yeah, great question. There's already prototypes, including ones that we've built, and others as well, exploring. It's still a very nascent category, everything from. You mentioned this briefly, but chess engines, we brought a formula in parameter chess engine on chain, and now players can stake and bet against the chess engine, knowing full well that they're always playing against the same AI model. And in no way can modulus or anyone else swap out the results of this particular chess engine by calling up our friend Magnus and saying, hey, Magnus, what move would you play? Yes, but that's just where we're starting. We're also starting a ZK gan project, so generative art, and of course, we're kind of marching towards that LLM goal, but there's a lot of space in between that. Of course. Again, I mentioned recommenders earlier, there's a lot of use cases for just any amount of personalization when it comes to the chain, but still at the kind of security standards we want. If you have a social media feed, but you want to know that the same algorithm that drives the equitable results you see on Daniel's feed is the same one that David sees, then ZKML can play a really, really key role there.
Speaker A: So, okay, at Modulus Labs, what's the current, what are you guys focusing on right now? What's the current bottleneck or constraint or problem that you guys are currently solving. Where are you in the roadmap, I guess, is the question.
Speaker B: Yeah, great question. So you kind of alluded earlier in the ZK week. We're seeing kind of the ZK overhead come down substantially. And that is amazing and obviously very helpful precisely because at the top we mentioned that AI is like a lot of compute. It's almost irresponsible. I think the exciting here is that our job's almost easier because AI as a class of compute is really structured, really repetitive. And those things, those properties of that compute allows us to make the ZK stack much more efficient in some sense, almost like you've down selected to a more specialized class of problems that gives your ZK prover a lot of space to be more efficient and kind of take advantage of that structure. And so modulus kind of what we work on for the most part is making the proving stack significantly more efficient so that we can bring much larger, much more expressive models on chain at still that same security standard.
Speaker A: Okay, so this is what your guys technology is. You guys aren't bothering with the AI world. You're not here to build AI models. That's for the AI industry.
Speaker B: That's right. Yes.
Speaker A: You're here just to build the bridge to be able to verify models on chain.
Speaker B: Exactly.
Speaker A: So you guys are operating in the ZK world.
Speaker B: Precisely. Precisely. You got it? Yeah.
Speaker A: Okay, sounds ambitious.
Speaker B: We do our best.
Speaker A: Yeah. How many of their, how big is modulus labs?
Speaker B: Modulus Labs is currently a very proud four people.
Speaker A: And when did you guys get started?
Speaker B: About seven months ago.
Speaker A: Okay.
Speaker B: Yeah.
Speaker A: Where did the original inspiration come from? Was there like an aha moment or how did the team come together and what was the motivation?
Speaker B: Yeah, I mean, this is gonna sound a little ridiculous, but of course there was all this kind of excitement around stable diffusion and generative models. And around the same time we fell in love with ZK and we started asking ourselves the question of, hey, what would we want to build here? And the background of the team is all AI researchers from Stanford. And so it was almost an obvious thing we asked ourselves, oh, how silly would it be if we put a very non performing AI model on chain to, I don't know, let's say, predict ETH prices? So that's what we did. We built the world's first on chain AI project as a joke. We hacked it over or hacked it together over a week, and it started predicting prices of ETH and making trades on l one with a uniswap contract. It's a joke, right? We did this purely as a proof of concept.
Speaker A: The point was not to make money on ETH. I don't think the goal was to make money. That wasn't the goal.
Speaker B: No, no, no. It was an idea, and this model would not be able to do that. To clarify, but kind of, and I guess just to really nail that point home, we didn't put a call function in the smart contract. So, like, once money went in, we put in like $500, right? I. No one could touch it, us included. But kind of something miraculous happened, which is lots of randos on the Internet, anons included, started donating money to the trading bot. And if you look at the kind of historic performance, there's like a front end. It kind of goes up into the right. It's not actually doing. That's just, like, buoyed up by the donations it was getting. And of course, eventually it lost all, everyone's money. As we've been telling very kind of.
Speaker A: Transparently, their algorithm failed to produce more ether.
Speaker B: It did, yes.
Speaker A: But it did succeed at being an algorithm on chain.
Speaker B: Exactly. Which was the point. Precisely. And there was nothing that modulus team could do to, again, tamper with anything that it's kind of like, imagine an autonomous robot just executing forever. So from that point, we were like, oh, man, what if we brought an actually performance model on chain? Imagine how cool that would be.
Speaker A: Okay, so there's two worlds that I see spawning here. There is the insular world of crypto, who's like, oh, we could build models to do things inside of the crypto world. And then there's external uses that need to verify models and their execution off chain. Can you talk about how these two worlds might develop independently?
Speaker B: Yeah, exceptional kind of insight. You're totally right. And that's kind of how we see it as well. In some sense, the crypto world is a little more convenient because there's already kind of a cultural expectation of compute integrity as really core value. Right. We want to see that as much of the compute that's related to our Dapps and on chain services are on chain as much as possible. Right. This is kind of the excitement that's driving all the ZK rollup kind of activity. But of course, the question is, what happens when you step beyond the on chain world? Does the rest of the world care about verifiability that this algorithm was the one that made that decision? Right. And you can imagine in a future where the judicial system uses large language models to make decisions about sentences, God forbid, or a medical system that uses a very sophisticated model that determines certain medication treatments. These kind of intersections are really sensitive, where liability is a big concern. I think verifiability is going to be a big deal. And what's cool is we get to get that flywheel started, the cultural appetite for it in crypto, and really kind of hopefully build up a really strong, kind of almost a strong example of what it's like to be able to attest and make our algorithms accountable, and then we can communicate that with the rest of the world. But Modulus, right now, we're very much focused on crypto with kind of an eye to the future.
Speaker A: Sure, sure. Yeah. But knowing what the eventual tam would be, especially as the tam is likely going to grow as AI grows. Right. So the mental model I have is like, that little, like, SSL certificate, like, shield in your. Yes, it's kind of like that. It is proved. This model that you're using is proved.
Speaker B: Exactly.
Speaker A: And, you know, thumbs up. Go for it.
Speaker B: That's right.
Speaker A: Rug pull resistant.
Speaker B: Exactly.
Speaker A: Or a rug pull immune. I would imagine that, like, as all of the AI people that are over there talking about AI doom, they're talking about, like, one of the paths towards AI doom. For first, before doom, we get to AI fun times. So in the AI fun times, an explosion of models, an explosion of usefulness, an explosion of human productivity and flourishing and wealth generation, precursoring the inevitable blow up. But before we get there, it's a world that we, like live. The humans live on models.
Speaker B: Yes.
Speaker A: And our life is guided by models and determined by models. And so with that cambrian explosion of models, I would assume that the surface area for rug pulls also. Gross.
Speaker B: Yeah, it's actually kind of terrifying how big the attack vector can be for generating catastrophic results. In your AI models, there's a kind of classic example of a vision model, which sees a stop sign and you go in, or a picture of a stop sign, say, and you go and you put three pieces of tape or manipulate some pixels in a way that's totally indecipherable to the human eye, and it thinks the stop sign is a go sign or, God forbid, is a toaster, anything. These AI models do have very substantial adversarial environment or attack vectors, and it's a little scary, for sure.
Speaker A: Okay. So we're helping secure our future, which sounds pretty important. And also, it's like making it. It's just, like, trustless. There's many different aspects of AI, and this is one way to make AI applications safer. It's not the AI safety conversation, but it is part of it.
Speaker B: Yeah. I mean, the way I like to think about it is you have all kinds of different models and your model might be more explainable, more robust to attacks, more equitable. It doesn't bias for specific political allegiance or anything else. And these are amazing attributes and very hard problems that people are actively working on. But without verifiability, without the ability to pin down that model at any given instance of use, you can swap out that fair model, that robust model for a different one or for no model at all. I can just be feeding it any answers I want as the operator. And so in the same way that the security center of blockchain is that it's all there. Right? Like you can just go into the ledger and see exactly the transactions. We make sure that the algorithms have that same kind of quality.
Speaker A: Right. And so the insular way of using this technology is that we get smart contracts that are AI's that get to do things on chain, and that's going to be pretty cool and tight. And I don't even know where to think about how to start thinking about that. But the outside world is at that point just using the blockchain as like a timestamping tool, correct?
Speaker B: Yes. Yeah, yeah. As in the blockchain is kind of this amazing environment where public verifiability is so obvious there, right? And so it could be this amazing kind of settlement arena for the world's compute, right?
Speaker A: Yeah, yeah. And in a world in which we are probably are going to be using AI models and we're probably not going to think about the rug pull surface area, right? And so like humans, when we use these things, we're going to assume that they're the things that we want them to be. And so we're not going to be looking for the rug pull. And this actually, this technology actually allows us to be cozy as we use these models. Yeah.
Speaker B: In some sense. And it's a little insidious. These models are very sticky. They're incredible, right? They're very personable, magical kind of features that we can add into every part of our compute diet as a society. And of course, while that's happening, we're very quickly expanding the surface area of potential attacks. And the goal is to make sure that before we have that catastrophic outcome, before somebody gets really injured, where a lot of money is lost because of the widespread use of large AI models, that we have that accountability piece in place along with all the other AI safety technologies.
Speaker A: Daniel, I would imagine that this conversation, just about ZKML, what we're talking about here, can go on and on and on and on and on.
Speaker B: I definitely can.
Speaker A: Are there any big parts of the conversation that I haven't opened up yet?
Speaker B: Good question. I mean, every part seems like it's filled with potential, right? But something that we spend a lot of time on, for example, is, and this is going to sound like the opposite of the kind of aspirational, exciting thing that's happening, is a literal cost of doing this process of running these computes or this very expensive class of compute in a zero knowledge setting and making sure that although we're excited to have our heads in the sky, we're marching towards real implementation, real use cases, real customers. So it starts with working with folks like Worldcoin on identity verification or self custody of biometric information, all the way to games and defi protocols and of course nfts as well, to push the envelope on accountability for machine intelligence. You know, we have this kind of bigger thesis, but at least for modulus, and I think the category in general, this nascent category, we want to make sure we march to the beat of kind of real impact, right? Making sure that it's actually making a difference in the ultimate lives of these service providers.
Speaker A: The phrase accountability for machine models, I think, is going to be something that really resonates with a lot of people, even at just like the cursory level. Right. AI is going to and has triggered just like a lot of people's, just like the hairs on the back of their neck. Sure. And so just as a branding, hey, we're helping AI be safe, is like a really good branding to lean into.
Speaker B: Well, kind of what's exciting, of course, is beyond just the branding being kind of very appealing for sure, is that this is, you know, AI researchers are not going to love kind of the way I phrase this, but I almost see these technologies with personalities, right? AI is this very expressive, creative, infinite potential, very powerful. But cryptography is very humble, it's very discreet. It says, this is the statement that I can show with pure mathematics, and this is the claim that I'm able to make and no more. And so being able to marry these two things, which have quite a bit of tension by their nature, is something that's deeply exciting for certainly me and I think the whole modulus team.
Speaker A: Yeah, you use that word expressive, which is one of my favorite words. We have all of this explosion of AI models that have all of this power and personality that you're saying. And I think maybe adding in that ZK circuit component also adds in just a stamp of authenticity.
Speaker B: Yes, I see. Yeah, it's like. I mean, you mentioned the little check mark, or it's like a Twitter verify check mark. Maybe back in the day when that was more substantial socially, but having something like that for your models, for your AI models, or for any mechanism that is sophisticated, compute. Precisely.
Speaker A: Daniel, I've learned quite a lot. Where should listeners go if they want to continue going down this knowledge rabbit hole?
Speaker B: Yeah, I mean, not to show our own stuff too much, but modulus labs, we're on Twitter. We try to put out decent content. And of course, I watched your guys.
Speaker A: First video on the scroll, YouTube. And, man, that broke my brain.
Speaker B: Oh, my goodness. Yeah, yeah. There's all parts of the stack to enter. Very technical to kind of more philosophical. But we want to make sure that we meet people where they are because it's really cool. And we want as many people to be in the know about this stuff and kind of be part of this movement. For your words now, accountable machine intelligence. Love it.
Speaker A: Love it. Daniel, thank you so much.
Speaker B: Thank you, David. Really appreciate it. Yes.
Speaker A: Thankless nation. We are here at Zuzalu, and I'm talking with our good friend Justin Drake. What's up, Justin? How's it going?
Speaker C: Yeah, all good. Thanks for having me again, David.
Speaker A: So, this is ek week. Zero knowledge week. This is. I've hopped into two talks, trying to understand what's going on. I stepped in to the talk, and it started to break my brain, and I did not. Nothing went in. And so, as someone who is both a cryptographer who's on the frontier of cryptography, yet also understands how to explain these things to a more general public, I'm hoping you can kind of help me understand what the hell is going on this weekend.
Speaker C: Right. So I actually took a bit of a break from crypto. A one year break, more. So more or less going into Mev. But, you know, I've been getting back into things in the last few weeks.
Speaker A: Oh, by crypto, you mean cryptography?
Speaker C: By crypto, yeah, I mean cryptography, yes. And it turns out there's been a ton of progress around this thing called Nova. Now, Nova is kind of this idea which started in two years ago, in 2021, by a paper written by three Naf Sati. And basically, he came up with this prover optimization for snarks. So, one of the big bottlenecks in snarks is the ability to prove very, very big statements with small amount of computational resources and also with low latency.
Speaker A: And just snark is just like a compression technology. It's a ZK.
Speaker C: It's exactly. It's a ZK proof. And the thing that we're very, very good at is getting the ZK proofs be extremely small and extremely easy and fast to verify. So that's on the verifier side of things. And a lot of the work now is on the prover side of things. We want to generate these proofs in the first place, and we can do proof for simple statements like in the context of zcash, I'm making a valid payment. But we want to be doing things like ZK rollups. And here we have massive statements where I want to prove that a whole Ethereum block is valid. And a lot of the complication from an engineering perspective stems from the prover side of things.
Speaker A: Okay, so just to make sure I'm with you, we have the technology to take a bunch of data and compress it into a really small packet of data.
Speaker C: Yes.
Speaker A: We don't have the technology to do that fast and cheap and quickly. Right, right.
Speaker C: Well, maybe now we do, or at least we're getting there.
Speaker A: That's what we're talking about. That's what's being talked about here.
Speaker C: Exactly.
Speaker A: That's the cool frontier that we're on.
Speaker C: Yes. So there's a whole class of snarks that are based on so called curves or elliptic curves. And the Nova techniques are on the order of ten x faster than the previous proof techniques.
Speaker A: Nova is the cool new thing that.
Speaker C: People are discussing, is the cool new thing. Yes. And the generic term is called folding. And the reason is what you do is that you're going to take structured computation. So instead of taking this huge unstructured statement, you're going to take very, very structured statements and try and fold the various steps into each other. And this folding process is much, much cheaper than snocking. Now, what do I mean by structure? Imagine that you have a cpu, and every cycle of the cpu, cpu's run, for example, at 3 instructions per second. Every cycle is this structured thing that can be folded into the next step. And then ultimately, by structure, do you.
Speaker A: Mean serial linear, step by step by step by step, yeah.
Speaker C: So what I mean is that you want to prove a very big statement that can be broken down into steps, all of which have the same format or template, so they still all have the same length and they're all proving something similar. Okay, here's an example. BLS signatures so, we all know that BLS signatures is one of the big optimizations that allows Ethereum to have half a million validators.
Speaker A: Right. The next is the thing that brought the validator requirement from 1500 ether per validator down to 32, because it was a very compressed piece of a previous technology.
Speaker C: Exactly. And what does BLS aggregation allow? It basically allows you to take, let's say, 10,000 BLS signatures and fold them onto each other such that if you verify the folded thing, you've proven that all the other 10,000 unfolded signatures are also valid. And just to put numbers on this, it takes about one millisecond to verify a signature. So if you want to verify all 10,000, it will take 10 seconds. But if you want to verify the folded one, that takes only order of one millisecond.
Speaker A: Okay, so some of the detail, the deep down details, I don't get, but the patterns, I think I'm understanding. Whereas if you tell me it takes one millisecond to verify a BLS signature, and then if you want to do 10,000 of them, well, it's a linear, it's linear up to 10,000.
Speaker C: It's linear, yes.
Speaker A: And with this new folding mechanism, like, there's that meme of this, like, you can't fold anything in the universe twelve times because it's too exponential.
Speaker C: Right, right.
Speaker A: And so I understand that. I understand exponential curves. And so it sounds like we have a way to do something previously linearly, that we now have something, a new way to do it. And it's exponential. And so it just gets that economies of. It gets the scale of an exponential curve. Is that a way to understand it?
Speaker C: So we basically have these two tools at this position you can think of, like the high duty power machine that is very expensive to use, and we have the hammer. Both of them can get the job done, but one of them is much, much cheaper to use, and the hammer turns out to be about ten times cheaper to use. So it is a constant optimization. We do know how to make snarks that are linear time, in the sense that the time it takes to do the proving grows linearly with the size of the statement. But now we've actually reached a point in the maturity of snarks where it's a game of constants. And at least for elliptic curve based snarks, Nova allows us to get this ten x boost relative to the status quo.
Speaker A: So what does that mean for crypto cryptocurrency? What does that mean? Why do we care about this, how does this really benefit? We enjoy things that are cheaper and faster just implicitly. But how does this impact our lives?
Speaker C: Right. So I guess one big thing is it's going to make it easier to deploy rollups en masse. ZK rollups specifically.
Speaker A: So this is a democratizing technology for deploying CK rollups.
Speaker C: Yes, but it's also the key to getting decentralized proving. Now, what do I mean by decentralized proving? I mean two things. I mean, first of all, the lowering the barrier to entry in terms of computational resources. So right now, if you want to be a prover for a ZK rollup, you need to hire some sort of rack in a data center, lots of compute, and it's not really friendly to doing so at home. So imagine, you know, a small box at your home and you can be a scroll prover or whatever it is, Zksync prover. The other interesting thing about Nova is that it allows for decentralized proving, which is kind of the next step after distributed proving, which is the next step after parallel proving. So let me try and explain. So, snarks are very parallel friendly in the sense that if you have multiple cpu's on your machine, or if you have multiple threads within each cpu, you can make use of these threads to do work in parallel, but they all sit within one machine that's parallel proving. Then the next step is distributed proofing, where you have machines that are geographically distributed all around the world and they're separated by the networking layer. And here what you need is basically these small proofs that can be communicated fast so that you can distribute the work.
Speaker A: And this is like distributing the sequencer, distributing the validator of a layer. Two, similar conversation.
Speaker C: Yes. Well, here is about distributing the prover. And the key thing about distribution versus decentralization is that there's only one prover that's distributed, whereas in the decentralized model, it's a untrusted kind of coordination of provers that ultimately help build this mega proof.
Speaker A: Right. And the idea of a decentralized ZK roll up is that ultimately we want anyone to be able to generate a proof of. Yes, but right now that's still too costly, too expensive, because just that's the state of things we're in.
Speaker C: Right. So Novak kind of helps in two ways. One is that it's this constant optimization by ten X. But the other thing is that it makes it much, much simpler to have this best in class decentralized proving, where 1000 nodes, let's say that don't trust each other can all combine work to ultimately form a final proof.
Speaker A: That's, I think a new part of ZK rollup for me is multiple different computers, nodes coming together to produce a proof. How does that fit into a ZK rollup? Where does that fit?
Speaker C: Right. So the Zkroll apps have the same kind of infrastructure as the layer ones. You have the proposers, the attesters, and you have the blockbuilders, which are sometimes called sequences. But there's a new role which is the prover. And right now, and that new role.
Speaker A: The prover, comes because it's a ZK rollup, because it's a snark. So the snark that is a ZK rollup needs to be proved. That's the thing, yes.
Speaker C: So, traditionally, the role of prover was subsumed within the blockbuilder. And the reason is that the block builder needs to do something which is basically compute the state route. But now, in addition to computing the state route, there's this other thing that needs to be done, which is to compute the snark. And that is just so much more expensive than computing the state route that it makes sense to unbundle these two roles.
Speaker A: Right. Okay. And so with a ZK rollup, what does it mean for many different nodes in a ZK roll up to produce? Everyone's doing a small snippet of work that gets aggregated, but they're not doing the same work independently, correct?
Speaker C: Yeah, that's correct. So just to zoom out in terms of why we care to decentralize the prover, it's all about liveness. So right now, all, all the ZK rollups have a centralized prover in AWS or whatever it is. If AWS goes down for 1 hour, the ZK rollup goes down for an hour. So we care about this strong world war three grade liveness. So we need to decentralize the prover. And as you said, what we going to do is we're going to take this whole big block, this roll up block, partition it into small steps, for example, transactions, but maybe even more granular at the opcode by opcode level, and then have each decentralized prover perform their mini proof, and then aggregate these mini proofs into a final proof.
Speaker A: So in a ZK roll up, the blocks are really big, and we need multiple nodes to process parts of this block in order to submit, create a proof that gets submitted down to the ethereum layer one. And the more nodes we have, the faster that that block can be processed and the more, and also the liveness of the actual roll up increases, correct?
Speaker C: Yes. I mean, even without big blocks, even if you're only doing, let's say, 3 million gas every 12 seconds. So 15 million gas every 12 seconds, which is what the layer one does today, it's still extremely expensive because the EVM is not a snark friendly. So we want to combat this unfriendliness of the EVM. But once we've reached that, you're right, we want to increase the gas limit of, or another thing that we want to do is have multiple instances of your virtual machine. And the reason is that any given instance of the EVM is going to be bottlenecked by sequentiality. So if you take the EVM, for example, is what's called a single threaded virtual machine, and so it can only do so much. So let's say it can do 1000 transactions a second or 10,000 transactions a second. And once we've reached that peak, we've maxed out on the throughput of one single instance of the EVM. It will make sense for these roll ups to have multiple instances of themselves.
Speaker A: Okay, and so when you put all of these pieces together, what does the future of ZK rollups look like before and after this new Nova technology?
Speaker C: Right. So I guess Nova is one of the pieces that I'm going to get us from the present, which is a fully centralized prover on AWS, to this kind of utopic future, which will happen in a few years, where not only do we have decentralized proving, meaning we have potentially hundreds or thousands of nodes collaborating to form the proof, but we have very high gas limits per instance of the virtual machine, and we have multiple copies of the roll ups to consume all the data availability that we have on chain. And when I say that Nova is only one of the pieces, there's other pieces at play. So another very important one is hardware acceleration. So if you use GPU's, it turns out you can get a lot of hardware acceleration, but the step after GPU's is actually to build an ASIC where you get another ten x improver performance and all these prover optimizations compound on each other. And we're going to need all of them to get to where we want to be, right?
Speaker A: And when I think when people hear the term ASIC, they think bitcoin mining, but this is not what we're doing. Only one node needs one ASIC to do the job. You don't need a wall or farm of asics correct?
Speaker C: Yeah, that's correct. So we have what's called an honest minority assumption. We just need one prover to be online and participating and to produce the proof to have liveness. And so one of the things that we want in these proven networks is some amount of redundancy. It could be 100 to one redundancy, it could be 1000 to one. So 100 to one means that even if 99% of all the provers in the world just suddenly go offline, the ZK roll up still keeps on progressing forward.
Speaker A: Okay, so we are currently in. We have some ZK EVMs, Polygon, Zksync are live on mainnet scrolls coming soon. The costliness of these provers are extremely high, and there's only one because of how high they are. And that's the current state of things. With further optimizations of each one of their own tech stack, you get some improvements with this new Nova cryptography mechanism. You also get some improvements with moving from GPU's to asics. We also get some improvements. Can you put some numbers on these things? So, we're going from some amount of block space in the ZKeVM world, and then there's you aggregate all of these innovations together, and we get a different number of how much total block space there is. Is there any sort of, like, how much magnitude more block space do we get out of as a result of all these things?
Speaker C: Right, right. So I guess one metric that we could be looking at is the cost per transaction of doing the proving. And nowadays, we only order of $0.01, so it's actually not that high. And with all the optimizations, we'll get it down to noise. Now, to actually answer your question around the total throughput of the system, the really cool thing is that it scales horizontally. So the more people come in, the more fees are being paid. The proven network can actually grow organically.
Speaker A: It's like a natural scaling mechanism.
Speaker C: It's a natural. Exactly. It's a natural scaling mechanism. And I think the main cost that we're paying right now is kind of this somewhat subtle thing is the lack of decentralization, whereby we're trusting decentralized provers for liveness. One of the things that ultimately we want to do at layer one within Ethereum is snarkify the EVM itself, the layer one EVM, and build a so called enshrined roll up. And in order to get to that holy grail, we need to do all the hard engineering work. So all the stuff that the roll ups are doing the non entrant roll ups will ultimately be useful for the layer one as well.
Speaker A: Okay. Okay. So with the cryptography conversations that are happening here at Zuzalu, this whole Nova thing, we've applied this to the ZK rollups into block space, into just reducing costs and growing efficiency and all that kind of stuff. Are there other verticals that this new Nova technology can apply to?
Speaker C: Yes. So, like, generally speaking, Nova is an improvement to the ZKP world, the snark world. And in my opinion, snarks are just going to completely change the world. For blockchain specifically, we've talked about scalability, but it also has a massive impact for privacy. But even zooming out outside of the blockchain world, we're trusting entities to do computation for us all the time. We make a Google search, we get some sort of answer. We have no idea on the validity of this answer. And so one of the possible futures is actually that the blockchain space builds so called co processors to the main processes. So every time you have, for example, a cpu in your phone or a cpu in the cloud, that can be accompanied by a piece of hardware, a coprocessor, that does all the proving work in real time to prove that the processor's work is valid, and it ultimately generates a snark proof.
Speaker A: In this world, what does an invalid processor look like? What's the utility here?
Speaker C: Right? So one kind of very technical thing, which doesn't really happen in practice is, okay, what if there's a bug in the cpu? That has happened sometimes, but it's extremely rare. But the bigger threat model is basically that, you know, it's just this trusted operator problem, trusted third party. Maybe your banker is giving you your correct balance on your bank account, but maybe it's not. Maybe you just removed a few zeros or whatever it is. And so when you go and open your mobile banking app, you actually have a mathematical proof that this is indeed your balance, as opposed to having to trust your banker.
Speaker A: So the idea, I think what you're trying to say is, with this new Nova technology, we can. We can apply it to crypto, but then also there's ways to apply it in the rest of the world. Just because it's cheaper to run this thing, it's cheaper to operate, and so we can start applying more trustless principles in areas outside of crypto.
Speaker C: Right. And I think the end game will be that these code processors will be roughly 100 times the size and consume roughly 100 times the power of the processor. So there is still a cost there. But right now, we're talking more about 10,000 x overhead or 100,000 x overhead. And so that really limits the number of applications that snarks are being used for to the most high value ones. And every time we remove an order of magnitude, we're opening up the design space. And I think just like progressively, the Internet has been eating the world. Every time you increase bandwidth by ten x, snarks are going to eat the world. Every time you reduce the prover cost by ten x.
Speaker A: So how big of a deal, this whole nova thing, how big in the cryptography world, how, like, excited. What's the level of excitement from the cryptographers? Because for the normal people like me, who doesn't understand this thing, it's like, okay, I take this at face value. Justin's excited, but the whole cryptography community, how excited are they? Can we rate this on a scale of one to ten?
Speaker C: So the applied cryptographers here in Zulu Zalu, I think, are very excited. I'm going to say eight out of ten. When they came here, I think it was more like a five or six out of ten. And all the presentations that have been done, all the sharing of ideas, people were like, oh, wow, we can actually combine these clever ideas. And actually, some new ideas came through at the various workshops.
Speaker A: So, in the history of cryptography, where does this stand on breakthroughs? Would you call this a breakthrough?
Speaker C: It's definitely a breakthrough for the applied cryptographers, especially people who want to build real world applications. Of course, if you really zoom out over the multi decade, there's improvements to the asymptotics. So you might have a quadratic prover versus a linear prover. That's a huge improvement. But now, as I said, we've reached the optimal point from an asymptotics, and it's all about improving the constants. And as I see it, Nova is optimal from both an asymptotic and a constant standpoint for snarks that are using curves, elliptic curves. Okay, so this is the end of the road for the proof system, or very, very close, at least, like, the.
Speaker A: Theoretical max of what we could get.
Speaker C: Yeah, pretty much, yeah.
Speaker A: And so with the many different ways to scale, blockchain cryptography is one of them. You're saying that Nova is the theoretical max of the cryptography side of that equation for scaling a blockchain?
Speaker C: Yes, specifically for this proving of statements and ultimately generating a small proof that can be verified by a blockchain. Yeah.
Speaker A: So now it's just like a matter of building the infrastructure around Nova to support it, make it better, refine it. But Nova's the deal.
Speaker C: Yes. I mean, I think there's going to actually be a bifurcation of two types of snarks. There's going to be the so called curve based knocks like Nova, and there's going to be the hash based snarks, like stocks and fry based things. And the jury is still out because the underlying cryptographic assumptions are pretty different, and that has implications from a performance standpoint. But I think what will happen is that we will see both explored in parallel, and both are extremely promising. One of the good things about the hash based stuff is that it's post quantum. So, from an endgame perspective, that's kind of the most natural way to lean right now. But it turns out that you can generalize Nova to use so called lattice based commitments. A little bit technical, but basically the equivalent of curves, but they're post quantum.
Speaker A: Okay, so there's future proof is the.
Speaker C: Way to read that there's a potential roadmap to future proofing Nova, but right now, the details haven't been fleshed out.
Speaker A: Okay. That always kind of seems to be where conversations with cryptography leave, is. Like, there's potential road. We haven't figured it out yet.
Speaker C: Right, right. Yeah. At least for the curves, you know, we've reached something optimal from a constant standpoint.
Speaker A: So, Justin, here at Zuzalu, there's just been the idea of Zuzalu. One of the big ones is cross pollination. Like, get all the brains together, get them to talk. How's that just going for you? Talk about your experience here at Zuzalu.
Speaker C: Right. I mean, I've had a few kind of mind blowing moments just meeting people that I was not expecting to meet. I mean, one within crypto. You know, there's like, for example, Lev, who's very much interested in fhe, and he's very much interested in moon math, like witness encryption. And there's various other people like him. But outside of the cryptography world, it turns out I've learned of two different projects to build stable coins that are backed by central banks, of governments, of nation states. And these seem to be, like, serious projects, one of which is Montenegro. And I got to meet the most likely candidate to being the prime minister of Montenegro and people as part of his team. And they've been working on this crypto law for a very long time, and it seems to be a very serious and interesting project.
Speaker A: I am interviewing Mickey in a couple of days here.
Speaker C: Oh, excellent.
Speaker A: And so this will be a featured content on the Zoo Zeiler track.
Speaker C: Amazing.
Speaker A: Yeah. Who else do you think I should interview while I'm here? If you have any. If you had to pick.
Speaker C: Right. So for the second fiat stablecoin backed by central bank, I've been told to not leak the alpha, but I can tell you privately, I guess, and then you can ask them if they want to be interviewed.
Speaker A: Yeah, that seems to be the central bank stablecoin. There's only so many central banks to go around.
Speaker C: Right.
Speaker A: Justin, thank you so much for guiding us through the world of ZK cryptography, and also, I hope you enjoy your time here at Zuzalu.
Speaker C: Yeah, thanks, David. Cheers.
