[
    {
        "speaker": "A",
        "text": "In general, my prediction would be that 2024 is the year where performance, you know, we end the year, basically, and performance stops being a differentiator. Everybody will have figured out high performance. Finally, now that we've learned how to modify nodes, everybody will like figure out in some way, you know, not everybody will be in production, few will be in production, the best teams only. But over the years, what's happening is that the best technology that was considered a mode of is finally starting to be, for lack of better work, democratized and accessed by everyone."
    },
    {
        "speaker": "B",
        "text": "Welcome to Bankless, where we explore the frontier of Ethereum's layer two rollups. Today on the show we have Georgios Konstantinopoulos, the CTO and researcher over at Paradigm, and Andrew Huang, CEO of Conduit. Both of these extremely smart gentlemen have unique vantage points over the future of Ethereum's rollups that needs to be shared with the world. Here is what you're going to hear on this episode today. What is the state of Ethereum's rollups in 2024? What's going right? And what is still left to do? How will rollups recompose with each other? What mechanisms are there to help with roll up composability? And do rollups even need to compose with each other at all? Or is that narrative just totally overblown? What about roll up security, multi client fraud proofs and multi ZK provers, why Georgios thinks we get them this year, and why Giorgio thinks we're entering a golden age of layer two experimentation? I learned so much in this episode. As soon as I'm done recording this intro, I'm going to go back and listen to it. Before we get into this episode, though, we disclose Ryan and I hold investments in some of the layer twos mentioned today. We also hold eTh. You can see all bankless disclosures@bankless.com. disclosures now let's go ahead and get right into the episode with Georgios and Andrew. Bankless Nation I am excited to introduce you to Georgios Constantinopoulos, the CTO of paradigm. Georgios is an enjoyer of Rust and has helped build Reth, an Opdenne Reith, a rust based execution engine for Ethereum and the op stack. He's been on bankless before, talking about MeV when we first discovered its implications all the way back in 2021. Georgios, welcome back to Bankless."
    },
    {
        "speaker": "A",
        "text": "Hi David, and thank you for having us."
    },
    {
        "speaker": "B",
        "text": "Andrew Huang is the CEO and founder of Conduit, which is a roll up as a service provider. Many of the layer twos which you have likely used were spun up and supported by Conduit, including Zora, Avo public goods network and many others. Ras, as they're called, as infrastructure supporting rollups. Both present and future have a unique vantage point for seeing the evolution of layer twos, which is why we're bringing Andrew on the show here today. Andrew, welcome to Bankless."
    },
    {
        "speaker": "C",
        "text": "Thanks. Thanks for having me. Excited to chat today."
    },
    {
        "speaker": "B",
        "text": "I think the whole entire Ethereum ecosystem, the crypto ecosystem, would really just like an audit of the state of layer twos, the state of rollups. There's many, many roll ups going in many different directions with different design strategies, different holes, and there's still a lot of unknowns for what the future holds, I think for descaling of Ethereum. While rollups did give us a lot of clarity for how Ethereum will scale, it has also given us a lot of questions, mainly in the world of fragmentation and security. So George? Yes, maybe we could just start with that. Can you just kind of give us the vibe of the audit of layer twos? How are we doing, what's going right? What do we still need to work on? Just overall, give us your sentiment check."
    },
    {
        "speaker": "A",
        "text": "Yes, of course. So starting from the basics a it exists and it's real, which I think is on its own remarkable achievement after many years of hard research and hard work. And I have pulled up here l two bit, which is a great website that many people reference these days. And you can notice that the current TVL in layer twos amounts to almost 20 bill, a number that has over ten x almost this year. The risk is in a lot better spot. We're suddenly starting to have rollups deploy more fault proofs. In 2024, we're seeing the security councils going live. We're gradually moving towards the so called stage two decentralization in rollups. If you keep looting at l two bit, you will notice that there is over 20 or 30 maybe rollups live of various types. Optimistic the k some are not even rollups. Granted, some might have the layer two or the off chain data availability, but overall things are growing, technology is advancing, things are being deployed, which I personally find really exciting. That is on the core tech side. Now there's a whole ecosystem being developed around roll up services. There's companies like Espresso that are building shared sequencers for people that want to outsource their sequencing needs. There's companies like Conduit, which we'll talk about in a second, that are building the infra for people to deploy more and more roll ups. There's other companies building alternative data availability layers. And you know, there's the evolving roll up stack and the layer two stack that we envisioned many years ago that is finally starting to play out in production. Now, are we done? No, we're not. We still have a lot more to do. The ecosystem still depends on permissioned fault proofs, it still depends on permission sequencers on. In general, the stacks are nascent. We still have a lot to do, but that is for the coming years. But overall, super excited, and I think today is a great day to be having this episode because we're almost at a pivotal point in the scaling story for Ethereum and its ecosystem."
    },
    {
        "speaker": "B",
        "text": "What are the big problems that you identify, Georgios, as things that we still need to work on in the layer two space. Obviously, security is one that you mentioned and brought up and will be an ongoing thing that we will need to work on for a while. There's been a recent focus on while layer twos are scaling, they are also fragmenting, which is one of the big problems. What would you say are the big obstacles, the big research obstacles, or maybe engineering obstacles that the Ethereum layer two space needs to focus on more?"
    },
    {
        "speaker": "A",
        "text": "Of course, I think we're actually way past the research phase. This year has shown that we are entering this deep productionization phase where all the research is mostly done. And right now we're just seeing production as deployments of things that we've known for a while. So right now, the things that I think are very important are, as you said, security. So right now you can check the state of the security of the ecosystem just by counting the multisigs that govern all the roll ups. As long as we have all of these multisigs, it's going to be hard for us to consider the ecosystem like really mature and secure. Now, what are we doing for that? We're moving towards multiple fault proof implementations for each roll up. We're moving towards delays on any kind of power that any Security council must have. We're limiting the power of every security council to, let's say, only when there is a layer one hard fork. We have a lot of work to do on decentralization. Right now, most, if not all rollups are sequenced by one sequencer, usually the one by the labs entity or the foundation entity that the company runs, that the project that builds them, which is okay given that there is the layer one fallback option. However, it also means that the system can go down, as we saw many times this year with various projects. So overall, we also have work to do there. So one, security. Two, decentralization. Now three, there is one not as spoken about topic. That is the tooling, which is all that we think about at paradigm. And for tooling, the problem is going to start manifesting when people switch to different opcodes, when people switch different precompiles, there's a lot, a lot of areas where things can start to change. And right now the tooling is not ready to support this evolution. In the layer two ecosystem. For example, if somebody builds a new opcode for their layer two because they want to experiment now, they need to go into the solid compiler and edit the compiler and figure out a way to expose that the user and people just don't have the expertise to do that. And some mistakes will happen if we don't prepare for that. So we really, really, really need to make the tooling like ready and robust and modular and extensible so that it's ready for this cambrian explosion of layer two innovation over 2024 as the barrier to entry goes down and as the stacks mature. And that's part of like the tooling that we're also trying to build with our teams in an effort to make them extensible and ready for layer two."
    },
    {
        "speaker": "B",
        "text": "Andrew, I want to turn to you and get kind of a similar perspective from you and what you see over at conduit. But before we dive into more or less the same set of questions with your perspective, maybe you could also illuminate what your perspective is. What does the perspective at Conduit give you? You are a roll up as a service provider. You talk to a lot of layer two teams doing different things, similar things. And so maybe first illuminate for us the vantage point that you have as your role at conduit. And then we'll kind of go into the same set of questions that I just asked Georgios."
    },
    {
        "speaker": "C",
        "text": "Yeah, definitely. So, as you said, we work closely with a lot of the later two teams. We work with folks that want to launch chains. We work with integrations that need to integrate on those chains. And so I think it's a very kind of like pivotal point in the ecosystem that affords us the opportunity to really like, see a lot of different things and importantly see what's kind of on the bleeding edge here and how we can help enable that. I think the TL doctor here for us is like if a year ago, I think nobody was really thinking about many different roll ups and I think it was viewed as much riskier. I think today it's in some ways kind of like becoming the default. And we're very excited to kind of help with that transition. And I think for a variety of reasons, both economic as well as, frankly, just at a technical level, enabling kind of new applications to be built. We're really seeing kind of the rise of the modular blockchain. And I think in some ways it's kind of the cosmos thesis, but playing out on Ethereum where people do want sovereign chains, but you kind of have those interoperability standards that allow folks to kind of transact across chains and across roll ups in a way that kind of makes sense. And so very excited to kind of be at the center of that and kind of playing a role in the, all the complexity. George was kind of mentioning around fault produce and removing multisigs and upgrades. And it's just one difficult foundation or whoever's running them to do themselves. Like it takes a lot of time, and not to mention if a random dev team wants their own kind of roll up, in some ways be impossible. And you see folks kind of messing up all the time and their bridge gets hacked or something happens. And I think one of the benefits of a Ras provider like conduit is you get all that same great tech and all of those migrations and upgrades seamlessly AWS. Instead of focusing on building the best data center and all of this undifferentiated heavy lifting, you get to focus on the important thing, which is building a great application for your users. We're very excited to help facilitate that transition."
    },
    {
        "speaker": "B",
        "text": "What are the strengths of the layer two ecosystem right now? What's going well for teams as a whole that you work with then also, what are some of the pain points? What are the hurdles? What are the difficulties that teams are experiencing?"
    },
    {
        "speaker": "C",
        "text": "Yeah, so one, I think the first question when it comes to rollup is how do you actually stand this up in production in a secure, reliable, performant way? And I think a lot of rollup frameworks make it easy to spin up a testnet, even like a local version. But there's a huge gap between that and something that is ready to custody and hold user funds. And that's really where conduit comes into play again, making that seamless. So you get that at the click of a button. All of the work we've put into reliability, security, performance, et cetera, you just get for free. And again, it makes sense for us to invest in because we run hundreds of these across mainnet and Testnet. There's small percentage points that might not matter for you, really matter for us. And that means you're getting the best offering. I think typically after that the next thing we see is, frankly, just for lack of a better word, PMF. I think early on in the narrative we just launch a chain and it's going to work and it's a new thing, and therefore we'll have users very quickly. It's become clear that rollups will need to differentiate in what they offer. And I find that exciting because I think we'll start to see instead of clones of your favorite DeFi app or the 10th or 11th clone of Uniswap, we'll actually get something new and differentiated. I think one example that was a dark horse for me, but it's been really exciting to see play out is something like Zora network, where they're very focused on the collecting side of things. And that network has grown in a really interesting way, where suddenly they have all of this data around mints and art that only exists on the Zora network. And I think they'll be able to create really compelling applications and new behaviors on top of that."
    },
    {
        "speaker": "B",
        "text": "So you're saying that you think that there will be a trend away from the highly general layer twos, and layer twos as an ecosystem, as a category, are all going to shift the Overton window towards more specialized niche layer twos that are optimized for more narrow use cases. Is that what you think is going to happen?"
    },
    {
        "speaker": "C",
        "text": "I'd say we see a bit of both. I think it ultimately depends on the brand. I think something like base, for example, right? Huge brand, a lot of access to retail users. That just makes sense as a generic chain that has everything. But I think for your average dev team, that a startup, they need to build something new and differentiated and aren't going to have the same distribution or brand benefits that some of these larger organizations have. And I think the only way to differentiate yourself is really on the go to market and what kind of uniquely is happening on your chain in terms of the state space. And that's kind of my recommendation to teams, is actually building something novel versus just trying to be kind of your hundredth dj on defi chain."
    },
    {
        "speaker": "B",
        "text": "So I want to ask the very big question that I think a lot of people are asking in the layer two space, which is few roll ups or many roll ups. And there are arguments for both sides here. The few roll ups arguments are the fewer rollups you have, the more net composability there are. So, you know, there's fewer different chains to be fragmented around. So like, you know, the more liquidity aggregation, you know, fewer networks in the dropdown on metamask, and so more composability is just good ux. And also rollups individually have costs. And so if we aggregate everything together, you can consolidate the costs in order to save money. And so these are some of the arguments for why there will be a few roll ups. But then there's also arguments on the other side, which are that technology costs always get cheaper, and that's something that conduit is doing. It's making it cheaper for layer twos to exist, and that will only improve over time. There's also rollups, desire for sovereignty that we know that this is a powerful force. Games, for example, will probably want their own chains. So what do you guys think? Which is it? Is it few roll ups? Is it many roll ups? Georgios, I'll start with you and Andrew, you'll follow along."
    },
    {
        "speaker": "A",
        "text": "Yeah, of course. So there's two axis on the demand side. One is cost and the other is customizability or being free. You called it sovereignty earlier, being able to do whatever you want on cost. How I would think about it is that rollups is an elastic scaling solution. You add more compute, the more load arrives. So if we end up having so much compute demand, then probably there will be many roll ups, because it is unlikely to expect, like we saw from all the past l one lessons, that one chain will be able to accommodate the world's compute. Or that's where I come from, at least others. Some might disagree, and that's perfectly fine. Now, on the side of customizability, of course that comes off at odds sometimes with the perfect horizontal scaling thesis, which means that, you know, sometimes when the demand might be enough for five chains, maybe there's ten chains, because people want some extra customization that you cannot get in the other place, for example, a big brand like Coinbase, or I don't know, say Starbucks or someone for some reason, maybe they would want to be on a separate area. Why? Maybe there's like a lot of customizations that they want to make, or maybe they want to own the brand, or they want to do specialized airdrops and whatnot. Hard to tell. So I think it is hard to bet on few, as in one or five or ten. And I would probably expect the power load distribution to nobody's surprise on where the demand gets kind of concentrated, let's say in the top, whatever. But they expect a very, very long tail of chains with also varying duration. Because imagine a game could be played over a day on people have called these flash chains one day, roll up, pop up roll ups, whatever you want to call them. Maybe you play a game for a day, an on chain game. That game maybe has, like, stupidly high state growth or whatever, so it would never make sense to actually put it on a real network. And then you just checkpoint the result into a layer two or a layer one or something else. So the whole co process or thesis that many people have been putting out, it might also apply in the roll ups, and that would enable thousands and thousands and thousands of rollups, but also with a very small duration. So, you know, power law, a lot of activity, but the duration of each chain might change. Demand for customizability also might affect that."
    },
    {
        "speaker": "B",
        "text": "The reason why I like having both George Yos and Andrew here is we have George Yos, the researcher, and Andrew the market founder. And so one of the perspectives I enjoy here is that conduit is tapped into what the demands of the market are. What can you tell us, Andrew, about the few rollups versus many roll ups conversation and in terms of what the market wants with your clients and needs over at conduit, for sure?"
    },
    {
        "speaker": "C",
        "text": "Well, I guess one point zooming out and kind of tying it back to what Giorgio said earlier. I think if you believe there's only going to be a limited amount of demand for crypto compute, then I guess, like the kind of few roll up kind of world makes sense. If you're really bullish on crypto and new applications taking off more and more demand, I think by necessity you're just going to need many, many of these different kind of crypto compute environments. So just as an argument for industry growth as a whole, I think it's almost somewhat bearish to believe that there's only going to be a couple of roll ups to serve all that demand. And so here at conduit, we're very bullish on crypto and believe in this centric world with thousands or hundreds of thousands of these rollups. And so just from that argument alone, I think we're very excited about it in terms of what we're seeing from a market level. I think ultimately, one customization is a good point, but I think even more than that, I think there are large economic reasons to launch your own roll up. I think the biggest factor that we see today is that when you launch and deploy on another chain, you're essentially paying rent to that chain. And by deploying your own chain, you get to internalize those fees, not to mention you have more control over your own ecosystem. It's narratively a great thing to do. You get to customize, you get to maybe build your own ecosystem. On top of that, you have your own l two with many different l three s on top. Other types of applications. I think just from a pure economic argument and a pure sovereignty argument, there's just this incredible demand to launch your own block space. In the same way that if you look at web two today, it's not one big global computer or a couple big global databases. Every company has their own application. If you look at Facebook, they have a ton of apps. And whatever they've built on top, it's this gigantic system that could be one big roll up, or it might be multiple roll ups, but then you have this long tail of other companies that also have their own roll ups. And I think if I'm thinking through what the future holds for crypto, that model seems a little more clear to me where I think it's unclear that every application needs maximum composability at all times and therefore needs to pay all this rent and all these other things to a single chain. It seems clear to me that asynchronous message passing, or if you look at APIs today, it's this asynchronous webhook thing, or you just have an integrations API that happens less frequently than normal, but then you co locate the logic that's really important. I think the scaling argument, this demand for customization, this demand for economics is really driving what we're seeing in terms of this cambrian explosion of chains."
    },
    {
        "speaker": "B",
        "text": "Yeah. If we're going to see thousands and thousands and millions of chains, we need infrastructure that we can copy and paste. We need highly replicatable infrastructure in order to make that happen. And this is where a lot of the battles are being fought, in the layer two space from all the super chains standards, the optimism super chains, arbitrum orbits polygon supernets, ZKC and Kuiper chains. The way I think about these things is that they're all economic zones, because the block space is very alike inside of a network, right? And so like the op stack mainnet is very alike to the base main net, right? And so these have a relationship with each other that's more close than, for example, like base is to arbitrum. And so this is how I think about these things. Economic zones. And they can engage in trade with other economic zones, right? Like arbitrum can trade with optimism via a bridge, like across. But trade is going to be easier inside of the optimism. Super chain trade will be easier inside of an arborshim orbit, and it'll be a little bit more costly to go between these things. This is my perspective for how I understand it. Georgios, how do you think about the whole evolution of super chains?"
    },
    {
        "speaker": "A",
        "text": "Yes, maybe to give you a bit of a cynical take to start, anything that we say in this conversation will probably be more speculative and more an expectation of what is to happen, given that there's none of these systems live yet in production. There exists one system called Astria that was deployed a few weeks ago, but it's still in testnet, and it even had some issues when they deployed their shared sequencer. So it might be worth zooming out and thinking what problem are we trying to solve as a first place? And the problem you mentioned earlier, David, it's how do we make these different cities talk to each other in a cheap way without introducing too much additional layer of trust? When is this useful? One would think first and foremost on DeFi or on transfers. I am on chain a, you're on chain b, and we want to talk to each other without having to think even about where am I sending money to? Right? And as you said earlier, we don't want to be in a world where I go to metamask and I pick from a drop down of 55 or 100 or whatever rpcs. That's terrible user experience. And honestly, we would have failed miserably if we're in that world. So the super chains, or the shared sequencers, or whatever you want to call them, they come in as a set of solutions that try to address that by allowing you to interact with one endpoint as a user, one place, and the sequencer smartly will route to the right area whatever transaction needs to happen. And all of these super chains roughly have that same vision that they want to abstract away the communication inside of their own ecosystems. Now, there's solutions that achieve that for heterogeneous systems like Espresso. And they introduce the require modifications to each of these stacks to make them compatible with each other. For example, to make the arbitrary, morbid stack shared sequenceable with the optimism super chain stack, Espresso needs to modify, with the same modifications, both systems to make them compatible and to what extent that will be feasible or not is TBD, and is an exciting area overall. One point that is worth unpacking, though, is that to the best of our knowledge, none of the systems offer, let's say, the holy grail of synchronous calls across all of these systems. There's no world where, you know, you can say a calls b, which calls back to a and does a lot of things together. Unless you're in the design where you're basically one chain, and that is where the optimism super chain design is going toward, for the most part, what you get from all of these shared sequencing designs, you get atomic top of block inclusion, which is useful for MeV, which we had talked about two years ago. David so the idea there is that the shared sequencer is able to guarantee that five transactions will always be at the top of the block. And these five transactions that will be on top of block a and not on top of block b will be extracting some kind of arbitrage opportunity that existed, and that makes money, and that's a valuable service to be offering, and that could be a valuable infrastructure protocol to be running. However, to go far beyond that in terms of conditional execution, let's say I send the transaction on a, and that means that the transaction also delivers on b and stuff like that. I don't think we have any design yet that is soundly implementing that."
    },
    {
        "speaker": "B",
        "text": "So that part is still in the research phase."
    },
    {
        "speaker": "A",
        "text": "I would think that it's the way that people are trying to do it without doing further modifications to their systems, I think is not there yet. The most promising design that I have seen is one in a blog post by James Prestwich, which describes a certain way where each block commits on every other block in the super chain. Again, research phase. So I take back what I said earlier, that we're done with the research phase. There is that component that is not figured out, but it's also worth understanding that it might not be worth figuring out fully. Maybe the holy grail solution of feature set nobody needs, and maybe you can get away with something you can ship in a year or in six months. That solves the real problem, which is not actually the composability but the decentralization. The problem is that every one of these systems is run by one person, whereas ideally there's not that much of a problem. As you said earlier, we have a cross, we have ten bridge protocols to do all the transfers. People will let the market figure that out. But the part about the decentralization is more critical, especially as more and more infrastructure gets launched and we put our trust on few intermediaries."
    },
    {
        "speaker": "B",
        "text": "George Rose, could you go into shared sequencing a little bit more? Just what composability benefits does shared sequencing give chains, and does that conversation change if we are talking about all these chains inside of a single setup, like all the op stack chains that are going to be a part of the super chain. These can shared sequence with each other in some degrees and produce some composability benefits. And then there's also the potential, like you said, of optimism and arbitrum, shared sequencing, and also getting those kind of composability benefits. Overall, what does shared sequencing get us?"
    },
    {
        "speaker": "A",
        "text": "Right, so it solves you composability and to some degree decentralization. Composability, though it solves to a small degree, as described today, and TBD, whether it can solve it to a larger degree. And surely shared sequencing in the same ecosystem, in the same flavor, is going to be cheaper, easier, more compatible, whereas trying to make two different ecosystems talk to each other will be harder and might not even be, you know, desired to some extent."
    },
    {
        "speaker": "B",
        "text": "Yeah, because that's always what I've thought is going to be difficult, because sequencing is the golden goose for layer twos. Like that is where they get a lot of their fees. So why would arbitrum and optimism want to give up their sequencing fees to espresso in the name of a decent but marginal amount of composability benefits?"
    },
    {
        "speaker": "C",
        "text": "Right."
    },
    {
        "speaker": "A",
        "text": "I think jury is still out there, and I think depending on who you ask, you will get a different response. From my point of view, the optimism ecosystem, for example, wants to build a moat around a different set of, let's say, infrastructure components or values, for example, the governance and the entire process around the law of chains. Whereas it's kind of almost acknowledging that the biggest part of the technology is open source, is to be given away. It is not really a moat. Or if it is a mode, it's a very weak one that's going to go away over time, whereas the real mode is elsewhere. So that would be one take. And also remember that to opt into the super chain ecosystem, and Andrew will tell you that very well, you pay a rent back to the super chain, the optimism collective, and that is where it comes in. Effectively, optimism is offering a service that is the shared upgrades, the shared governance, and the shared sequencing, which is going to be in part mandated by optimism, smart contracts, and it's happy to take a cut for it and all the rest. Let the ecosystem figure out, because it's hard to pick a solution yourself. Maybe you don't even want to build it yourself. And Andrew can cover more on how this looks like from the business perspective of how the roll ups actually, because we've done this with multiple customers so far."
    },
    {
        "speaker": "B",
        "text": "Yeah, yeah. I definitely want to get there because I think that that conversation of synchrony and chain composability and chain governance, I think is one of the most interesting ones. And I know that Andrew, at conduit, you're like right at the heart of that conversation. But Georgios, one last conversation before I ask that question to Andrew. Is universal composability a dream? Like every single layer, two across Ethereum, different constructions, different setups, even if it's like ten years in the future? Is that a pipe dream or how far can we get there?"
    },
    {
        "speaker": "A",
        "text": "I think it is hard to beat against the mad Ethereum scientists just to say it as a prerequisite. So, you know, David, it's hard to bet on a ten year time horizon. My view is that we don't have any fundamental, let's say, problems to be solved, but there's also a degree of like a prioritization. And I think that we also need to think about the demand side. When the demand side requires that we solve universal composability, trust me, we will find a way to solve it. But I think sometimes we should acknowledge that in the ethereum ecosystem, we don't always take the most, let's say boots on the ground demand side. Then we always think, what's the most perfect protocol I can design for the next ten years? So I think trying to answer that question, we could entertain it, but I think it's more worth focusing on the important topics on the ground. Exactly, exactly. Because we're in this very pivotal phase in crypto and we really need to stay focused, stay close to the customer and on the user experience. We solve fees, then we solve wallets, we solve wallets, we solve products, and then, you know, right off to the."
    },
    {
        "speaker": "B",
        "text": "Races and yeah, the horizon always continues. Andrew, I want to talk about the spawning of these super chains and orbits and hyperchains and super nest, because from my perspective, conduit is the place where these things spawn from, since conduit is the place where it's the cheapest to make more chains. So I always kind of see the super chain spawning out of conduit, and there are competitors in which the super chain also can spawn from. But a lot of op stack chains come out of conduit. So what is your perspective on the evolution of these systems? What do alike layer twos? What benefits do they have of being both optimized and built by conduit? Like call it a neighbor change? Right. Just overall, what's your perspective on the growth of these ecosystems?"
    },
    {
        "speaker": "C",
        "text": "Yeah, I think the growth is really exciting. I think narratively there's kind of optimism. Probably fired the first shot here around like the super chain and kind of homogeneous block space, kind of tighter interoperability. But I think other ecosystems have quickly followed in their ways and credit to them maybe zooming out. I guess the question would be how much does that interoperability actually matter to users based off of the current applications that we're seeing? It's not clear to me that there's a huge benefit outside of the current verging protocols that already exist just to rattle off a couple of customers. Somebody like Avo, it's this decentralized exchange, you know, not a lot of interoperability opportunities outside of bridging from other roll ups. You can take a look at public goods network, right, and kind of running some bitcoin grants rounds, but again, not a ton of opportunity for interrupt beyond bridging to the chain Zora network. Again, you can make the same case. And I think that will just frankly be kind of how these new roll ups actually launches. They need to do something differentiated and new, and by definition that isn't going to naturally need that level of interoperation that, you know, we desperately think everybody needs. And I think in some sense there is this. I'm not going to call it vestigial, but I think we're so early in terms of how one, like crypto computers developed, two, how applications have developed that it's so hard to make the case that this is the end state of maximum global composability. And that's the most important thing. And maybe drawing an illusion to web two. Back before we had networked computers, Unix pipes are the equivalent of composability. And people use them all the time then. Now you just use it for a bash script on your local computer. Everything happens in the cloud, everything happens over the network. It's just this new dominant model that became possible because the compute landscape really changed and the capabilities of that allowed for new things. I think there's somewhat of this vestigial overhang, maximum composability, that even if you look at the numbers today, aren't necessarily like the largest use cases, particularly in like emerging roll ups."
    },
    {
        "speaker": "B",
        "text": "Interesting. George previously gave us an axis about a roll up designs and constructions. There was like the customizability and then the cost. I want to present another spectrum, Andrew, along like this whole like super chain conversation. And there is, on one side of the spectrum, every single chain is a part of a super chain, whether it's the optimism super chain or an arbitrum orbit or as a polygon supernet, or every single chain is a complete independent chain. And it's not part of these collectives. The reason why I always kind of thought this super chain conversation is cool is because these are digital collectives of block space. And there's like, the optimism collective, which manages the super chain. And where we end up on this spectrum, where every single chain is its own independent ecosystem, versus every single chain has determined that it's beneficial to be a part of a collective. Only the future can really tell us the bowl case for optimism. The optimism collective, is that the value of being a part of the collective, the value of being part of those shared upgrades in that homogenous block space, is worth it. So that the fees that the collective charges, like the union fees call it, is worth it. That's a worthwhile trade off. Or maybe it's not. And people are more inclined to stay an independent layer two inside of their own ecosystem and sacrifice some of those composability benefits. Do you have a perspective as to where we end up on this sliding scale of unionized layer twos or independent layer twos?"
    },
    {
        "speaker": "C",
        "text": "Yeah, I mean, like, maybe like a reframing of super chain is kind of taking out the interoperability aspect is like, how much is it worth it to have homogeneous block space, right, that, you know, you can interact with in the same way has all of the same security guarantees. And it seems like, you know, in a world of many, many roll ups, like that matters a lot. There's a reason why in web two you talk about like API compatibility, right? And like different clouds, right? They copy each other's APIs so that you can just migrate in or you make a new database product and it's postgres compatible. And so there is a good reason as to why to do it in a compatible way and to have these shared upgrades that maybe isn't necessarily so tied to the interoperability aspect, even though that might be a nice future add on, I don't know that that has to be only part of the super chain. For example, you can follow the same upgrades and do that without being a part of the super chain, or even on the arbitrum orbit side, you could stay closely to the spec, stay closely to what the governance approved versions are, and upgrade in concert with that. I think ultimately it's an area that infra providers like conduit will play, and a service that we offer is that you can get the gold stamp that your chain is going to be compatible and equivalent to all of these other major networks that are very popular and everybody use. I think to answer your question, I think compatibility is a key thing, particularly in this early phase as we're getting this explosion of chains, as we're getting these integration headaches where it's like, oh, a new chain spins up every day, how am I going to integrate? And just knowing that your stuff is going to work properly is a huge benefit that ultimately ties into the types of customizations that we're interested in at conduit, where one thing we, we can't talk about it publicly, but we have some exciting custom chains in the works. Ultimately, one of the services that we provide is, listen, we're happy to do customizations, but we also want to make sure that one, you're forged compatible with future upgrades, and two, that you're not doing it in a way that makes you so unique that people can't use you. We're very excited about both the max compatibility as well as this broad spectrum of customization that ultimately will enable new types of applications."
    },
    {
        "speaker": "B",
        "text": "Okay, so compatibility, I really want to double click on that. So you're saying that there's some benefits of having homogeneous block space and true composability in the blockchain transaction sense of the focus that so much of crypto Twitter has been on lately. But I think what you're saying is that there's also just ecosystem benefits, there's infrastructure benefits, there's shared standard benefits. Maybe developer outside of my wheelhouse here, because this is all very technical, but just there's like the benefits of the EVM, for example, or like the optimism ecosystem has rust, the rust op stack client that I think George, Jos and many others at paradigm are using. And so if you want to be a part of that shared ecosystem, you have benefits of being inside of the op stack or part of the super chain. Is that part of your answer?"
    },
    {
        "speaker": "C",
        "text": "I think that like similar to how, you know, browser and the JavaScript have taken over kind of web development and everything is based around this kind of core kind of open source software stuff, there's a very good reason to stay EVM compatible and like work with, you know, the optimism super chain. Really anything that's like compatible and has a, a big open source community around it to benefit from all those network effects. You can see firsthand with what George's is doing with breath and opera and all these other things that you just work with EVM, you work with optimism or whatever role framework and you just get these massive performance improvements for free. Ultimately, I think as a dev and as somebody who's going to launch their chain, you want to be able to ride that wave up versus fighting against."
    },
    {
        "speaker": "B",
        "text": "The tide of George Aries. Can you just illuminate, much better than I could, the technical benefits of staying inside of the ecosystem? Just all of the developer ecosystem tailwinds that one would get by joining one of these broader ecosystems versus swimming alone, for example. What are the benefits? Can you help us shed some light on that?"
    },
    {
        "speaker": "A",
        "text": "Well, there's the benefits from the infra operator side, the chain builder, and then there's the end user using a chain that's a part of that. But I would think that from the info operator side you basically get a managed service of something that you would otherwise you yourself need to do. That is security upgrades, governance, patching, having on call support, all of these things. I think these matter a lot more than people think from the user side. By being part of such system, you would get the free composability to your users. And you also get of course that one RPC. You don't need to talk to every single thing separately. So in general, in one word, maybe what you get is that you get homogeneity. I see you writing in the doctor, you're writing the trust as a differentiator. I think you get the trust anyway. If each system is done properly, I think it's honestly, you know, right now it's. We're in a bit of an embarrassing state where, no, you don't get that much trustlessness in the end state, whether you are in a shared or not system. If that system is running off of a specific version of a broader standard, I think it doesn't matter if it's part of a super chain or if it's not. Ultimately, just to illustrate the point of an example, if the blast l two is deployed correctly with appropriate fault proofs, and you're in it, you have same protocol security as any other l two or l two ecosystem deployed with the appropriate parameters, do you get the same composability? No, because maybe you're deployed on a separate ecosystem. If you're at Tatel two, if you're in the ecosystem, then you get the composability. So on the trust component, I feel like you're good anyway."
    },
    {
        "speaker": "C",
        "text": "And even at a more pedestrian level, I think compatibility is important for tooling. I know George just brought it up, but just to make it explicit, things like block explorers, things like tenderly, things like Dune, all of this tooling, they need to run RPCs, they need to run against RPCs, their software works against certain versions. If you think that every single one of these tools has a cracked infra team and is going to be able to keep up with all the different customizations. I think you have a very overrated sense of how many engineers there are in the industry. I think ultimately, again, you just want to be able to ride this wave where you have the same stuff you can plug and play just by switching out an RPC endpoint versus something massively custom that then you're fighting against the tide."
    },
    {
        "speaker": "B",
        "text": "Georgios, I just want to double check on that trust element, make sure that we're talking about the same things. The way that I kind of understand this is that when there's going to be 10,000 different chains, the overhead for users to understand the safety and security of the chain that they're on or they're interested in using is going to be too much. And I don't really think about this when I go to any websites. That's fair, but sometimes there's one of these websites that I go to and my browser is like, hey, this website you might want to think twice about. And I would have no idea how to identify that if the browser didn't tell me this. And so this is like one of the benefits of having a shared state of whichever the chain is built on. So like the op stack. And so you could imagine that little shield of security in your browser when you go to an HTTPs website, you might have something similar. So it's like you are on a verified op stack chain with a verified client. That's kind of what I mean by trust."
    },
    {
        "speaker": "A",
        "text": "That's a great point. That's a great point. I had not appreciated that earlier when I was talking. Yes, spot on. Every shield that we have on our browsers, when every browser will bundle a wallet and whatnot, every shield that we have in the browser will for sure be extra shielded, let's say, when it captures an op stack chain or an orbit chain or whatever else, I don't know that the user will know whether it is part of a specific flavor, but the user will need a certain lockbox to indicate it."
    },
    {
        "speaker": "B",
        "text": "Beautiful, beautiful. I want to pivot to the modular conversation, Andrew. Right before we started this podcast, I actually noticed that conduit put out a tweet about Lyra, which is one of the conduit op stack chains, pivoting its data availability from what I believe was the ethereum layer one to Celestia. Give us your perspective as to the evolution of this conversation. Where we have Ethereum layer twos that are potentially consuming Celestia for data availability. Where do you think this goes in 2024?"
    },
    {
        "speaker": "C",
        "text": "Yeah, I think what this really enables is bringing roll up costs down significantly to a point that really enables new apps. I think rollups kind of quite expensive, particularly if you look at December, right, mainnet gas prices were like spiking, there was a ton of activity, and even if you have your own roll up, you're still not fully insulated from all the activity that actually happens there, right? Because as mainnet gas prices increase like five x ten x 100 x whatever, it is suddenly like your app, which maybe had sub cent fees, sub ten cent fees, you're paying like a dollar per transaction. And that meaningfully changes the business model and the economic model for a lot of these roll ups, where frankly a lot of them are either one. The point of the chain is that fees are low to enable new types of behavior. Like on Zora, network collecting and minting is very cheap. Or frankly, a lot of these applications are paying gas fees for their users and they're just running up this huge bill. That's ultimately where all DA layers are really going to come into play, is separating that data cost from the activity that's happening on Ethereum mainnet, while there is an additional security assumption around that data being available and being able to relay it to Ethereum and eventual integration with fraud proofs. But today, frankly, the just economic reasons of having ten hundred x thousand x cheaper fees or really just bottom line for the roll up are just too insane to ignore and are really enabling any team to launch a roll up today."
    },
    {
        "speaker": "B",
        "text": "Interesting. One of the perspectives for app chain bears. We'll say that app chains probably won't have enough users or transaction volumes in order to justify themselves because roll ups have costs. Using Celestia or just cheaper data availability providers for cheaper DA than Ethereum helps with this thesis where, well, if we have cheaper data availability, it's the main cost for rollups. Therefore there are going to be more app specific roll ups that can economically justify themselves. What's your perspective on this? How far will we be able to go down the long tail of app specific rollups being able to economically justify themselves? So there's two variables here. There's just more users making more transactions and so we can justify more that way. And then there's also costs going down on the infrastructure and data availability sides. Just give us a peek forward on your perspective with this."
    },
    {
        "speaker": "C",
        "text": "Yeah, I mean, I think ultimately with Alt DA, as you said, it's like 95 or more percent of the cost of a roll up. And Alta basically makes that free, I think enables a lot more use cases and enables startups to just experiment. And not all of these are going to stay around forever. Startups rate 90% or some wild statistic, 90% of them fail. I think you may see something similar along long tail roll ups, but the fact that if you go to AWS, you spin up an EC two instance, it costs you $20 a month. Ultimately, if you can get the price down to a point where it's kind of risk free to start and just kind of see where it goes, I think that's ultimately the world that conduit wants to live in and what we want to enable. And ultimately I think we'll see a lot more innovation that way. And you know, I'd say we're still kind of not quite there yet. I think Alt da gets us a lot closer to that reality."
    },
    {
        "speaker": "B",
        "text": "Can you, Andrew, give us some perspective of the lowering costs of being a roll up over time? Since when you started conduit and there was a lot of juice left to squeeze, you've squeezed some juice and now roll ups are easier and cheaper to deploy. There's more, more juice left to squeeze. I'm sure give us a sense of where things started, where things have been and where they are recently, and then where they are going. In terms of just like the trend of lowering cost of rollups."
    },
    {
        "speaker": "C",
        "text": "Yeah, for sure. So I think like maybe a year ago when the company was starting, there's no documentation. The code base is changing in very significant ways. And like, not all the software was production ready. And so kind of starting in that environment was very challenging. I think it's allowed us to get expertise in the stacks that we're using and really build production grade, production ready deployments. Ultimately, I think that's the most important thing is understanding the pain points, understanding the vulnerabilities of these stacks where things might go wrong, and then allowing us internally to build solutions for that. I think one good example of that is something that we built internally called conduit elector, which allows for high availability sequencing of op stack chains as well as arbitrum orbit chains. And prior to this, in order to upgrade a chain, you'd have to have downtime. You have to bring down the sequencer, upgrade the code, let it sync up again, bring it back up. Downtime means that people can't use the chain. You're losing money, you're losing revenue. Conduit elector actually allows you to roll out those upgrades with zero downtime, which means that users don't even notice that an upgrade happened. That's also important for things like if a hardware failure happens or something like that, we automatically failover. Versus I think for a lot of op stack chains out there today it's a manual failover, which again means that when it goes down, somebody needs to get paged, they need to wake up, they need to figure out what the issue is, and then they need to actually fail it over. Cutting our teeth on those early problems allowed us to build a best in class solution, that one much better for the reliability of the chain, but also frankly, giving us a lot of insight into the future of things like shared sequencing."
    },
    {
        "speaker": "B",
        "text": "Yeah, one of the interesting things I always just think about conduit and these other rollersbed as the service providers is like I alluded to earlier, this just like an epicenter of many chains. All the chains are approximate to each other inside of a Ras. Could you just share with us what's the Ras business model? The archetypal business model for a Ras. How does a Ras make money? And then what can a Ras do for the web between the chains?"
    },
    {
        "speaker": "C",
        "text": "Yeah, for sure. So I guess in terms of business model, it's typically two components to that. There are the infrastructure and hosting fees, which are you're running a bunch of stuff for the chain, you're running the sequencers, you're running any additional components to sync with layer one, you're running the RPC. Hopefully it's auto scaling versus a fixed set of nodes. That's actually a non trivial thing to solve. And then you have your metrics, you have your alerting, so you have all the stuff to make sure that, that it's production ready and going to be ready for Mainnet."
    },
    {
        "speaker": "B",
        "text": "And is this all just like SaaS stuff? This is just SaaS models."
    },
    {
        "speaker": "C",
        "text": "So that's kind of just like a SaaS model, right. And like I think over time costs will be brought down. And like, I think crypto software hasn't been engineered in a way that makes it easy to do multi tenancy, for example. So like if you look at a lot of traditional web two companies, whether it's like planet scale or like you pick any SaaS, they've kind of built multi tenancy into the model, which means that you get to share a bunch of customers across, across one hardware stack. Today everything is pretty distinct. And what that means is everybody gets their own dedicated capacity. And so that's great for stability and uptime. It does mean that for the longer tail, it's just a bit more expensive to run. And over time, internally we're working on solutions here that will again bring the price down and allow you to scale with your volume. Today, the RPC is somewhat that, but like you can imagine, for sequencing and the main bottleneck being processing these transactions and access to states, you can elastically scale that up to absorb a burst of transactions and then scale that down. Just getting the price to performance ratio automatically correct over time is definitely something that we're keen on. The other aspect of the business model is sequencer fees. As you know, sequencers sell l two gas and buy l one gas. That diff is the sequencer kind of like net revenue. And ultimately, like, most of that goes to the customer. It's their right, right. It's kind of like the revenue model. And then typically, Ras providers take a percentage of this and then role frameworks, for example, optimism and arbitram, or kind of, you know, name your role up here, may also take a kind of a percentage of that as well."
    },
    {
        "speaker": "B",
        "text": "A percentage of that as well. And is that the fee, the 15% optimism fee that base pays to the optimism collective? Is that what you're talking about?"
    },
    {
        "speaker": "C",
        "text": "That's right. So for the super chain, it's 15%, and then I believe it. For arbitram, depending on the license, I think it's around 10%."
    },
    {
        "speaker": "B",
        "text": "Okay. Okay, understood."
    },
    {
        "speaker": "C",
        "text": "These models are obviously kind of evolving over time."
    },
    {
        "speaker": "B",
        "text": "Sure, certainly. Yeah, well, crypto is evolving all over time, isn't it? So the RAs business model is just volume. It's just volume. Right. It's just total transactions. That's really where it comes from."
    },
    {
        "speaker": "C",
        "text": "Yeah, mix of transactions. I mean, like, it really depends on the network. For example, for Zora network, we're probably around 50 50 in terms of hosting fees and, like, sequencer revenue. But also, they have one of the largest because they have so many integrations. We actually just have a ton of load on the RPC endpoint that we don't monetize today. And so in the future, you can imagine, again, us, like, bringing costs down, like, significantly and then being able to charge more granularly, kind of, with the RPC."
    },
    {
        "speaker": "B",
        "text": "Okay, so imagine that there's a chain that's just massive tons of volume. It's a single chain that's doing a ton of volume. Your share of the revenue of that, it probably decreases as a percentage, but it increases in nominal terms. Right. And so maybe some of these smaller, less used, less popular chains, and you guys are taking, like, maybe let's get more of a 50 50 split. But then as these chains get larger, your percentage of your revenue comes down on a per chain basis, but the total, like us dollar revenue goes up. Is that right?"
    },
    {
        "speaker": "C",
        "text": "I think that's one way that it could work. I think ultimately we're still very early in, like, these models and whether or not sequencer revenue is the primary revenue driver for the customer. One example is like, I keep bringing up both Zora network, right? They have a $1 mint fee. And so if you just look at the mint fee and how many mints they have, that's the primary revenue driver. And so sequencer fees are kind of less important for that. And so ultimately it's going to be case by case, based on the customer. And what we want to do is just kind of align incentives and make sure that we're getting value when we're."
    },
    {
        "speaker": "B",
        "text": "Providing value for the customer, case by case, based on the customer, the customer being the chain. That, that sounds hard to scale if we're going to have millions and millions of roll ups, which is what we all kind of want, at least thousands, starting with thousands of. How do you scale out a business model in which each, the economics of each chain has to be negotiated?"
    },
    {
        "speaker": "C",
        "text": "Definitely. So that's where I think today our ethos is do things that don't scale. And in the future, we'll have to figure out a way to wrap back to this and figure out something a bit more scalable. I think our focus today is on just serving as many chains as possible and getting our infra out there. We're less concerned today about particular splits or whatever. We're more interested in growing the market because I think a year ago people didn't know it existed, and now it's taking off in an exciting way. And we just want a ten x or 100 x that kind of over the next year."
    },
    {
        "speaker": "A",
        "text": "And David, one way that I've been thinking at least about conduit from the very beginning is that's almost like the Switzerland of infrastructure. Just deploy things, it's there for you. We will co locate services, happily. We will offer every service that is not a commodity, like for basically cost price, because we want to go where the value is and to ultimately, as Andrew said, grow the pie. Whereas right now, if you look at the infra market, it's a bit embarrassing how much people charge for services that are embarrassingly cheap to run or not that sophisticated to operate. So with conduit, we hope to commoditize all of these areas and to go to where the actual value is."
    },
    {
        "speaker": "B",
        "text": "Andrew, one more question about the economics of a Ras. I've always understood that rasses are kind of in a tug of war with the roll up frameworks. And so the roll up frameworks, the op stacks, the arbitrum orbits, they want their fees, brasses want their fees. And there's kind of like a thumb of war of who gets the fees. How do you think about the relationship between ras and roll up frameworks?"
    },
    {
        "speaker": "C",
        "text": "I think maybe in the long run, if you really analyze it, I think you're right. There probably is some sort of competitive aspect there in the future. I think given the state of the market today, it's so early, it's so small, and there's so much room to grow that I just don't think it really comes up. And ultimately the way that I think about it is we want to work very closely with all the different rolled frameworks and enable the distribution of their software. And I think it's a very different skill doing that than core protocol development of the actual role. I think there's a ton of synergies and complementary skills here. We're very excited to package up what we've done with the op stack and the orbit stack and bring that to new frameworks and new ecosystems. I think ultimately, again, our attitude today is let's grow the pie together versus fight over the small scraps that exist today."
    },
    {
        "speaker": "B",
        "text": "Beautiful. Georgios, I want to turn the conversation to l two security. You brought up multi client fraud proofs earlier, which I believe are the same things as multi provers, but provers are in the ZK sense. Correct me if I'm wrong, I want to start there. There's a lot of focus on rollups being centralized because they don't have shared sequencing. But I actually don't think that that's right. One of the benefits I've always thought is rollups actually get to have centralized computation because of fraud proofs, because of ZK proofs. Maybe you can unpack that a little bit more and explain it better than I can, and then we'll get into the conversation of why multiprovers and multi client fraud proofs are important. But can you just talk about the role of shared sequencing when it comes to decentralization and how important is that?"
    },
    {
        "speaker": "A",
        "text": "Of course. So a roll up is an extension of the mainnet block and a sequencer is just a privileged party that's able to trust them for some things to order and to give us some batching benefits before we extend that mainnet block. Now, right now we only have one sequencer and that one sequencer, maybe they can misbehave, they can misbehave by doing anything they want. They could introduce an invalid state transition, they could omit a transaction, they could do a lot of things to combat that. Their layer two system design basically introduces a proof, or a game, or whatever you want to call it, that the sequencer needs to follow in order to make sure that even though it's one person, that person cannot screw us, which means that they cannot take away funds from us and they cannot censor us. And for that, there is two mechanisms that roll up. One is the fault proof and the validity proof, which ensures that the sequencer cannot steal funds from us. Because if they do, somebody in the optimistic case will come in and challenge them, and that will abort the invalid state transition. Or in the valid debrief world, where the sequencer is responsible for also posting some cryptographic information that says, hey, what I did is actually correct. Now, for the censorship use case, not for the soundness of my funds, but for the censorship use case. Every roll up protocol also comes in with a force inclusion function, which means that if the sequencer dies, stops responding, goes on vacation, whatever, the user can go to the layer one smart contract and their wallet, ideally in the future. Again, this is something that's also in an embarrassing state today. The wallet should basically choose, hey, sequencer is down. Instead of sending the transaction, the sequencer actually send it to the l one, and that will keep things going. Now, we're not in this world, so because we're not in this world, we have multisigs on all sequencer upgrades, and we know who the sequencer is. So until we have the ability to have anyone to be able to submit default proof, and until we have the ability for anyone to spin up a sequencer to continue the chain, if the previous sequencer dies, we're going to stay in with multisigs and it's going to remain centralized. Now, how do we get to this world where anyone can propose fault proofs or anyone can be a sequencer, that is the word of multi privileged, or of whatever you want to call it, a multisig of fault proofs, or multisig of validity proofs. And the idea is that because the fault proofs and the validity proofs are novel technologies, instead of having one of them decide the outcome of a dispute or of a certain operation, why not have, let's say, two or three or more? Why don't we have a quorum? Let's say a two of three quorum? Many configurations will exist. But, for example, in the optimism context, we're going to have one fault proof that runs on mips, built on Geth. We will also have another fault proof that, again runs on mips, but built on ref. And we'll have a third proof that is built on risk zero and is actually validity proof. And we will only allow a withdrawal to go through if two of three of them agree on the outcome. Now why would we do that? Because it gives us more redundancy, and with more redundancy, we gain more confidence in the security of the system aggregate. And by doing that, we can finally reach the so called stage two decentralization in roll ups, which will let us remove the training wheels. I think this will happen in 2024."
    },
    {
        "speaker": "B",
        "text": "Oh, wow. This year, this has been like one of the big things that's been holding layer two roll ups back in terms of just their full decentralization and trustlessness. Right. And the way that I kind of think about this is like it's just the multi client design for Ethereum that has protected Ethereum so many times now doing the same things for layer twos, but inside of the fraud proof context. And so if there's one bug in one fraud proof, or one bug in an optimism client, all of a sudden that's catastrophic. But if we have multiple client fraud proofs, then all of a sudden we have that redundancy. 99% uptime and full decentralization."
    },
    {
        "speaker": "A",
        "text": "Exactly."
    },
    {
        "speaker": "B",
        "text": "I've always thought that this is like the most complicated thing about building decentralization into layer twos. You said that you think that we're getting these as like, you know, available for the main roll up standards in 2024. What makes you so confident that we're getting them in this year?"
    },
    {
        "speaker": "A",
        "text": "That would be my prediction in particular, because we have one MIPs fault proof already on OPJs that optimism has developed and has on Testnet, and there's a work in progress on the RET side to incorporate this as part of the Op rest project. So, you know, one might ask, why are we doing the op rest project? Not just because we want the high performance, but because we thought that it would heavily accelerate the stage two decentralization for rollups. I think this is also the first time that we talk about this in public. But yeah, in general, we're very excited about using the, let's say, SDK RET software stack as a general sail around for the entire EVM ecosystem, whether it is performance, whether it is indexers, rpcs, or the decentralization of layer two."
    },
    {
        "speaker": "B",
        "text": "And maybe just to tie this back to an earlier conversation, if you are an op stack chain, and these multi client fraud proofs get delivered, then everyone gets to upgrade all at once because we're all on the same standard. If you're an op stack chain is why. It's one of the benefits of being with the herd."
    },
    {
        "speaker": "A",
        "text": "You're being aggregated. Yeah, exactly."
    },
    {
        "speaker": "B",
        "text": "Cool. Okay, so that's the big thing about layer two security. What other conversations are relevant, would you say, Georgios, in the layer two security conversation? Or is that kind of just like the big one?"
    },
    {
        "speaker": "A",
        "text": "I think the modification story will be interesting. Andrew touched on it a bit earlier. You know, when you give people the ability to experiment, they will do crazy things, both in the good and in the bad sense. So I think that people will experiment with, like, things that are, you know, in general, like interesting ideas. For example, native yield rebasing, coin or fee sharing or whatever. But who knows if people will be able to implement these soundly, you know, will there be a library of plugins? Let's say that people are excited to interact with, and these are the whitelisted ones or the, you know, the conduit app store offered ones. I honestly do not know. People will probably go through a big pirate phase where people will change the gas token to pay, people will change the runtime. You know, there will be l two s that are not evm. L two s. We're already seeing that with ellipsis. We're seeing that with eclipse. We want to see it with like the move layer two. We're going to see it with every runtime that exists."
    },
    {
        "speaker": "B",
        "text": "A runtime, is that just another word for a virtual machine?"
    },
    {
        "speaker": "A",
        "text": "Yes, yes, yes. Sorry. So the runtime and the virtual machine are equivalently used here. So it could be evm, svm, move, vm, brain fog for all I know. Like, whatever you want, it doesn't matter. So there's many, many choices to experiment on the runtime, on the transaction format. People will come up with novel account abstraction techniques, people will come up with privacy systems, all sorts of things. I think we're entering an era, David, where nodes have been thought to be like a hard thing to ship or to work on or whatever, you know, because they require people to learn about databases, about peer to peer, about each runtime. They require a lot, a lot of knowledge. I think we're entering an era where the node modifications become so, so, so easy that it will open up an era of experimentation in blockchain that we didn't see before, which is very exciting for everyone working on infrastructure."
    },
    {
        "speaker": "B",
        "text": "I think the state of layer two development and progress over the last few years has really been about minimizing the diff between Ethereum and layer two s. The fight for Ethereum compatibility evolved into the fight for ethereum equivalents. And as a result, we have a lot of very popular layer twos that are just carbon copies of Ethereum, but faster. And I think maybe, George, what you're saying is that not only have we neglected non ethereum versions of roll ups that can settle on Ethereum, but also the technical difficulties of building these systems has also been easier than ever. Is that kind of the groundwork that you're laying?"
    },
    {
        "speaker": "A",
        "text": "In general, my prediction would be that 2024 is the year where performance, you know, we end the year, basically, and performance stops being a differentiator. Everybody will have figured out high performance. We will be in a world where parallel evms that we've seen discussed a lot, new databases, all of that stuff. Finally, now that we've learned how to modify nodes, everybody will like, figure out in some way, you know, not everybody will be in production, few will be in production. The best teams only. But over the years, what's happening is that the best technology that was considered a moat is finally starting to be, for lack of better work, democratized and accessed by everyone. And when these performance things stop being like the differentiators that people go for, we're entering the next area of innovation, which is the UX, the account abstractions, the signing algorithms, the gasp on sponsorships, everything that has been kind of like in toy mode in the last year, which it's grown a lot, you know, but it's still small account abstraction and the like compared to, let's say, where metamask is in terms of adoption. All of these, every chain will start to experiment so much with the feature they're able to offer that somebody will hit gold on some unique feature that enables some unique app. So that's why the layer two vision is also very exciting. They've. Because it is what allows you to start deploying chains on conduit or anywhere else, these chains that will be modified ad nozium, and we'll see a lot of nonsense in the process. A lot. A lot of nonsense we will see in the process. But somebody will hit gold, and that somebody will make something very valuable."
    },
    {
        "speaker": "B",
        "text": "Andrew, it sounds like Georgios is calling for a golden age of layer two experimentation, which I think if I were in your shoes, I'd be maybe a little bit intimidated, because I think conduit is all about, like, how can we replicate the same stuff over and over and over again. But when there's a bunch of new stuff, all of a sudden there's new things to replicate. So if there's going to be like a bunch of new pieces of software that you have to support, how do you think about this? If this is the, what we're going into in 2024 and all of a sudden it's not just the OPSAC and arbitrum orbits, but it's like the third thing and the fourth thing and the fifth thing and then the 6th thing just around the corner. How do you think about this?"
    },
    {
        "speaker": "C",
        "text": "Yeah, for sure. I think that's a great question. I think ultimately depends on the form factor of the customization, and it is something that we think about here at conduit. It's like, what role of stacks do we support today? We support orbit and optimism. What is the next one then, in terms of customization, how does that actually make its way into the stack? For example, if it's just an execution client change on the op stack, or the arbitrary, morbid side, that is actually pretty workable, and we support that today. You can actually have minimal modifications to our infrastructure. Everything just works, and then we just slot in your custom execution client. I think the more custom you go in terms of the entirety of the stack, that is essentially writing a new roll up framework. If you look at Starkware, if you look at any of these other things, I guess the most similar thing would be a Cosmos app chain where you take this base SDK, but then you customize a bunch of stuff. Ultimately, time will tell how important that model is and whether or not you need to reinvent the wheel or you just want to reuse the existing pieces of the stack and just customize what's important to you. And so jury's still out. I think we want to support everything that we can, and ultimately, it's kind of our challenge to figure out the kind of best way to make that scalable."
    },
    {
        "speaker": "B",
        "text": "Andrew Georgios, I've learned a ton in this episode. I'm going to have to go back and relisten to this to make sure I got everything. We're entering what seems to be a bull market. Bull markets get very, very busy. They can also be very distracting. But Andrew, when we are done with this podcast episode and you go back to work at Conduit, what are you going to focus on? What is the nearest term thing that you are focusing on right now?"
    },
    {
        "speaker": "C",
        "text": "Yeah, I mean, I'd say that given the gas fee kind of spiking in December and, you know, slash becoming production ready and conduit supporting it. Our biggest task is really migrating a bunch of chains to da. I think our next goal after that is making, you know, blockchains and roll ups as accessible as like using AWS. And so I think today, you know, we offer a self serve kind of testnet API. I think our question is like, what happens when we make that accessible and permissionless for anybody to deploy to mainnet? And what kind of cambrian explosion does that enable?"
    },
    {
        "speaker": "B",
        "text": "Giorgio, same question to you."
    },
    {
        "speaker": "A",
        "text": "So at the top level, I'm excited to continue working with great people like Andrew and others who are working to push on the limits of what people think is hard in making it really, really easy. Personally, David Est just wrote a gold doc called what is ten x rath in 2020 2024. So I hope to join our team into pushing the limits of performance and commoditize the ten k TPS in 2024. I'm excited for the ref SDK to be used to build new roll ups, to build new experiments, and to in the end push forward the layer two industry. And I'm also finally excited about the rest core, let's say roadmap in 2024, the upcoming Cancun 4844 release, and finally by end of March to be 1.0 production ready, ready to support Ethereum layer one audited by Sigma prime going ham in the year and pushing the frontier really hard."
    },
    {
        "speaker": "B",
        "text": "Going ham in the year pushing the frontier. I love that Deutsche asks, why does the world need rest? What is Reth and why does the world need it?"
    },
    {
        "speaker": "A",
        "text": "Reth is an ethereum node compared to Geth, Aragon, Netherminden. It is written in rust and it's a modular and blazing fast node that is also contributor friendly. What this means is that while it achieves best in class performance on most bench forex that matter, it is also aimed at developers. It has a developer community and is built to be used first and foremost as a library. And we see Reth almost like as a testbed for building EVM native infrastructure. And we want to give to the world access to these high quality tools that we've been using over the years with the node as the first demonstration of how good these tools can be and hopefully with other infrastructure built on top of it. Like Op Ref, like the op fault proof, like a bunch of infrastructure that conduit will be running in the future and other portfolio companies. We really want to push the frontier of what is hard. That should be really easy on infrastructure and we're going to commoditize with it everything that we think is currently hard or perceived as hard."
    },
    {
        "speaker": "B",
        "text": "Well, thank you, George Rose, for doing Hero's work, speeding up Ethereum and helping with its decentralization, both on the layer one and on the layer two is what I would call very noble work. So thank you both for coming on bankless and sharing us with your perspectives. And what you guys are building is making me very optimistic for the world of layer two twos in 2024."
    },
    {
        "speaker": "A",
        "text": "Big gear ahead. David, thank you so much."
    },
    {
        "speaker": "C",
        "text": "Thanks for having us."
    },
    {
        "speaker": "B",
        "text": "Bankless nation, you know the deal. Crypto is risky. Layer twos are risky. Hopefully they're becoming less risky. That's the plan. At least you can lose what you put in. But we're headed west. This is the frontier. It's not for everyone. But we are glad you are with us on the bankless journey. Thanks a lot."
    }
]