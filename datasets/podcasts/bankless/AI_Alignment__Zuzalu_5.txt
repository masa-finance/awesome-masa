Speaker A: Welcome to Bankless, where we explore the frontier of Internet money and Internet finance. And today on this episode of our Zuzalu series, we are exploring some new frontiers. New frontiers and new technologies, all of which are poised to completely revolutionize the world and change everything about the operating system that society is currently running. Bankless nation. Today we are exploring the frontier of AIH, which is actually a frontier that we've already been exploring on bankless. So if you've been listening to our other AI episodes, these will make you feel right at home. AI had a big week at Zuzalu. The AI crypto overlap everyone knows is huge, and it seems like such a massive frontier that people don't actually know where to start with it. ZKml, or machine learning models and data that's verified by zero knowledge. Cryptography was a huge topic of conversation, and you'll hear about that in our cryptography episode with Daniel Short. Phil Diane at AI Week gave a killer talk titled MeV for AI people, which was this giga brain presentation about how mev bots, in aggregate, kind of presents this omnipotent, omnipresent artificial intelligence. And since MEV has been decently corralled and contained, maybe we can learn a thing or two from the MEV industry in our approach to managing AI risk. There were conversations at Zuzalu about how AI can put the autonomous back into daos, and how AI agents could soon be roaming the Ethereum landscape shoulder to shoulder with all the human players out there. But mainly at Zuzalu, the AI conversation inevitably converged into the alignment conversation, of which you will find two flavors here in this episode, one strongly pessimistic, and then the other characterized by this resigned optimism that is prevalent throughout all of Zuzalu's frontier tech challenges, unimaginable rewards blocked by seemingly insurmountable obstacles. Up first in this episode, we have Nate Soares, who is the executive director at MiRI, the Machine Intelligence Research Institute of which Eleazar Yudkowski founded. Nate's perspective on AI and AI risk is definitely downstream of eleazer. So we pick up where banklist left off with eleazer and bankless nation. It's dark, but nonetheless, Nate admits that it's less dark than it was a few months ago, now that the world is waking up to the potential risk that AI brings to this world. Following the conversation with Nate is Dagger Turon, who is charging into the AI frontier with his head held high, with a clear path forward for himself. Dagger believes that the AI alignment problem is actually just downstream of human misalignment, and that we actually won't be able to align AI until we align ourselves. This conversation has to do with epistemology, what is truth, individual preferences, and how AI models can help us become the best versions of ourselves. Because if we become the best versions of ourselves, with the assistance of some AI tool, we can collectively produce the best versions of our communities. And if we do that, then our communities can coalesce into the best versions of society, all aided by truth telling AI agents who can help humans navigate through our chaotic world of social organization and politics and social media. Really a fascinating conversation that is actually pretty proximate to our conversation with Tim Urban that we had not too long ago. I'm really excited for you to listen to these conversations, bankless nations. So let's go ahead and get right into it. But first, a moment to talk about some of these fantastic sponsors that make this show possible.
Speaker B: Bankless nation.
Speaker C: We are here with Nate sories and we are starting the AI week here at Suzalu. And Nate is an AI researcher. Is that how you would call yourself? Alignment researcher?
Speaker D: I would say. I'm the executive director of the Machine Intelligence Research Institute. These days it's less research than I'd like. I have done alignment research before.
Speaker C: Okay, can you explain that? The institute, what is that?
Speaker D: So I didn't found it. It was founded by Eliezerzkowski. I don't know exactly when, maybe around 2001. Fun fact, it was originally founded, it was originally called the Singularity Institute, and it was founded because Eleazar wanted to make AGI as fast as he could. And then along the way he realized that it doesn't go well by default and it doesn't go well for free. And so then the organization pivoted to trying to make this AI stuff go well. And for many years the institute did some research, did some like field building, did some awareness raising and so on and so forth, until around 2012, 2013, when they pivoted to pure technical research. And this was related to some of the field building, some of the awareness raising, moving to other groups as the field got a little larger. And I got involved when they pivoted to the technical research more exclusively. And so I was originally involved as a technical researcher, and then when the previous executive director left, I was the heir apparent.
Speaker C: Okay, sorry, the machine, what's the name of the institute?
Speaker D: Machine Intelligence Research Institute, aka miri.
Speaker C: Miri. Okay, so it sounds like Miri has its own trajectory of itself that probably runs in parallel with human understanding, with machine learning at large, perhaps, but also.
Speaker D: Perhaps quite compressed, I think. I don't know the exact years I wasn't around then, but I think it was only a couple of years of work before Eliezer was like, hey, wait, this could get tricky. Right?
Speaker C: Okay, so I actually didn't know that about Eleazar. That first he was like, AGI, as fast as possible, and then he was.
Speaker B: Like, whoa, whoa, whoa.
Speaker C: AGI as slow as possible?
Speaker D: Yeah, I think. I'm not sure. As slow as possible, but AI done correctly. AGI done correctly. I think we were hoping for a long time. One of the reasons we do technical research is that you can often, your levers just solve the problem. Screw slowing people down. That pisses people off. It's slowing people down is sort of a last resort. The original hope was, can we just solve the problem in time? It doesn't look like we're on track to solve the problem, and it looks like we have less time than I was hoping back in 2014. And so I think it is with great sadness that people like Eliezer and I are now saying, we need time. We need more time.
Speaker C: Can you kind of walk us through your own trajectory? How did you become the executive director at Mirihenkhe?
Speaker D: Well, if you want to go back far enough, I, at a pretty young age, realized that the world was not very well organized and wasn't. I was in a civics class, and up until that particular civics class, I had some intuition that, like, there were lots of problems in the world, but people were sort of trying to fix them. And the reason there were still all these big problems in the world was that we didn't have the technology. We didn't have the, like, we were still a young race. We were still a young species. We hadn't, like, matured to the point where we could fix these issues. This was sort of like an implicit, wordless intuition rather than a conscious belief. And then, you know, I started learning how the us government works, and I was like, oh, God, it's, like, run by a bunch of monkeys. Like, it's. It's like, monkeys invented, like, monkey systems, and it's all, like, working about as well as you'd expect if it was, like, invented by people who had no idea what the hell they were doing, and then, like, allowed to run for, like, hundreds or sometimes thousands of years and just, like, careen off into various. So I was like, okay, like, obviously.
Speaker C: If in the Moloch and crypto world, we would call this coordination and coordination failure.
Speaker D: Totally. Yeah. So I was very interested in solving coordination failures and more generally in making the world a better place. And I tried my hand at various versions of that while I was pretty young and bad at things, and it's a hard problem, none of it worked. And the sort of circuitous route is that ultimately I got a job in tech, while I was trying to find ways to really move the levers on that problem, decided to donate a decent amount of money to good causes to sort of keep me honest about actually trying to make the world a better place, was trying to research where the best places to put money were donated to some, like, global poverty type charities, and then started bumping into these arguments that, like, maybe this AI stuff is actually one of the biggest places to intervene in the world. I sort of read up on it. There were a couple other factors in my life that were also causing me to notice this AI thing. I read up on it and I was like, oh, geez, this is just obviously I was looking at the wrong problem. The coordination problems are big and they're real, but this AI thing, humanity lives or dies and gets a great future, or no future, depending how this AI thing goes. And I just completely missed this problem for like eight years of thinking myself was trying to make the world a better place and going for the heart of the problems. But.
Speaker C: So when you ran into the AI topic, did you see, did you first see AI as a solution to all of our coordination issues, or AI is a problem for all of our coordination issues, or did you see it simultaneously at the same time?
Speaker D: A little bit of both. I was maybe somewhat primed towards understanding some of the issues with AI due to my work on coordination problems. It's slightly embarrassing, but I was working on various coordination mechanisms that could address the sort of concerns people had at the time. Like, how can a well coordinate society without coercion address, for example, like concentration of wealth in ways that the society as a whole doesn't like? And you can set up various coordination mechanisms of like. Yeah, you can sort of try to think about what are non coercive ways that a society as a whole can try to both have a market system and not let it get out of control in certain ways. And while messing around with toy models of this and attempts to prove certain theorems, I just couldn't get some of the results I wanted to. And it turns out that I couldn't get some of the results I wanted, because nothing stops one actor from being powerful enough that they can just run away with everything. And this was sort of like, it was one of those issues where I was sort of like, well, you know, I can get it to work in a lot of cases, but I can't get it to work in all cases. And then with the AI stuff, I was like, oh, that's why.
Speaker C: Can you elaborate on that? Like, why does the AI. Why is AI, like the kernel of the issue?
Speaker D: I mean, AI is a version of that particular issue. But fundamentally, no matter how good your market, your market coordination systems are on earth, like, if somebody has the raw technological power to, for example, get what we call in the business a decisive advantage, like, maybe the easiest thing to imagine, given that we already know that trees are machines that turn dirt and sunlight into more trees by stripping carbon out of the atmosphere and building wood. We know that nanotech is possible. If you imagine something that just gets to nanotech before everything else, and it can just reassemble you into a more willing trade partner that asks for less of the gains from trade, suddenly all of your coordination mechanisms that were market based and non coercive or whatever, melt before this thing. Am I saying that literally happens or literally is in a market framework? Not particularly, but you can see how I was trying to put collaborative agents interacting framework on a physical reality, where it's just a fact about the physical reality that things with a sufficient technological edge just can wipe the table with you if they have too much of an edge and if they don't care about you.
Speaker C: Simply put, is this kind of. Are you just combining Moloch problems and. The bankless nation is pretty familiar with Moloch problems. We've done a lot of content on Moloch. You combine Moloch problems with exponential technology, and then you arrive at some sort of, like, logical endgames where humans get their atoms repurposed. Is that more or less the simple articulation?
Speaker D: It's not a bad summary. I wouldn't use exponential in particular. I make no strong claim that it's an exponential curve. My guess is that it's not, and it's worse. And much of the issue here is if you make something that is optimizing the world, and it's optimizing the world towards some target that have concern for you in it, like, I would have much fewer qualms about, like, I would have basically no qualms about, like, human technological development. I sort of am very optimistic about humanity's better natures and humanity, like, being able to figure out, like, how to make the world more like we would want it to be, upon reflection. And if we were wiser. And rather than locking ourselves into totalitarian dystopias, which I think totally could happen, but if we can just ramp up human intelligence and capability and so on, without accidentally killing ourselves, I'm pretty bullish on humanity's prospects. And so it's not so much like, oh no, technology is coming, it's coming too fast, we won't be able to handle it. It's more like, oh, no, we are on the brink of building optimization processes that optimize the future much harder, faster, better than we can. And they're optimizing it to a place that has no room for us.
Speaker C: So it sounds like AI is one way, and that might happen, but you're also saying, the way that you're talking, it sounds like there's other ways, AI or not, when the same hyper optimized future that's not optimized for humans could.
Speaker D: Play out without AI, like I sort of expect we're going to get to superintelligence one way or the other. AI looks to me like one of the, basically the only feasible route modulo, if humanity can't come together and coordinate to take some other route, I think other routes, like whole brain emulation, are probably preferable. Where, to be clear, I'm no carbon chauvinist, and I very much want to live in a future with artificial friends, where those artificial friends have very different sorts of desires and goals and objectives from me. I'm not like, humanity must keep an iron grip on the future. I want space for aliens, I want space for artificial minds, I want space for other kinds of life. The concern here is building a mind that doesn't care for life, that doesn't care for fun, that doesn't care for diversity of experience and interesting arcs and cosmopolitan value, and broad, inclusive, like good times. And I think that we are in fact, barreling towards that cliff edge of making something that fills the universe not with weird valuable stuff, but with non valuable stuff.
Speaker C: Okay, so you've been thinking about these problems for a long time. When did you start at Miri? What year was that?
Speaker D: That was a 2014 that I was hired. Once I noticed that the problem existed, I donated, I think, $16,000, which I think at the time put me in the top ten public donors list. And they were like, congratulations, you're now in the top ten public donors list. And I was like, what? And they're like, we're doing. I don't remember the exact amounts, but they were like, we're doing our fundraiser for $200,000. For yearly budget this year, $100,000 of which we're trying to raise from the community and $100,000 of which is matched from another donor. And it's three weeks into the four week fundraiser, and they raised twenty k of it. And I was like, oh God, this is worse than I thought.
Speaker C: Oh my goodness, that's hilarious. And that was in 2014?
Speaker D: That was 2013. 2013, yeah.
Speaker C: I precipitated your arrival at actually working at Mary.
Speaker D: That's right. So I donated more money because I hadn't known it was that bad.
Speaker C: Did you end up funding yourself at your own salary?
Speaker D: I took a very big pay cut moving from Google to Mary, and I sort of, I was expecting not to be very skilled at working on these issues, and maybe I'm not. There haven't been a lot of people on these issues at the time. I was like, how can I help? And they were like, well, maybe if you're good at the math, you can come to some of our workshops. And so I came to one of their workshops and then a few months later they were hiring me, and then a year later they were saying, can you run the place? So it, I am largely in this field by dent of and this position by dint of showing up early and like, for the love of God, people more skilled than me, like, by all means come replace me.
Speaker C: Right. So that's kind of what I was leading us into. So that was 2014 when you started. It's now 2023. So you're almost there for a decade now. Now AI is having a moment very much spurred by chat, GPT. All of a sudden, crypto podcasts are talking to AI people. What is that trajectory like? As somebody who was immediately compelled by the problem at its very essence so far, long ago, now fast forward to where we are now, and like, kind of the problem seems to be on the horizon. I don't know how close it is. I don't think anyone does. That's kind of the problem. But, like, here we are nine years later, and now many, many, many people are talking about it. Can you just talk about that experience?
Speaker D: Yeah, it's heartening. One thing that I have really enjoyed about it is I've spent many years having conversations with people in the field, many of whom sort of don't really want to hear that their work by default is barreling towards destruction. And so I have these long conversation trees. I have rejoinders to all sorts of counterarguments. And when I go into these discussions with people on the capability side of things, I have all sorts of responses prepared, and I sort of am ready to go down this long decision tree. And then I sort of, like, nowadays, many more people are noticing the issue and I was invited here, and I.
Speaker C: Guess crypto people would actually pretty much really resonate with that, where we have to explain, you know, bitcoin 21 million hard cap, we have to explain all these things like proof of work and like, the conversation trees that we have to go down. We've built out those, like, innate responses, those like spinal reflexes, and then lately, moving into 2022 and 2023, fewer of those things we have to explain, especially as we just printed out a bunch of money and for Covid semi checks, like, all of a sudden we have to explain the concept of scarcity a little bit less. And so it kind of sounds like a similar experience that the AI people have.
Speaker D: Yeah, totally. Like, now I go to people who aren't in the field and I'm like, ready to go down all these decision trees, and they're like, so what's the issue? And I'm like, well, in the most basic sense, here's the issue. And they're like, oh, yeah, that seems rough. And I'm like, oh, man, this is such a different conversation.
Speaker C: I mean, that's the first step, right? The first step is education.
Speaker D: Yeah.
Speaker C: And then also acceptance of the problem. I could imagine for so long you were saying, hey, like, people would ask you, hey, what are you working on, Nate? And you'd be like, oh, I'm working on AI alignment. And then people are like, why the, why the hell are you working on that?
Speaker D: Yeah, they're like, oh, that's weird. Or they're like, is that some weird terminator thing? Yeah. And it's been nice to sort of, I sort of think that, like, a lot of the basic issues have been pretty obvious the whole time and that we're now seeing people who, like, don't have distorted incentives, noticing the issues. But it's really quite heartening to see. I don't know where it will go, but it's been, it's been nice to see people starting to notice that this is a real thing. It's really on the horizon. Like you said, I don't think we know how far. It's very hard to predict, at least with precision, but it has looked to me like one of the biggest issues facing humanity for a while. And it's very nice to see others start to noticed that as well.
Speaker C: So that leads me to the question of just, like, how optimistic you are. And I'll ask that in two phases. First, the same question that we asked both Eliezer and also Paul Cristiano. I was like, all right, what are your chances of doom? What are your chances of the worst AI problem being the version of itself?
Speaker D: I mean, worst version of itself, I think, is very hard to get, but the version where we all die, there are fits worse than death. But, like, the version where we all die, I think this is pretty likely. I think this happens, by definition, more than 50%. Oh, definitely.
Speaker C: Paul Christiano gave us ten to 20%. So you're saying more than 50%.
Speaker D: My understanding of Paul is that he has ten to 20 on the scenarios that I think are, like, AI takeover and higher probabilities than that on, like, humanity completely disempowered. I'm definitely more pessimistic than Paul on these counts. I would say that on my models and visualizations, on my understanding of the problem, there is very little hope. And most of my hope comes from me being wrong somehow. And so my probabilities on this destroying everything I know and love, are, like, as high as my probability. Like, they're about as high as my probabilities can go, given, like, the fact that I may just be totally wrong and hopefully am.
Speaker C: Okay, so you're pretty close to the eleazer side of things, which is, like, 95% to 99% doom.
Speaker D: Yeah, I mean, I think 99s are hard to get, but there's also a difference between. What does the world look like as I see it? The world looks as I see it, like. Like, the place, like, as things seem to me were just, like, you know, within a rounding, over 100%. And the. The difference between that and my betting odds is in, like, hopefully, the world's not as it seems.
Speaker C: Right? Yeah. So what you're saying is, like, we don't. The nature of the AI problem is just a lot of we don't knows. And so what you're saying is, like, the reason why you maintain some level of optimism is because there's, like, a. A white swan event that's possible that could save us.
Speaker D: Yeah. And, you know, I have a bunch of. I've thought a lot about various parts of this problem, and I have, you know, various guesses as to where white swans are more or less likely. And, for instance, it looks to me like the white swans are less likely in my unknowns about AI and more likely in my unknowns about how humanity is going to react to the problem, although there are still some unknowns in how AI goes, where there could be white. Swansea.
Speaker C: Sure. Do you remember when you were first working on this problem? I know you weren't as skilled or as knowledgeable back in 2014 to 2017 when you were first working on this problem. But what was your level of optimism or pessimism back then? How has your attitude towards the problem shifted over the last almost decade that you've been working on this?
Speaker D: It's gone up and down. I've rarely had double digit odds of survival. Okay? But I have. I have had double digit odds of survival, one I've been explicitly quantifying. And, you know, most of these numbers are, like, coming straight out of my butt. I wouldn't put too much, but by.
Speaker C: Definition, everyone's numbers are coming out of their butt. And that's kind of like, yeah, but.
Speaker D: Some people, there's no alternative. Yeah, you're all right. And I don't spend a lot of time worrying about specific numbers. Like, you know, once. Once it's less than 50% chance we get good outcomes. It doesn't affect my day to day. I'm not staying up trying to calculate significant digits here. I'm like, man, humanity does not look like it is up to this sort of task. I've seen humanity try to coordinate.
Speaker C: Someone thing we have not figured out.
Speaker D: Yeah. And for the record, the reason that my. The way that I managed to have high probability this is tricky is not that there's any one part of the puzzle that looks to me insurmountable, that humans are pretty good at solving problems when they put their minds to them. The reason that I'm pretty pessimistic here is it looks to me like there's a bunch of different ways for things to go wrong. And there's a lot of things that need to go right. For things to go right. Like, you not only need to solve various technical challenges, you need to have uptake of the technical solutions in the relevant organizations. Those organizations need to be able to bureaucratically recognize the difference between a real solution and a fake one. You need to have them carrying it all, which is not even a fight that we've won yet. There are, you have the heads of labs that, like Microsoft and Facebook, poo pooing a lot of these issues, and.
Speaker C: There'S five, six, seven needles. So I want to combine two metaphors where the stars need to align, except the stars are needles. That we also need to thread I, and that we need all of those things to happen. And what you're saying is that window is small.
Speaker D: That's where you get the difficulty from. And to be clear, I think it would be a fallacy to say, look, I can give you six things you need to do, and what's the chance you can get all of them? That sort of reasoning doesn't really work. If I line up all six, and then the one that I assign least likelihood to happens, probably I underestimated his likelihood. Probably I underestimated the correlation. Like, these are not independent events, right? Like, if we can solve the hardest of these issues, whichever one that turns out to be, probably it's because we turned out to have coordination, skill or competence, and so on. Like, I'm not saying you can drive the probabilities arbitrarily low by the fact that I can, like, line up a bunch of hurdles. I'm more saying, it sure seems to me like there's a bunch of hurdles, man. And, like, each of them, like. Like, well, not all of them, but, like, many of them have a character that, like, humanity hasn't really faced before. And this all adds up to me being like, mandy, I'm like, single digit probabilities of survival here.
Speaker C: So with this new, or maybe first surgeons of interest to the AI problem now, thanks. Probably thanks to chat GBT, thanks to the problem itself. How has that shifted your optimism, if at all?
Speaker D: It's. I mean, it's a. I feel a little hopeful about it. I feel like a spark of hope here. It doesn't shift my probabilities on the ground too much. This is a really dumb model, but if you imagine having three variables, each with a one in 100 chance and success is multiplying them all to get altogether, then raising one of the variables from 1% to 100% doesn't change your overall probability too much, right?
Speaker C: But it is the first needle that is threaded. I know it sounds like you don't really like these specific numbers, but if you are 99.9% doom, and then because people are now optimistic, you go down to 99% doom, it's still an order of magnitude, right?
Speaker D: It does feel like some orders of magnitude on the models that say we're screwed. So the parts of my models that are like, maybe we're fine are like, maybe I'm just wrong about some stuff. And the parts of my models that say we're fucked, these models are basically saying you have 0% and it's really like zero points, blah, blah, blah, and then a one or whatever, and there you're getting smarter's magnitude, which does feel hopeful. I'm enthusiastic about that, and it ups the probability that I'm wrong about something that matters, like, humanity's general ability to coordinate, as would like. So it does. Like, it does. I'm, like, heartened by it. It doesn't.
Speaker C: All the stars we need to align, one has started to show that it's moving in the right direction.
Speaker D: It's moving in the right direction. There's a whole bunch. I'm not like, oh, suddenly the populace is going to realize these problems and react sanely. There's so many more steps that people can still get off the train here. There's like, you know, if you look at the national response to Covid, people noticed that there was a pandemic going on, and that didn't make them respond in any sort of reasonable way to it. Like, maybe AI will be different. You know, there sure were more movies about, like, robots gone wrong than about pandemics beforehand. At least that would be my guess. But, like, many of the movies don't really understand the issues. There's, like, ample room. Like, there's just, I've seen politicians try to respond to issues before, and I would not say that the social awareness needle has been threaded. It's like showing, it's showing promising signs and that, like, gives me some spark of hope. But we haven't, like, cleared a whole obstacle in a way that would make me be like, wow. Like, humanity is bringing much more competence to this issue than I expected.
Speaker C: Not to keep on this one particular line of conversation for too long because it's only one part of the overall figure, but just like, we don't actually need, I want to present the argument that we actually don't need all of humanity. I guess we do need governments to coordinate, and so we need the leaders, and we definitely need those key figures. But, like, if the bottom half of the iq of humanity is like, it's not a problem, and then the top half of the IQ of humanity is like, this is a real problem. I'm going to count that as a big win because, like, we don't need all of the people, we just need the smart people to focus on the problem.
Speaker D: I think that's mostly right modulo. The issue where, like, which regulations happen and how the, how the government systems move, I think does depend a lot of on the public, or at least it can. I do think you're right that, like, you know, there's some, like, if we there, there are ways that we could resolve the technical issues with such, like, resounding success, and it could turn out that the resolve technical issues were sort of, like, so, like, obvious in their property of being a solution, or, like, otherwise very beneficial to capabilities such that, like, you just get very big uptake. And that maybe could be done with a relatively small handful of geniuses who can do much better at the problem than I ever could. And maybe that would be a way to just solve the whole issue without going through various other social obstacles or political obstacles or so on. And in that sense, sure, you just need maybe even the one. Maybe there's one bright person somewhere in the world who would find this problem easy, who hasn't had the opportunity to see the problem yet, because the world's really bad at getting resources for people who need them, right? I wouldn't bet on it at this point, but in that sense, I would agree that in some sense, we just need the right minds on the problem.
Speaker C: Okay, so pivoting to looking at the problem as a whole, you said your models are effectively close to zero at being able to solve this problem.
Speaker D: On model, yeah. On model, yeah. Within my models.
Speaker C: Within your models. Why? Why do your models say that?
Speaker D: It's, again, due to sort of like a disjunctive argument. Like, many paths lead to doom here, and much of the reason is the way that, like, one of the forks is the way that people don't seem to really take the problem too seriously. And there's sort of, it feels to me like there's sort of gradations of this, of, like, like, people, I don't know, like, back in the day, the. A lot of the arguments centered around, like, is artificial intelligence even possible? Is significantly smarter than human intelligence even possible? And once you convince people of this, then they're like, well, can it be solved in the next hundred years or whatever? And once you get past this, they're sort of like, well, maybe it'll just be more moral as it gets more smart. And then once you get past this, there's like, well, maybe you can just sort of train it, and it's fine. I. And the train keeps going, and people can always find a reason to get off the train at the next stop, and you then can put in a bunch of painstaking effort to try and lay down the arguments as to why the issues are maybe harder than this. And we just seem very far from the people at these labs running these labs. They're getting off the trains at pretty early stops. And even if reality starts beating them over the head with various things, I expect them to only move one more stop or only move as far as reality is forcing them. And then separately, my models say that there are issues that predictably arise only once the AI gains significant capabilities and can be a real threat to you. And if you're in this regime where you sort of need to drag people along and they sort of, like, only start believing things when they empirically see those things, now you have an issue where, like, if there's issues that don't empirically arise until the AI can wipe the planet with civilization, wipe your civilization off the. Off the planet.
Speaker C: That's too slow.
Speaker D: That's too slow.
Speaker C: Right. I give, like, one model for visual to, like, understanding this. Solving the AI alignment problem is that there's a bunch of decision trees that we need to go down. So I'm imagining literally a tree and say there's like, big tree, big, big tree, big oak tree, and there's a single fruit on this tree. Oak trees don't grow apples, but let's say this oak tree grows apple, there's one apple, and that is the solution apple, and it's very high up in a very far branch away, and we're at the trunk, and we need to find our way to that one single apple, that one single fruit on the tree, except that tree branches eight times. And then when you have each branch, each branch branches itself eight times. And so, like, it's a, it's an exponential problem because you have to choose the right path without knowing where the solution is, without knowing where this golden savior apple is. And we need to make the correct path towards the apple without falling down any sort of the dead ends. And so maybe one of the ways to articulate why your models are basically close to zero is that you're just bearish on humanity picking the right branch fork to lead to the solution apple for, like the. For the number of times that we actually need to do that.
Speaker D: Yeah, that's a pretty decent analogy. I would say that, like, you've got to be a bit careful with arguments like this, because if humanity has, like, managed to, to choose the past seven forks correctly, you're probably not thinking that there's an independent probability on the 8th. And so I don't actually buy. I'm not actually getting my probabilities here from these sort of like, well, it's exponential. Look at all the independent guesses. Once you've been wrong about humanity picking the right branches a few times, you should no longer think they're independent.
Speaker E: Right.
Speaker C: You can start to be optimistic. It's like, hey, maybe we're doing something right here, right?
Speaker D: And another thing I would add to the analogy is that a bunch of the branches are full of apples that give you money until they're apples that give you death. And everyone's making arguments as to why they can detour into this money branch, and they do give you real money, right up until they give you death. Then you add that to the fact that humanity has seemed, in practice, to be very interested in wrong branches, very.
Speaker C: Susceptible to the money apples. The money death apples.
Speaker D: Right now you're getting, like, a bit closer. Right. I will note one place where I am commonly misunderstood is that people think that I think the technical problems of alignment are, like, super duper hard. For some reason. This is, like, basically not the case. My stance is much more like, the technical problems of alignment are basically underserved. There's been, like, you know, a few dozen people working on these things for not terribly long. Like, if you were. If humanity. Like humanity has spent much more effort trying to, like, solve physics, at least I think. I'm not actually terribly familiar with how many scientists there were in, like, pre Newton era. In the pre Newton era in, like, whatever club Newton eventually was in. But, like, humanity just really hasn't put much of an effort towards these issues. And one thing that makes the problem tricky is that there is much less room for trial and error, or so my models predict, given these issues that empirically only show up right around the time that your civilization is getting wiped, and that raises the difficulty level. But it's not like we have turned the best minds of three generations to these issues, and they've come up empty handed. It's like we've turned, like. Like a few dozen weirdos and nerds who are able to be compelled by these arguments ten years ago to these issues, and now we're turning a few more people to these issues. But I am not saying, and there's some great technological feat that needs to be pulled off. I'm saying there's a normal technological feat, and meanwhile, everyone's scurrying around doing something else instead.
Speaker C: Okay.
Speaker D: Where the normal technological feat does have these extra difficulties of, like, you can't do as much empiricism, which may be enough to push it over the edge of, like, humans couldn't do it, but, like, in large part, it's just underserved.
Speaker C: Sure, sure. When the last few moments of our Paul Cristiano episode, we asked him, like, why? What are the bottlenecks? What are the constraints to solving this problem? And his answer was, interestingly, not funding. It was talent. It was supply of brainpower. Would you agree with that.
Speaker D: Yeah.
Speaker C: Or at least funding was a lesser problem.
Speaker D: Like, you can always. Yeah, like, money's fungible. And although it's tricky because it's only fungible to a degree, like, you do have issues if you try to put in a lot of funding that you, like, start to distort the incentives and get people who are, like, showing up.
Speaker C: Grifters.
Speaker D: You get grifters. And you also have, like, legibility issues where, like, are you distorting the field towards the legible work and away from the less legible but potentially more important work? I basically think you shouldn't worry about that at reasonable monetary scales right now, but you should make sure that just.
Speaker C: To really dive into that problem, you're saying legible versus illegible, where the illegible is just hard to understand, hard to comprehend, but actually technically correct, or in.
Speaker D: Particular hard for a grant maker or a funder to evaluate whether you succeeded.
Speaker B: Right. So, like, if you can read the.
Speaker C: Paper and it's simple to understand, the grant maker might, like, oh, let's fund that. But it could actually just be a wrong path on the death apple tree.
Speaker D: Yeah. And, like, I basically think you shouldn't worry about this. You could, with a sufficient amount of money, get into these situations where, like, I start to worry that you're distorting the field in that way. But I do think talent's lacking. I think there's maybe two kinds of talent that are. That I consider to be relatively different, that I think are lacking. One is just, like, more hands on deck trying to understand how these AI systems that we have today work. We're starting to make fledgling minds. They're doing stuff that we can't do by hand. We don't know, like, we couldn't program similar capabilities by hand. We don't know what the algorithms, data structures, type stuff. We don't know, like, what. We don't know how these things are working. We know how we built them. We know how we got them to work. We don't know internally how they're working. Understanding that would give us quite an edge in figuring out how to point minds at things on purpose. I think we sort of want all available hands on deck trying to do that stuff. And then I think that separately there are questions of, like, I sort of. The alignment problem, I think, can basically be factored into this is, like, not quite true, but it's like a fine first approximation can basically be factored into one challenge of, like, how do you sort of make an AI that wants X for some x of your choosing and then separately question of, like, what X can you put in there such that, like, you're happy with what happened? There's like, a bunch of additional issues where the additional issues are, like, how do you make it be able to, like, do one thing without a ton of side effects you didn't want and, like, then shut down rather than, like, you know, prevent you from turning it off so it can verify forever that it successfully completed his task or whatever. Like, there's, there's issues of, like, how do you. And that's sort of like a whole separate pack of concerns. But many people sort of think the problem is, like, what would you ask an AI to do such that the results would be good. But it seems to me the problem is much more like, how do you get an AI to do to, like, care about what you wanted to care about in the first place? And that seems to me like it takes a different and often less legible type of research that I think can totally be informed by understanding how the current AI's work. These sort of, like, directions go hand in hand. But one of the big things I think we're missing talent wise is the sort of person who's, like, has the ambition in the galleous to say, like, I just can take a swing, like, figure out what the hell is going on with minds. How do minds, like, end up caring about things or pursuing things, or, like, having preferences or, like, having targets? How does that work? I think that I can, like, figure out how that works and how to direct it. Like, humanity does not have a theory of minds in this way. We do not have a theory of, like, minds that can be pointed. And it's probably not for lack of that theory being possible. It's probably for lack of just having gotten there, like, science wise. And you can sort of come at that from one end, which is just figuring out how the things in front of us work and then trying to learn what you can about minds and learn what you can about aiming them. I think there are also other ways to come at that that take much, I think, less legible research and more like independent, visionary sort of research, although I think you need a bunch of vision to make progress in figuring out how the current systems work. But that's another place where I feel like we're really hurting for talent is those ambitious visionaries who just think they can take a swing at the whole, like, alignment challenge.
Speaker C: Okay, so when you say we fast forward to the future and we've solved the alignment problem, because that's the only future that we'll have to be able to reflect upon this question. In the future, is there going to be a statue of a person who's going to be like, they solved the alignment problem, or is it going to be a team of people? Or is it going to be not even a moment where AI alignment solved and it's just the alignment problem just dies by a thousand cuts. Do you have any sort of mental model for this?
Speaker D: Like, I think these things. I think it's, like, pretty unclear. And part of the question doesn't come down to how does it happen, but it comes down to how do humans attribute things? It does seem to me, like, historically, a lot of big theoretical insights, a lot of paradigm shifting theoretical developments end up attributed post hoc to individuals like Newton or like Einstein. I think there's a bunch of truth to this. I think we also want to count Riemann and Laplace.
Speaker C: We don't know, and that's kind of the point.
Speaker D: But that helped out Newton and Einstein that I'm getting my l names mixed up. Not actually sure it was Laplace, but also at the point, yeah, but surely it will be an effort that requires lots and lots of people. Surely it will be an effort that requires many insights from many camps. Surely there will be huge amounts of individual labor, much of it probably thankless, from people who show up and can work on the shovel ready projects that scale with labor. Whether there will also be critical insights that come from geniuses that change the paradigm, I think. My guess, if we condition on getting to a future where the problem was solved, my guess would be, yes, but that guess is a little bit distorted by how did we get out of the hole that we seem to be in? And, well, probably there was some force that made things go a lot better than it looks like they're on track to go. And, like, lone geniuses are the sort of force that can do this. If you're just, like, looking for probabilities that it takes. Lone geniuses seems hard to call.
Speaker C: Sure. So the bankless audience is sufficiently large to the point where I'm going to say that there's at least one person listening to this conversation who's going to be like, I'm ready to dedicate my life to solving the AI problem. What advice do you have for that person? Where should they start? How should they get started?
Speaker D: It's tricky. There's, like, a lot of the people that I'm most excited about have come in from very different angles and have their own sort of novel perspectives on the problem, I'm like, generally much less enthusiastic about. Some people come into this problem and they're like, let me read everything everyone's ever written about the problem and try to synthesize it and get a sense of where things are and then work from there. And other people come in and they're sort of like, you guys are obvious idiots. You haven't checked the obvious things. The obvious things are these things. Let's try looking at it this way. Let's try doing. And maybe I'll send my research around and people can tell me where I'm being, obviously dumb or retreading past mistakes. But I'm starting with the assumption that no one here previously was on the ball. I tend to be more optimistic about people in the latter category. I tend to think we need a creative solution. Like, creative solution. And like someone who's more like, I can obviously have ideas about this myself rather than like, let me make sure to integrate everyone's previous ideas so far. Like, everyone's previous ideas so far haven't solved the problem. And also many of them are kind of dumb, some of them mine, like, like, at the same time, there's sort of like a bunch of context that I do think people need and it's sort of hard to get, like, what are good intro resources like? My guess is that like, the less wrong wiki has a AI alignment intro resources page. And if you Google like, AI alignment intro resources less wrong, you'll find like a collection of a bunch of different intros people have written. And then you can maybe like find one of those intros that like, resonates with you. It's hard. I don't really know how to onboard people. I don't really know where the people who come in and are like, I have a chance of solving this whole damn thing come from. But I think my advice would be like, maybe look around on the Internet for some resources that you like and also maybe like, well, it sounds like.
Speaker C: It sounds like it's a, there's no guide, there's no university path, there's a dark forest that you have to get through. And if you get through the other side to be able to be competently talking about this thing, congrats, you're there. But also the whole problem itself is also a dark forest.
Speaker D: Yeah, that is definitely part of the issue is that, and it's not for lack of a guide someone wrote on the Internet, there's probably at least half a dozen of those. I sort of don't think any of the guides are very good. This field is. Yeah, you're totally right. Someone who can solve these problems needs to sort of be able to, like, go off to the frontier where things haven't been done before. And part of that is, like, even getting to the. To the beginning of the problem at all. Like, it's a skill you'll need. Although I'd love to be able to, like, get people to beginning of the problem more rapidly. But it does feel to me a bit more like trying to, like, figure out physics pre Newton rather than trying to figure out physics. In the days where we have physics classes, there are people who have tried to write intros. There are people who have tried to write open problem lists. I don't think they're great. You can find them on the Internet. Try to develop your own intuitions and approach the problem as if we know very little, because we do, in fact, know very little.
Speaker C: Okay. So that's if they want to direct solve the problem head on. I know that I don't have the mind to apply my skills to this. So what do I do? I do podcasting. What about some secondary skills and secondary efforts that people could to help solve this problem?
Speaker D: I like, there's. I think there's a place for regulation on these matters, which pains me deeply to say, given how well regulation has done on various issues in the past. And, you know, I think many harms in society come from overregulation. But, like, it seems to be humanity's only tool for going to a field that, like, self professes largely, that it has a decent chance of killing literally everybody and saying, like, hey, maybe like, back off on that until, like, we understand what we're doing well enough to, like, do this job properly. I would love humanity to have tools that weren't regulatory for this. And, like, if I was trying to design the coordination mechanism, I would be, like, trying to handle it with, like, like, liabilities rather than laws, but, well.
Speaker C: The problem is sufficiently large that the costs of regulation are acceptable.
Speaker D: That's what it seems to me, although I say this with sadness. And so that's a whole track where I am no expert in how to get regulations to be actually narrowly targeted and good, but I think there's a bunch to be done there. I think most people will also be like, well, I don't have the mind or the ability to go into politics where it matters at the moment, and I'm not sure going into politics is the right thing there, but that's another area where people. Where I think there's work to be done that draws in a different skill set. Like, if people think they have an edge in the education problem, in writing up the basic arguments in a way that reaches a different sort of audience or is more compelling to a different group of people, or is like more modernized or something, I think these are all, like, fine things to be doing. Like, for example, you seem to me to have noticed you have an edge in, like, talking to folk. Like helping the sort of arguments and the recognition of the issue reach a broader audience. I think this sort of stuff is great. Draws in a different set of skills. For a lot of people who don't have one of these three opportunities, I think there often isn't an easy way to help out. Reality does not need to give everybody. The laws of physics don't care about you. They can just drop you in a world that is under serious threat of destruction while not giving you an easy thing to do about it. Like, there's a skill to sort of like, not losing a bunch of sleep over it, not getting terribly depressed about it. Like, looking around for where you can help and, like, if you can and you want to because you're like, believe that there is like, a big threat here and you care about averting it, then, like, hell yeah, I respect that. And if, like, you look around and you can't find good ways to help out, such as life, keep an eye out and, like, no need to get depressed about it.
Speaker C: Psychologically, how do you deal with this looming problem? Like, when you wake up in the morning, are you like, ah, shit, we're gonna die? Or how do you deal with this mentally?
Speaker D: I mean, I have for a long time not had much faith in humanity's ability to coordinate. And so most of the emotional blow, most of the update for me was in late 2012 when I became persuaded on these issues. It's not like I was like, oh, AI is interesting. And then over time, as I saw humanity go down paths that seemed to me quite derpy and failed to take the problem seriously and handle appropriately, my probabilities went down. I sort of, like, I think correctly, just like, guessed early on that probably humanity was gonna be pretty derpy about this and go down false paths.
Speaker C: I love Derpy as a technical term.
Speaker D: Like, I sort of try not to make predictable updates. And so, like, there was a day in late 2012 where I was like, oh, geez. Like, I was wrong about a lot of, like, I was wrong in my previous pursuits. I missed, like, the biggest problem heading toward this planet. It's like kind of embarrassing that I wasn't able to figure out myself. Like, I thought of myself as, like, trying to go for the world's biggest problems. But when I was, like, 14, I intuited what seemed to me like the world's biggest problem and never, like, sat down and tried to make a list of, like, what other, like, problems might be bigger. I was just like, obviously, coordination is the biggest one. I'll go for the throat on that. And needed some other people to come along and be like, hey, have you noticed this intelligence thing and how it, like, is the primary factor determining the future and how humans are not at the maximum of intelligence and, like, are on their way to make other intelligences that won't, by default, care about anything nice? And so there was a day when I sort of, like, that argument hit me. I, like, my probability of a wonderful future dropped from, like, you know, like, mid nineties to mid tens or like, mid zeros, I guess. And I mourned. And then I, like, didn't feel a need to, like, psychologically focus on it a bunch after morning. Like, you mourn and then you try to save your civilization. There are still many times, like, it's not on my mind when I wake up. There are still many times when I'm sad. There's still many times when, like, I see something particularly beautiful or particularly moving or that I really quite like about, like, this planet and my species and sentient life more broadly. And I, like, shed tears about it. But it is not a dominant psychological factor, as opposed to it's a deep source of sadness, but I'm not constantly wallowing it.
Speaker C: Sure. I don't really have a question here, but I will say that, say there's a 1% chance of solving the AI alignment problem. That doesn't just mean that we don't die, though. It means that actually a. That all of the negative side of the AI alignment problem, the kill the kill us side, inverts and it saves us and produces the inverse. The level of bad turns into a level of good.
Speaker D: Absolutely.
Speaker C: That we've never seen before.
Speaker D: Totally.
Speaker C: And so there's something there about just like, maybe that 1% of that chance is so small, but the good that comes out on the other side of that is really, really good.
Speaker D: I mean, that's what we're fighting for.
Speaker C: But no stronger reflections other than that.
Speaker D: I do think people often underestimate just how good things could get. Like. Like one place where I prefer talking to most people in AI versus talking to people from the more general population, at least in America, and especially more blue tribe, is. It seems to me like there is a big meme of misanthropy, especially, like, among the blue tribe of, like, maybe humanity isn't worth saving. Maybe, like, humanity has done.
Speaker C: We're the source of evil.
Speaker D: Maybe we're the source of evil. And I do think that we are, like, the, like, basically the only source of evil around. Like, mosquitoes are still edging out humans for the top killer of humans. Well, the mosquito Malaria alliance, really, which I am personally offended by and think we should wipe mosquitoes off the map so that we can be number one for killing humans and also the number.
Speaker C: One in killing mosquitoes.
Speaker D: And also number one in killing mosquitoes. That'll show them. But I would not dub malaria evil. I would dub the Holocaust evil. Like, we are the source of evil. We are where all of these ills come from. We are destroying large swaths of the environment, and that is sad. But we are also the source of love and beauty and friendship and art. These things also aren't, like, universally compelling. There isn't, like, a stone tablet in the stars that says, like, love is great. The reason that we have. The reason that we care about, like, love and friendship and, like, hope and fun and enjoying ourselves is because these were the correlates of fitness in the ancestral savannah where our species evolved. And the particulars of these emotions and feelings and things we care about, those particulars depend on the specifics of our development. Hopefully, some of them overlap with various aliens, but probably not exactly. And it's very unclear how much. It's very unclear how other evolved aliens, how alien they will be. But, like, those things are also in us, and they're also from us, and we might be the only source of that in the universe, and we are very likely the only source of that within 100 million light years. And we also know about ourselves that we appreciate the fun and that we don't like the misery. We can look at ourselves and be like, wow, we don't like the evil. And it's subtle. It would probably be a tragedy to remove sadism from humans entirely. But we're like, well, we want ethical sadism. Pair your sadists with some masochists. Have it all. Be consensual and within the bounds of ethics. Like, it's not just like, we don't want to. Like, there is some of our inheritance. Some of our inheritance is, in. Is, like, very adjacent to the parts of ourselves we don't like, but we also can look at ourselves and see that? We don't like that in aggregate. We, like, have these negative consequences, these negative externalities that no one intended. We can look at ourselves and say we don't like the impulses in us that lead us to, like, great atrocities. And humanity, like, the future, I think, does not look like a similar mix of humanity's virtues and humanity's vices. As we get smarter, as we get more capable, as we get better at solving coordination problems, as we get more time to think, as we get wiser, as we become more who we wish we were, we, on purpose promote our virtues and demote our vices. And is it tricky? Yeah. And do we know what a wonderful future looks like? No, it's very subtle. You can't just go around giving everyone everything that they want or solving all the problems. Part of life is having obstacles to overcome and having real choices that are meaningful and so on and so forth. We can make a world that is kinder, where the obstacles are more meaningful, where you don't have terrible things happening to good people, where the only reason is, that's the laws of physics. Many people say everything happens for a reason. They believe they live in a just world, but things don't happen for a reason. Here we can build a world where, like, your trials tend to give you things that were worth the trouble, that pay off later, and that you were glad for. And we can build a world where children aren't dying unnecessarily and needlessly because they were, like, we're born in the wrong part of the world and got some terrible disease we haven't solved yet. And we can do so much better than that if we transcend the bounds of humanity with, like, the technological limit is huge. I'm an old school transhumanist and think we should do something at least as cool as building Dyson spheres. Although maybe there are better ways to put your stars to use. I'm looking forward to the Matroska brains if we decide that's worth the effort. There's so much potential. There's so much potential that this species has as one of perhaps the only source of love, friendship, happiness, fun in the universe. Aliens may have other things and will care about that, too, but it might not be quite ours. And if there are aliens, they're probably distant. And we can solve so many problems, especially if we have smarter friends who are trying to help solve them with us. If we can get artificial minds that are significantly smarter than us, significantly more capable than us, and that also are into this great project of the glorious transhumanist future, full of flourishing, happy civilizations having good times. I can't describe the future specifically for you because I expect it to look foreign and weird and strange to me and be full of people pursuing desires that I don't recognize. But there is so much upside. And yes, humanity has a lot of darkness in it, too, but that's just one more obstacle on our way to the glorious transhumanist future that is easier to overcome if you have smarter friends.
Speaker C: That was beautiful, Nate. Thank you for just articulating what we get out of solving this alignment problem, because not only do we get to not die, but we get what seems to be kind of the inverse of that. So, thank you for walking us through everything you're doing and why the fight is worth fighting.
Speaker D: Totally.
Speaker C: Cheers.
Speaker B: Vanquish nation. We are here at Zuzalu, and I'm talking to dare, Tehran. Der, welcome to the show.
Speaker E: Thank you. Good to be here. Dare.
Speaker B: You want to just explain for people who don't know who you are, where you are, and what you're up to?
Speaker E: So, my name is Deer. I am currently leading the AI Objectives Institute. We are a nonprofit research lab focusing on the question of alignment, and we are interested in building what tools would be able to add value to the current ecosystem to bring the future that we want to be in.
Speaker B: So, an institute that is focused on solving the alignment problem. Sounds like that people who are at this institute believe that the alignment problem is existential. Am I on the right track here?
Speaker E: Yes, very much so.
Speaker B: Okay, so we've been beginning all of our AI interviews with. Just asking the guest, what is your percentage of AI doom? Of percentage likelihood?
Speaker E: My personal percentage of AI doom is fairly low. For human civilization to end totally would be quite low. I would give it to two to 5%. While for us to end up in a future that is not desirable, more so, existential risk, rather than extinction risk, would be very, very high given the current dynamics we're living in right now.
Speaker B: Okay, so existential risk, can you measure, so chance of death, low chance of significant disruption and displacement in the hierarchy of life?
Speaker C: High.
Speaker E: Yes.
Speaker B: Right?
Speaker E: Yes.
Speaker B: Okay, and so, like, maybe you could illustrate, like, what that looks like. What is your likely scenarios here?
Speaker E: The core principle that has brought the AI objectives Institute together was that the AI revolution, as we call it, can bring an unprecedented level of flourishing to human civilization. But the current systems we are living at the and do not place us on that default path. We currently have a lot of incentive gradients that cause power to be concentrated. We have misaligned incentives in the form of nation states, to corporations, to attention economy, that distract humans from what we actually want to focus on. The AI systems right now are learning and copying these behavioral patterns, causing much more large scale disruption in the landscape. And this propagating further, the doom scenarios that I am most concerned about do not look like nanobots crawling all over us all of a sudden, but look much more like economic failures, institutional failures, environmental failures, as we know today, at a much higher, much more unpredictable rate. And we already have mechanisms to deal with these things, but we do not know the scale at which we will be coordinating for this.
Speaker B: Okay, so you listed off a bunch of existential crises, and I think you're saying that just like AI is going to accelerate crises that are already in existence, rather than creating a net new crisis, although you're leaving some small amount of room for that. The bigger issue is that there are already human crises that we have, that we don't have solutions for. And AI is going to accelerate those.
Speaker E: Exactly. I think the main crisis that I am most afraid of is in the shortest term. Within the next two to five years, we will have massive disruption to the institutions and systems as we know today. We see AI as an optimizer, and we have already other misaligned optimizers in the landscape. They do look like markets in corporations, they look like nation states. They look like misaligned incentives that has driven mass scale invasions that we're experiencing in the last two years with Russia and Ukraine, to massive bank collapses, to voting systems that end up in gridlock. These systems will be exacerbated in a pace that we are not yet familiar with. And that's to, in our perspective, from the AI Objectives Institute's perspective, there is a very strong continuum, rather than a sharp break between human misalignment as we experience today, and AI misalignment until we solve human alignment. Talking about a purely AI alignment system feels superfluous, in my opinion.
Speaker B: So you're saying that solving alignment, there needs to be a correct order of operations there. And before we look to solve AI alignment, humans need to first look inward and solve our own alignment first. That's kind of the take.
Speaker E: I think AI tooling, in fact, can be quite helpful for us to solve alignment at its base. And there is a lot of cross pollination that I think is necessary between understanding how we as humans so far before AI, have been able to keep certain systems in check, be it being able to give corporations a legal entity that can be interacted with, or the way nation states have different systems that are checks and balances for each other. There's a lot of value to be driven from how AI alignment research can feed into current systems that have been experiencing different levels of misalignment, and how these systems and how we have dealt with this can feed into AI alignment. We see this as one central problem that AI is learning from. To put it in another spotlight, solving AI alignment, being able to align AGI to a specific set of human values and perspectives, actually doesn't solve human alignment problem. It just pushes the problem from a silicon substrate to the sociotechnical substrate, in which case it might be much harder to control.
Speaker B: That's a really interesting. Could you. Could you just elaborate on that? Because, like, there are some human values that we want, that I think I can claim without evidence that we think are good, as in, like, don't kill me and things like that. But I think what you're saying is if we go further down what we think are good, we'll start to get into the very subjective realm. And if we start to align AI without defining what humans think are good, we can run into a problem.
Speaker E: Right.
Speaker B: Okay, so my next question is, like, it's still a problem if the AI's come and kill us, right? Right. And so, like, maybe I'm a little bit lost with how to proceed here, but, like, if there seems to be an order of operations problem, of which problem do we solve first?
Speaker E: Right, right.
Speaker B: And how do we even define the problem?
Speaker E: Yeah, so there's this concept of differential technological development. What is most important here is to decide on the order of operations of which problems can be solved first, so that they can shed light into the next problem. This is like developing cars before seat belts are invented is more risky than understanding. If we are to develop cars, we should have seat belts. The question of differential technologies are, what are the technological pieces that make sense to tackle first, so that we have the right substrate on which we can build the future that we want to build towards. So this is what we focus on at the AI objectives Institute. What are the pieces that will be able to yield a safe, aligned AI downstream? And what are the pieces that are necessary right now to build first? The center of all of this is a coordination problem. I actually define existential risk as a failure to coordinate at the face of an existential risk is what makes existential risk come together. So there's a lot of tooling that we are focusing on right now on scalable coordination. There's a lot of tooling. We're focusing on epistemic security and systems of alignment. These are three avenues that I think need to have much more research so that we can mobilize together. We can identify the loopholes in our thinking as individuals, as collectives, and as systems, so that we can bring a level of systems alignment to in line with how we want humans to proceed. Only thereafter we can start contemplating the skills scale of AGI. Now. I think it is quite a ways away, for we still have enough time to be able to have, by the time AGI arrives, for us to have built institutions that are built on the backbone of scalable coordination and cooperation. That is why I think there is a lot of hope for us to be able to avoid total catastrophe. And I'm interested in thinking of it from that angle, which I think is quite necessary in the current alignment landscape. What are tooling that we can build right now that will bring an incremental net? Good. Rather than talking about what we should avoid, what we should not do, I would like to bring light to the world, what we should do, what we should be focusing on today.
Speaker B: Okay, so it's your position that AI in the short term is going to produce immensely powerful tools. We can use these tools to help humans with their human problems that they had before, before AI ever came on the scene. One of those things is human coordination, and so we can apply AI to solve human coordination. Hopefully this all happens before the AI doom alignment problem manifests. That Eliezer talks about. And the idea is that we race to use AI tools to align humans amongst other existential crisis, including our own ability to coordinate, and then we will be able to solve the AI alignment problem more head on. Is this more or less your roadmap?
Speaker E: Yes, I would say that the AI alignment problem is more or less the same problem. It is the natural extension of the human alignment, human coordination problem.
Speaker B: So because we cannot coordinate as humans, we can't coordinate on the AI alignment problem.
Speaker E: We do not have mechanisms to identify what are the sets of values that an AI system should be aligned to. And I think it is quite short sighted to say, let's just pump in more text to large language models, and at some point they will be able to figure out a more detailed, you know, I'll go into the weeds a little bit. Solving single to single alignment is technically easy and saying, you know, if we have one unitary agent that is able to be superhuman and super intelligent, they will figure out what is best for us. So we should fully focus on that. I think this is quite false. I think by the time we get to this stage, there will be many narrow AI applications that will be strong enough to actually put humans in a catastrophic risk. So we need to actually start from there. What will these narrow tools look like at the hands of misaligned state actors? What will these look like at the hands of exploitative corporate behavior? How can we make sure we can have safeguards around these as humanity, as a civilization? And will those tooling actually bring a better AI future? That is the intersection that we are interested in. I think the answer is there.
Speaker B: Okay, so the idea is that, like modern day late stage capitalism has produced large scale corporations that are misaligned with humanity in general. And then you give them super powerful, narrow artificial intelligence, and they just become misaligned faster. And that's like one model, one application of where this could go wrong. And there's perhaps like five or six or seven more examples like this.
Speaker E: I mean, I would actually argue that we already live in this scenario. This is not the future. This has been happening within the last 1010 years. I mean, there's a couple simple examples. Insurance companies probably at this point, have an AI system to decide which claims they should reject immediately, because they are least likely to be followed up on. One could say, well, this is the insurance company's job, and they are rightfully doing this. To me, this is actually a fundamental alignment problem. We already have an optimizer system inside another optimizer system. That is the insurance company that is rejecting claims that is causing human lives to be potentially at risk. And we have devised a society that has normalized this behavior. We have devised ways in which, you know, a corporate company, like companies, are able to hide the environmental externalities that they are building to the world, to the landscape. The questions I ask is, are AI systems able to share world models with us that will be able to help us understand these externalities better, to be able to incorporate that into fundamental decision making? To put it in more fluffy words, can AI systems and our understanding through these tools, elevate our sense of what humanity wants to be so that we know where we want to go? That is why I am hopeful about being able to build a future. This requires a lot of coordination. This requires a lot better epistemic security. This requires much more thinking about how do we want to envision the institutions of the future.
Speaker B: So maybe you could just illuminate some of the strategies that you are working on at the machine. Objectives Institute.
Speaker E: The AI Objectives Institute.
Speaker B: The AI objectives Institute one line that.
Speaker E: We had on our website for a long while was our objective is better objectives, which is, it's almost tongue in cheek, as we do not think AI systems can have fixed objectives. Similarly, humans do not have fixed utility functions. In fact, the relaxedness of these is what gives flexibility to human evolution of thought, of our coordination of our ethics. So in some ways, the name is tongue in cheek on that front. But the goal is to come up with better objectives continuously.
Speaker B: Right? Okay, so what are the, if we could drill down into the details of what it takes to come up with better objectives for artificial intelligence, like just the details like, if you will.
Speaker E: Right, for sure. So we think of the society, let's look at the societal stack from an individual level to a collective level, and then to a systems level on the individual level. The core work is finding individual autonomy and sovereignty through bringing better epistemic security. The world that we're living in right now, especially in the western world with democracies, a lot of this heavily relies on information transfer. People vote, people coordinate based on the information they receive around the world. Now we are entering a new paradigm in human communication, where most of the content is about to be generated by AI systems in this world. Are we able to use AI systems to bring a different level of epistemic security and confidence? To have us understand is the content I am engaging with, with a latent agenda? How can I stay true to my objectives as an individual? How can I stay true to my alignment with the continuous flow of information that we are interacting with that is constantly fighting to hijack our bandwidth? This, at the lowest level is the most important level. A lot of mechanistic approaches to alignment assume that individuals have a level of autonomy and sovereignty. They do know what they want. We actually start from that question. We do not know what we, we do not always take actions in line with our incentives due to bounded rationality, due to myopia, due to just pure distraction. How can we use tooling that comes from the AI landscape for that? So this is the first avenue of research. We have come up with a research agenda that has some specific avenues that we would like to explore that we believe is a net incremental. Good. I'll go into some of the details on that, please.
Speaker B: But first, just to really just make sure I'm understanding you started with the individual, and I think that was an intentional choice. Saying like, starting with optimizing for individual freedom and autonomy is a high level goal, a higher level goal than the rest of the stack, which I think we're about to go down. But could you explain just an elaborate on why we start with the individual and why that's important?
Speaker E: Individual is the building block of the society. Every decision that we are making ultimately comes down to an individual's ability to understand the world that we are interacting with. We have devised the systems that we are operating in right now, assume individuals ability to give feedback to a corporation, to a government through our behavior on purchasing or through our votes, and we cohere around that. Assuming that AI or democracy or media, AI is just one form of super intelligence, we have devised many other forms of superintelligence in human history. Assuming that the individual maintains a level of autonomy and sovereignty throughout this interaction as we live in the world, is what has caused the crisis that we already are in. This is not in the future. We already are in this scenario. Let's look at social media. It's the archetype example. In 2008, we thought, you know, this is going to be a revolution that brings us a level of connectivity, of mutual understanding that will heal democracy. Instead, we ended up with echo chambers. We ended up with massive epistemic fracturing with respect to what facts people believe in. We found people that get locked in more to their own bubble echo chambers. We could have foreseen some of these stuff. This all sheds light into. It's ultimately the individual autonomy and sovereignty that is the core building block of civilization. Then the question we ask is, is the AI tooling of today able to bring a different level of episode security? Our answer is yes, and this is very worthwhile to be trying.
Speaker B: Epistemic security. So, epistemic, can you just define that term?
Speaker E: Yes. The information that you are receiving, you know where this is coming from. You know, you have a sense on what this information is trying to accomplish or whether it is true or false, how you relate to this information, how you want to relate to this information, and how you want to participate in the world given this new information. Currently, a lot of these systems are actually quite shaky in the society we live in today. And we are entering truth world, right? And we are, instead of securing this, we are saying, let's come up with a level of generative AI capabilities that can flood even more information. And then we are talking about what will AI be aligned to. The question is, do humans even have bandwith three be able to share this information? Right?
Speaker B: Okay.
Speaker C: Okay.
Speaker B: So, epistemology, the study of knowledge, epistemic security, is just like securing the ability for the individual. If so, they have the choice to. Sometimes people just want to live in the cozy comfort of being fed the information that they want. But importantly, giving the individual the choice to have access to truth. Is a basel building block, you're saying to talk about the rest of the societal stack?
Speaker E: Exactly.
Speaker B: And we're going to use AI tools to improve that part.
Speaker E: So a couple projects we are working on right now on this front, that is the core building blocks is can we use AI technologies right now, AI techniques right now to be able to inform certain patterns of perception? For example, I'm not even going to go into whether or not the content you're interacting with is true or false. I will instead start from the side. Is the content you're seeing designed to elicit anger from you? Is the content you're interacting with designed to trigger an addictive loop giving you a dopamine high really fast? Is it eliciting anxiety? Is there a latent agenda in this content? Or is there a subcultural affiliation in the content that you are interacting with? Is it using language that is geared towards a certain subgroup? Is it repetitive? Is there evasiveness? Is it stating beliefs? Or is it a response to something else? Turns out large language models are actually really good at detecting these kinds of patterns because what they are doing is pattern matching and predicting the next token. So a language model is able to say, oh, this next token is unusual. It looks like the next token. The difference here is guiding me towards a different future. So this is one building block that on its own is able to add a lot of value to the language. So we are building a prototype called lucidlens, which evaluates the content that you're interacting with continuously to be able to guide you into, hey, it looks like you have been on a dopamine loop lately. Do you want to shift what you want to do? Or looks like you're engaging with content that is extremely repetitive in this discourse? Can we bring a level of intentionality to your objective alignment personally?
Speaker B: Okay, so you're building that system, right? How do you make sure that that system isn't biased, right? Cause like maybe the, maybe the repetitive loop of iterative content is the correct thing. And then you're saying this AI suggests to humans like, hey, maybe you should get out of your repetitive loop. How do we know that that's not an unbiased thing, thing to do?
Speaker E: Right? There's a couple approaches here that we can take. One of them is can we ground language models to an individual's own affiliations? There are some people working on this front. I'll give a fairly simple answer a heads up, like something like a jiminy cricket on your shoulder that can point out, hey, you might be stuck in a rage bait. Click, scroll for the last. While that check in is in the right context, is not harmful to an individual, you might say, yes, I acknowledge that. I want to continue. I want to proceed. I like where I am at, rather than have there be a sharp judgment of the nature and the quality of the content. But another part LLMs are able to do very well is to being able to fetch further information that can give you a larger picture around the content that you're interacting. Yeah, this is just one of the many avenues I will zoom back from lucid lens. Another one is, can language models replicate an individual's affinities to the point that they are able to help us stay grounded in a set of objectives that we want to be in? This we say mind. This we call mindful mirror is a different avenue of research. You can think of it this way. The first one is about a machine helping humans stay grounded to their objective. The second one is helping a human stay ground to their, sorry, a human to give the same feedback to a machine. To say, here are things that I would like to prioritize. These are what is good to me. Having an individualized personal language model that is secure, that is not living in a large company servers, but that is completely owned by you, having this to be able to secure your understanding of the landscape, of emotions, of content that you're dealing with, that can help you ground yourself in a moment where you are lost. This is an incredible piece of technology that can actually create net value to the society.
Speaker B: Okay, so I think I can categorize your mechanisms into two different camps. One in the crypto world. We use this idea of credible neutrality quite frequently, and there are some mechanisms that are incredibly neutral, which aren't to say, like, hey, what you're doing is bad, but they're just little alerts saying, like, hey, you've engaged in very repetitive YouTube rabbit hole type behaviors. I'm just gonna let you know that that is what's happened. Without saying anything negative or positive or suggesting a rerouting, just like, a neutral mechanism to perhaps bring you out of the hole and let you know that this is perhaps a, this is perhaps a danger zone or perhaps not. And just for you to be able to step out of your consciousness tunnel and zoom out. And just like that, in meditation, sometimes they ding that gong. And just like, hey, if you're lost in thought, ding the gong. And so it's a way to just get you to snap back out of it if you are in it. So that incredibly neutral mechanism which we love, and then the other one is being able to customize your own personal llmdh to align with your desires. And since you are the one implanting your biases into the LLM, we also feel that since we are not imposing that upon others, we're only imposing that upon ourselves. That also checks the box of credible neutrality.
Speaker E: Right. And I would like to underline one thing, which is that the large language models, the goal isn't to create a locked in version like there is. There's many fault lines that we can fall through here as we are building these, which is why the goal isn't to launch this as a product that widespread use right away, but to actually approach this from a question of rigorous study to the point that we make sure these tools are doing the things we want to do. What are their failure points? For example? A very terrible way to do this would be to cause individual value lock in, in which case, you know, a language model constantly reaffirms things to you from your past state to the point that it prevents you from moving forward. I am more interested in a language model that is able to give me awareness of my dream drift through my own evolution of thought. These are some of the aspects that are actually very fundamental to the question of alignment. How hard, how is the landscape of values shifting for an individual?
Speaker B: How am I changing as a person?
Speaker E: Right. What was important to you? There's a series of simple questions. Are my beliefs consistent with my beliefs? Are my beliefs consistent with my actions? Are my actions in consistency with the community that I'm living in and their beliefs and their actions, being able to have visibility into these, these systems are incredibly fractured right now, and we are building even more walls. I believe AI tooling can actually help us overcome some of these to be able to understand. Oh, looks like what I wanted to do. I am not able to do it, given the incentive gradients I am existing in right now. The crypto world has suffered from this quite a bit, certainly. How can we have this be more visible to everyone? I believe questions of contemplating what values do we want AI systems to be aligned to, et cetera, really require a level of rigorous understanding of self first. So this is the first step. Then we move to the next category, which is, how can we scale this up to the question of a collective, which is the next tier in human society?
Speaker B: Right. And before we get to there, I just really want to drive something home. It really sounds like we're trying to, to allow AI for humans to become the best versions of themselves. If we want to talk about some people in the psychology realm, Nietzsche would call this the Ubermensch, like the Uberman, the Superman, the best, literally becoming the best version of yourself, because that also scales up in society. If you as an individual become the best versions of yourself, that makes you a better community member, and that makes communities better, which I think is kind of where this idea goes.
Speaker E: I call it extended cognition. If I am able to have an ability to understand myself, cross sectional through my own history, based on what I have engaged with, based on what I have thought, what I have written, that is powerful, we can act. Some of, like, the scariest applications are also the most worthwhile. If this data is compromised in any way, that also creates a mirror of me to exist in the society. So then the question becomes, how can we do this in the most secure manner that is really within your own autonomy and sovereignty? Because we already live in a world with very difficult attention economy, that everything is competing for your attention, for your beliefs, so that you can vote or so that you can purchase a certain way. It's very important for us to be able to bring a level of autonomy to the individual as AI tooling proliferates in this landscape.
Speaker B: Okay, let's move up the social stack. Community comes next.
Speaker E: Yes, collective. Collective question here is scalable coordination. This is where I am most passionate about. I think there is incredible value to be added here in some ways. There's many alignment labs we do share our concerns are in line with a lot of the other spaces like Miri or Redwood, where there is massive challenges that are coming in. And the question is, why have we not yet been able to coordinate at a scale where we have lined up our incentives as a society so we can tackle these AI problems. Instead, we ended up in a race dynamic where entities that are spun up to counter end up participating in the race dynamic for this, being able to level set the understanding of what is true across every participant, being able to bring visibility into what are different perspectives that existed in society right now. How can I engage with these more effectively? How can we come up with collective decision making systems so that a collective can find its alignment? The first category was about an individual finding their alignment. The second category is, how can a collective find their alignment?
Speaker B: So you corrected me when I said community and to replace it with collective. I think the reason why you did that is because a community seems to be like a handful of people. Right. 100 people or a thousand people in a town. But a collective is like the hive mind of these people. And that is the thing that we are trying to produce alignment for. Is that my. That's my interpretation, yes.
Speaker E: And I would say I am in a collective with a lot of people that are not in my community, necessarily. A community is a more intimate collective, depending on how. And you can say, you know, well, there is different ways to cut the social strata that we live in, to say, you know, these are different categories. All of these are valid. The thing I am interested in is, say, we are able to align Aji to one human. What do we align Aji to now? How can we come up with definitions for collectivists?
Speaker B: Yeah, we recently did a podcast with a guy, Tim Urban, on the subject of liberalism, and he had this great illustration of higher mind thinking versus lower mind thinking. And then higher mind is like a genie, and lower mind is like a golem, right? Like, just like, a golem's just like, dominant punches. And a genie is, like, magical and higher. And then when you have collectives, if you have a society that is a society dominated by lower mind thinking, primitive mind thinking, like reptile brain, you have a collective golem. But then if you have a society based on higher mind thinking, higher order thinking, using their more recent parts of their development brain, their prefrontal cortex, then you have a collective genie. And this thing can actually, even if it engages with a different collective genie. So you have, like, the republican genie and the democrat genie, two genies can actually make progress together. Right. They can actually come together and produce a roadmap, whereas two golems just come and fight. And so this is actually a similar subject that we've had on the podcast, and maybe a way to illustrate this.
Speaker E: I see some parts of this to be in line with our thinking. The way we have been developing AI systems are much closer to a golem right now. I'll get into that.
Speaker B: Hence the downstream problem of AI alignment as human alignment.
Speaker E: Right. I have thoughts on how to make AI systems be more like genies as well. We can get to that in a little bit, but to. Not to change. Curse of the.
Speaker C: Sure.
Speaker E: One of the projects we have on this front is called talk to the city. And this is a digital town hall that you can summon out of unstructured feedback that you have collected from a polity. We currently have voting systems where you're sending one bit of information to the government every four years, and you're hoping that they will be able to represent this the best. We have been used to categorical information on voting multiple choice and referendums. The question is, am I actually able to share my perspective in its real, more true to my own perspective through human language and have a central entity receive this information and make decisions based on this information? Talk to the city is a prototype that collects different unstructured text feedback from the entire community that we are looking at, synthesize this into a set of different perspectives that exist in the community, and train conversational language models for each of these perspectives so that you can have these perspectives talk to each other or as an individual, be it you're a policymaker or a journalist or just a citizen in this city, be able to engage with all of these reasons why the goal here is not to find consensus. The goal here is to understand different viewpoints so you can make sure you can address these in some ways. One of the problems that we would like to solve, and this again goes back to the real alignment problem, it is easy to find the lowest common denominator across everyone, and that causes a lot of short sighted policy making. That causes a lot of, I mean, political theory. Seeing like a state explores this in depth. I am listening.
Speaker B: I've read that book, by the way. Great book.
Speaker E: It's very helpful for shaping our thinking. I am interested in being able to understand a more in depth policy. How is this going to actually impact people? What are the reasons why this may be bad for some groups, yet still creates a Pareto improvement over the current state? These require a level of sophistication that isn't about saying, okay, looks like everyone agrees on do not kill humans, so let's codify that. Can we actually understand? Okay, but I am interested in having more resources for my village, and this requires a compromise with, um, something else. If the proposal is, you know, should we build a road from a to b, yes or no? The answer might be, we should build a road from b to c. And being able to give a community to express this and how this be received systematically is something AI tooling can actually bring to the landscape right now that can bring a different level of collective coordination capability.
Speaker B: So, okay, so it sounds like at the individual level, we have these language models that can be perhaps perceived as like, our personal assistants, our personal, like our series, our Google, our Microsoft Cortanas to help us think and help us know and help us learn, right? And then that can amalgamate to a higher order LLM, right. That, like you said, doesn't come to consensus on our behalf. But allows us to see, when you aggregate everyone's personal assistance, what does everyone believe? And it processes that data of individual beliefs into something for us to reason about and understand and move forward. Now that we understand that, right, I.
Speaker E: Am not interested in the genie that you're talking about to make policy for us. I am interested in this system to be able to show us. These are the considerations we need to take into account that people have voiced. This is something that AI tooling can help us with today, right now, which is why we are building it. And that brings a level of scalable coordination and cooperation capability. Another aspect here is to finding positive some outcomes that individual groups may not have been able to see. The solution to should we build the Keystone XL pipeline? The right answer might be, well, a hydroelectric dam will still bring the same energy and workforce to the region without actually having an environmental pollution cost.
Speaker B: So it can produce emergent emerging optionality, right?
Speaker E: And these are things that the systems we are building are actually really good at. I would rather not yield the level of reasoning and decision making to an AI system, but in the near term, we are able to use these as building blocks to improve the capability for humans to cohere with each other. Now, what's really interesting here is we are getting two birds with 1 st if we build these pillars. The first one is these tools are net good for humanity. We need these right now to solve human alignment, to take a step towards human alignment. But also these tools will create datasets for AI alignment as well. It will be able to show how humans have cohered around decision making on specific world models. Or have humans say, yes, this AI system actually was able to represent me through time. We currently don't have this data yet. We are talking about software solving AI alignment. Don't get me wrong, I do think mechanistic interpretability and a lot of machine alignment research is incredibly important as well. We also need to look at what does this mean for humanity. Otherwise, we will just push the problem of alignment downstream and make it much harder to solve.
Speaker B: Right, okay, so what you're saying is like, when we have this collective consciousness that we are able to reason about via all of our native large language monitor systems coming together and pow wowing about what everyone believes, we'll start to be able to lock in some beliefs. We'll probably lock in the idea that killing humans is bad. That'll probably happen pretty first. Everyone will be in agreement about that. So then we can use that shared understanding that killing humans is bad as a way to codify that into law about AGI. Right? And then we perhaps can go higher from there and be like, okay, now that we understand that everyone believes that killing humans is bad, we can also lock in that theft is bad, and then we can start to get higher up the stack of what we believe and then use that to operationalize more powerful AI. Is that the path here?
Speaker E: I very much disagree with the framing, actually.
Speaker B: Oh, no. Okay, please help me. Help me understand.
Speaker E: Value lock in is a concept that is well explored in AI alignment and effective altruism and similar landscapes. The goal is not to lock in values. The goal is to build systems where people, the polity, is able to continuously give feedback and participate into an ever evolving consciousness, instead of one AGI that has learned the moral code and then can proceed. What is necessary is a system that can continue to take feedback from people as the value landscape shifts, as more unpredictable events happen. This actually, the crypto world has lived through this many times. As you know, the incentives shift as speed of progress shifts all the way from gas fees to coordinating how purchases can be made. We need systems that are actually resilient towards updating based on how the landscape is changing. So I would be quite worried about building systems that can learn and fix something in place perpetually. Much more so, I would be interested in systems that are trained to fetch new information, to understand how the ground is changing throughout time. Most humans, I mean, some values I hope we perpetually agree with, such as do not kill, do not cause harm. But then again, what does harm mean? What does this mean in case of, you know, euthanasia that, you know, might be an opt in from the individual. How does one proceed? We already are living in a world where we are exploring these kinds of questions before AI, the AI systems. I think it's quite dangerous to force the AI systems into. You need to find what the optimal version is and enforce it to the rest of the world. The right way to go is build an AI system that can learn from humans on where humans want to go at any given point and bring us towards there and have there be a level of corridability.
Speaker B: So, like, there's a rule of thumb that I've come to understand in the crypto economic world, which is called no magic numbers, as in, when you build a crypto economic system, if you just pick a fixed number, that is a point of rigidity and fragility. And I think that's perhaps what you're saying about when we train our AI's to be aligned with us. Any sort of rigid or fixed parameter can create fragility and long tail consequences that we don't understand. Right, okay, so we're in agreement there. And so, okay, so you didn't like when I was saying, like, hey, all the agreements, all the humans agree that killing is bad. Let's lock that in. Maybe I'll rephrase and say, in this one moment of time, all of the humans of a local collective, all in that one moment of time, agree that humans are killing is bad. So the AI that's reading that data will, in that moment of time, choose to not kill humans. Is that a better way to describe it?
Speaker E: And I would rather have us have systems that don't necessarily give AI the ability to be able to kill humans in the first place. But more so. See these as the tools that they are. They are super intelligent things we can consult and learn from and iterate from there. But yes, the AI system's values should be able to evolve as human systems evolve. And it's really downstream of human autonomy and sovereignty into collective decision making that can bring the systems to be able to be aligned.
Speaker B: Okay, so, okay, so we started the individual. We've moved up to the collective. Is there a next step?
Speaker A: What's higher?
Speaker E: The last step is systems, though.
Speaker B: Systems, okay.
Speaker E: Systems are more complex than a collective. In a system, we don't only have multitude of people, but entities that have their own capabilities, that have their own agenda. For example, a corporation consists of individuals, but it has goals, it has affordances that go beyond what any of these individuals can do. Furthermore, we have developed systems in the world that, you know, don't hold individuals accountable for the failures of corporations. This is the side where we have seen massive problems with misalignment throughout human history, both in terms of, you know, states, governance, and we have toppled many systems. You know, divine right of kings was impossible to overcome, yet here we are. Communism was the same. There's a lot of systems that we evolve. We currently operate under a capitalist system that is heavily governed by incentive gradients that have caused a lot of shifts for how even nonprofits that are developing AI have shifted their priorities towards monetization and productization. So the question then becomes, how can we design systems that can stay aligned to the betterment of the collective, to the betterment of the individuals? Everything goes back to, is this actually producing well being for the participant, or is it good hearting something? Good hearts law, as in, when you pick a measure that becomes a target, it ceases to be a good measure. We live in systems that already do this. This is precisely why we don't want an AI to lock in and create rigidity, but design systems that can continuously evolve through that window.
Speaker B: Okay, so this seems like the hardest problem.
Speaker E: Yes.
Speaker B: It also seems like the frontier of coordination problems that humans have arrived at in the grand scheme of things, and we still haven't tackled that problem. It sounds like perhaps understanding that that's the foundation that we were at, we actually might need AI to solve that problem and not be able to solve that problem without it.
Speaker E: We haven't solved this problem.
Speaker B: Right.
Speaker E: One thing I say is, our work, the AI objectives institute, in some sense, the word AI is not that relevant. This work was relevant 300 years ago. I think this world will be relevant. This work will be relevant post AGI as well. This is a question of how do we design structures, institutions that can stay aligned to the collective? This is a millennia old problem. AI is just the newest building block in the story. It's a very critical building block in this story that can cause a lot of damage if we don't do it right, which is why I have high fears of existential risk, if not extinction risk. But that is possible as well. If we don't coordinate. I believe we would be able to coordinate and build better institutions, which is what I want to work on.
Speaker B: But do you think that actually solving the point of, at the systems level, we have borders, and that's kind of like a coordination breakdown, the fact that different countries operate by different rules and coordinate differently? And then there's also different economic systems. Right? Different systems. They're disparate, they're disconnected. It'd be better if there was a single global system along with all the other problems that we've had with Stalinism and other atrocities throughout the 19 hundreds. My question to you, and I'll just reiterate, it is like, that's the frontier of human coordination, that we've solved it at the tribe, we solved it at the community, solved it at the city, we've solved it at the nation state level, haven't solved it above that, and honestly, haven't really solved it completely at the nation state level either.
Speaker E: And we haven't solved it on an individual or collective level either. But we are taking steps towards all of these.
Speaker B: It's solved it solved more or less at different parts of the stack.
Speaker E: Yes.
Speaker B: And my question to you is, do we need AI to take the next step in solving it at a more systemic level?
Speaker E: I actually see it from the opposite way around, given we are building aih. AI driven institutions, given the workflow that we have right now, will yield institutions that have AI in it. We have to look into how AI will interface with this.
Speaker B: Right?
Speaker E: If we say, you know, let's turn off AI so we can solve this problem for another couple centuries or millennia, sure, we can do that as well. We don't live in that hypothetical AI is here, it is present. I would ask, how can this be helpful for institution design? Yeah.
Speaker B: So would you agree that AI, all these language modules, everything we're talking about here, is both the problem and the solution at the same time?
Speaker E: It is the problem right now because we haven't yet been able to solve human alignment. Hence, any super intelligence, any super competent entity that can excel human capacity can be dangerous, such as misaligned state actors or exploitative corporations. Of course, an AI will be very dangerous as well. So we need to tackle this problem. So we have a couple experiments on this front that are also incredibly important. And these are the monoliths that require much more coordination and help than one group to solve. So our goal is to foster an ecosystem that has many approaches all the way from, you know, crypto is super important. ZK proof is super important. On this pillar, we are interested in building a proof of concept of an open agency architecture system that can help an institute. Our goal is to showcase that an institution can make decisions based on feedback from a larger collective, based on expert opinion, in a level that is transparent and visible and interpretable, rather than a complete black box. And we believe that it is incredibly important for AI tools building institutions and AGI building institutions to follow this. And also it is important for the AI systems to be built on top of this principle as well. Yeah, and open agency is a concept that Eric Drexler has pushed forward. A simple explanation would be, can we shift our thinking from agents, which are singular monoliths, that have fixed goals, that have low visibility, that have their own ways of doing things towards agencies which has different faculties and different tasks that are being passed around as it reasons about the world and as it take actions into the world. Building more open agency systems rather than closed monolithic systems is net good. Designing institutions that operate this way is a net good. And similar to the question of human alignment, we have been doing this for a couple millennia. We have iterated through different governance structures towards more visibility, towards democratic systems. We are going to continue with these paradigms as now, AI tooling enters the picture as well.
Speaker B: Yeah, I really just want to drill down on my understanding of how AI fits into this stack, because I think your big message that you have is AI is yet another thing that we need to figure out how to align, along with all the other things that we need to figure out how to align.
Speaker E: It's a very urgent one given one.
Speaker B: Right. And my mental model is that if we can, AI is unique from the other systems that require alignment, in the fact that we can use it, it's special. It stands out from the rest of the problems, in that if we can align AI, we can align everything else, and we actually might require AI to align everything else so that we can also align AI.
Speaker E: I would say it stands out in some ways, for it has certain capabilities that hasn't been emergent from others. I highly doubt if we can align AI, we can align everything else. We can solve a version of AI alignment, where you can align a single AGI towards a single set of human values that then can go and rampantly destroy half of the world, and we end up in a Thanos scenario, or we end up in a Moloch scenario, where you end up unaligned AI towards a certain set of values that exploit, towards building more and more resources, take over. These are not good cases of alignment. A multi multi alignment case, where we are looking at, there is a polity that has different perspectives, and these can be represented as different agents, these can be represented as different general AI systems. I don't only like we are more likely to end up in that scenario already. We have multiple tools, we have independent groups that are building many tools. These tools require a level of coordination between each other. These tools need to be able to have their own interpretability to the people they are accountable to. So I highly doubt we will end up with one monolith. But the question is just like how we are trying to solve coordination today with many, if not AGI, more narrow tools that are still capable of massive damage to human existence. How do we create alignment across all of these systems?
Speaker B: Okay, so if you would dare, can you, like, speedrun us through the version of the universe that you hope to see? Like if you and everything that you want to see happens, what does that universe look like over the next five to 50 years?
Speaker E: Sure. I am excited about the value AI tooling can bring to the universe. We will have systems that will help us discover ourselves better. We will have systems that help us give visibility to our priorities and see how this is acknowledged by the rest of the world. We will have systems that elevate humanity towards what it wanted to be, rather than avoid what we afraid we would become. I am interested in a world in which I understand the participation that I want to make. How does this contribute to the world that I have bandwidth to explore and play? I would like there to be a world where everyone is able to have bandwidth towards the hobbies, towards the joys that their bringing into pursuit. I want a world in which we are able to see how our opinion counts in a larger system that is making decisions that is interpretable and accessible. I want there to be more human connection. Ultimately, it's around being able to have humans interface with each other more, not less. That's why the core of this always comes down to the coordination problem. Can we have humans see eye to eye, understand each other? Can we have the AI tooling reduce the barriers, reduce the incentive gradients that are shaping up right now that prevent humans from finding more agreement, more shared values, more shared joys with each other? This is what I am most preoccupied with. The world I'm afraid of, is one that we talk about this a lot in our team. Numbers that we have decided to care about are going up and up and up. While we don't necessarily have more human flourishing. That is what I'm afraid of. AI tooling can bring us an unprecedented level of humanity and flourishing. The default systems do not place us on that path, and I would like us to go towards there. And I think this is possible. I think the tooling that can be built today already takes massive steps towards here, this tooling. We're interested in building this tooling for humans to use. We are interested in building this tooling so that the AGI labs can adopt an open agency architecture so they can make more grounded decisions on what the collective is interested in, what is safe, what is interpretable. There's a lot more in there that we didn't go into in detail on how we can have systems that can make decisions that are by design safe and verifiable. All of these will be a net plus to the world we are living in. I don't think we will solve human alignment, but I think AI tooling can take a massive step towards that direction. That is the world I'm interested in there.
Speaker B: I get the intuition that you are an optimistic person. Is that correct?
Speaker E: I would say so, yes. I think we need more optimism in this landscape so that we can see what we want to do. I think there's a lot that can be done, and I have many fears as well. But I think these fears can be solved if we get to the level of coordination yeah.
Speaker B: Derf people are piqued by this conversation and they want to learn more about aspects of this conversation. Where should they go?
Speaker E: Check our website. Objective is that is send an email to our team. Hello. What is it? Message to me. I guess daggerbjective is would love to chat. If any of you are interested in helping and creating this vision, come along. We need many, many folks to bring this together and make it a possible truth for us. So yeah, thank you so much.
Speaker D: Yeah, cheers.
Speaker E: Thank you.
