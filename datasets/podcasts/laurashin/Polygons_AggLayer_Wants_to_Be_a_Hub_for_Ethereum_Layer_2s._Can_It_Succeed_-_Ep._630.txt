Speaker A: What the AG layer enables is it basically like quarantines each chain and it says okay, even if this chain has an unsound prover, the security assumption reduces to the case that it's the only chain in the ecosystem. So an attacker might be able to rug users by exploiting a soundness bug in that prover, which is the exact same cases as though the chain were running on its own without the shared deposit contract. But it fundamentally can't threaten funds that are that are locked in other chains on the aglayer.
Speaker B: Hi everyone, welcome to Unchained, your no hype resource for all things crypto. I'm your host, Laura Shin, author of the Cryptopians. I started recovering crypto eight years ago and as a senior editor at Forbes, was the first mainstream media porter to give a cryptocurrency full time. This is the April 9, 2024 episode of Unchained. Polkadot is the original and leading layer zero blockchain with over 2000 plus developers. And the Polkadot 2.0 upgrade will be a massive accelerator for the ecosystem, making it faster, more secure and adaptable. Perfect for gamefi and defi to build, grow and scale. Join the Community@polkadot.net work ecosystem community hey unchained listeners. As you know, it's hard keeping up with a fast paced world of crypto, so we've got just the thing for you. Subscribe to our free unchained daily newsletter at unchainedcrypto dot substack.com. youll get the latest crypto news in original articles from our reporters, as well as summaries of other happenings and bullet points. Plus our meme of the day, all curated and written by our amazing team. Its still your no hype resource for all things crypto, just in newsletter form. Sign up at unchainedcrypto dot substack.com. again, the URL is unchainedcrypto dot substack.com dot. Today's topic is Polygon and its new ag layer. Here to discuss our Mark Boiron, CEO of Polygon Labs, and Brendan Farmer, co founder of Polygon. Welcome Mark and Brendan.
Speaker C: Thanks for having us.
Speaker A: Thanks Laura.
Speaker B: So I actually don't believe we've done a show that just focused on Polygon, at least not in a deep dive format. So why don't we just start with a brief primer and your backgrounds. Brenton, do you want to go first?
Speaker A: Yeah, sure. So I'm Brendan, I work on ZK, R and D, mostly at Polygon. I joined Polygon actually as part of an acquisition. So I was previously working on a project called Mir, which was ZK focused l one that was acquired by Polygon in 2021.
Speaker B: And Mark?
Speaker C: Yeah, I'm Mark Bouron. I'm the CEO of Polygon Labs. And before being CEO, I was the chief legal officer at Polygon Labs, and before that, chief legal officer at DYDX, and before that, counsel to plenty of different companies in the space.
Speaker B: All right, and let's just give a brief description of what Polygon is before we dive into all the stuff you guys are up to.
Speaker C: Sure. So Polygon Labs is a development company that's been developing a suite of theorem layer two scaling solutions. And so the one that we are most known for is for having developed Polygon Pos, which is a proof of stake network. And we've also launched Polygon ZkVM, which is a full ZK secured roll up on Ethereum. And now what we're focusing on, in addition to kind of scaling through these different networks, is scaling between networks as well, which is where the AG layer comes up.
Speaker B: All right, so now let's dive into details on that. We've had this debate about monolithic and modular blockchains going for a while in the crypto space, and in February is when you launched this AG layer, which claims to offer the benefits of both. So explain what the AG layer is and how it offers both of those benefits.
Speaker A: Sure. So the Ag layer, I'll give, like, a technical kind of jargony explanation and then go into what it means. So the AG layer is a decentralized protocol that enables basically two things. So it enables cryptographic safety for cross chain interaction at lower latency than Ethereum finality. It allows you to safely send messages and execute transactions across chains very, very quickly. The second thing that it allows is it allows a bunch of different chains to all use the same native bridge. Users can bridge l one and l two native assets seamlessly across chains in the ag layer ecosystem. And so if we take a step back and think about, like, the current state of rollups, if I want to move funds or do something with my funds that are currently on Polygon Zkvm, I want to do a swap on arbitram. I need to route that interaction through ethereum. So I need to withdraw my funds on polygon zkvmdev. I need to wait for a proof to be generated. I need to wait for that proof to be verified and finalized on ethereum l one. And then I need to submit a deposit transaction on l one and wait for that deposit to become finalized before I can access my funds in arbitrary and so all in, this takes like 45 minutes. And that's like a state of the world that I think is incompatible with good ux and with taking an ecosystem that's currently fragmented, which is the ethereum l two ecosystem, and unifying it so that it feels like a single chain. And so when we talk about the modular versus monolithic debate, I think both sides sort of have a point. The monolithic side is right, that what we mean when we talk about scaling in crypto is scaling access to liquidity into shared state. And so if we're creating this ecosystem where we have a bunch of chains, but state and liquidity is fragmented across all of them, then we're not really succeeding in doing what we're setting out to do. And I think, by contrast, the modular side is right that a single chain can't accommodate the throughput required to fulfill this vision of the value layer for the Internet or Internet value. I think the way we describe the AG layer is where the aggregation thesis is the synthesis of these two views, like the ability to add block space in a modular way, but unify that block space so that it feels like you're using a single chain.
Speaker B: And like, just to talk a little bit about that user experience. So when people are using the ag layer now, is there any difference? Like when they're, I guess, kind of on ramping into there, is there something new or different, or is it really more just about the experience? Once you're in it, that's different?
Speaker A: Yeah, I do think we have to set expectations. The AG layer is not currently live. What's currently live is a shared bridge that's shared by two chains. In the ag layer ecosystem, when the ag layer is deployed, and when we have this infrastructure that allows for fast interactions between chains, then I think your description is correct, where within that ecosystem we can solve fragmentation and we can unify, and we can have super low latency interactions, but there will always be friction. When you're coming from l one, there are some caveats there. We could have what's called base sequencing and bass roll ups, which can improve that a little bit. But in general, the ag layer is designed to erase fragmentation and friction once you're in the ecosystem.
Speaker C: I think one thing that's worth pointing out, Laura, is that when we talk about this friction going from l one to l two, I think it's a friction that's not really a comparison to l one's that is fair in the sense that if you think of how do I actually get onto an l one? The answer is you go to an exchange and you buy the l one token, and then you move it to your wallet on l one. This is the same thing for l two s. You just have this added advantage that you could go from ethereum, but most of the time for l two s, what you're going to do is you're going to buy eth on an exchange, and then you're just going to send it to the wallet on the l two, and you're going to start using it. One of the benefits of the ag layer, by bringing this all together in this aggregated way is that all you would need is one chain that is part of the ag layer to have that on ramp from that exchange for all of the other chains in the ecosystem to be able to receive that. So, for example, right now, we've got Polygon, Zkvm and Astar that are both on the unified bridge that will be part of the ag layer. By simply going straight to Zkevm or by one connection to Zkevm or one connection to a star from a centralized exchange, you could then go to either of them. One good example of how this is going to play out once Polygon PoS joins the AG layer. Polygon PoS has been around for so long that it has connections to all of these exchanges. You'll just be able to on ramp directly through Polygon PoS to any of these other chains that are part of the ecosystem. You end up having that same feel without ever needing to go from ethereum to l two in the first place.
Speaker B: And then if we compare to similar systems, there's optimism with its super chain. And then Cosmos has been working on interoperability for a long time. And so some of the ideas that form the basis for the ag layer aren't necessarily new. So how would you differentiate the AG layer from those other efforts?
Speaker A: Yeah, so the way that I would draw a distinction is, I think ZK technology unlocks a lot for the user experience and for how we think about bridging and movement of assets between chains. If you think about cosmos, when I bridge between different chains in the cosmos ecosystem, I'm not getting the same assets that are fungible on my destination chain. So if I'm bridging from, let's say, eth, from one chain of the cosmos ecosystem to another, what Im getting on the destination chain is a wrapped synthetic version of ETh thats not necessarily fungible with the native version of ETh on that chain. With ZK technology, what we can do is we can allow chains that settle on Ethereum to share a deposit contract that enables us to allow users to take l one native and l two native assets and bridge them seamlessly across chains. And further, it opens up the space of the cross chain interactions that can happen. If you think about an atomic bundle of transactions, maybe I'm some defi trader, or I want to do a cross chain arbitrage. And it's really, really important for me that all of the transactions in this bundle of transactions that I'm submitting actually execute successfully, or my trade might not be market neutral, or something bad could happen to me. This is not really possible in the cosmos ecosystem, but it's something that we can enable in the AG layer, and it's something that we can guarantee safety for, because the AG layer gives a cryptographic guarantee that for me, as the operator of a chain, my chain can only be settled to Ethereum if it's consistent with, with the other states of chains that exist in this ecosystem. And so I really think, like, ZK technology is sort of the missing piece that unlocks, like, a much, much better user experience versus cosmos. And obviously, I think in the optimism case, if you think about what the drawbacks are for fraud proofs, in order to do bridging through the native bridge, we need to wait for the challenge period for fraud proofs. That limits what we're able to do in terms of moving assets seamlessly across chains, in terms of triggering cross chain interaction without relying on third party bridges. I think if you look at what the actual characteristics are from a security and from usability perspective, third party bridges are not really as great a solution as native bridges, because they expose users to security risks. They're capital inefficient. They rely on there being liquidity on the destination chain. So if we think about the end state of crypto ecosystems, which consist of hundreds or thousands of chains, I think the ag layer vision is one that's much more likely to make that a good user experience than different competing views of the world.
Speaker B: So primarily, is the benefit really just the lack of that delay period, like the low latency? Is that the main thing that gives that seamless interoperable experience?
Speaker A: Yeah. So it's low latency and also fungibility. So the fact that I can just take ETh on ZKVM, I can bridge it to the OKX chain, do a bunch of stuff, and I'll get eTh on the OKX chain. I don't need to worry about there being a liquidity provider that's able to swap into the native version of ETH on that chain that's used as the pair and all the amm pools. I can just not even think about the boundaries or the trust boundaries between chains. I can operate as though it's one single, expansive, unified block space.
Speaker C: When you think about that in the context of DeFi, it does allow for much deeper liquidity in that ecosystem than what you would get otherwise. And this is what Brendan was touching on. When you have a wrapped version of an asset, it doesn't sit in the same pool as the native version of that asset. And therefore, liquidity in each of those pools can never be as deep as if you just have the native version all sitting in one pool. And so when it comes to the execution experience or price experience that DeFi users can get, it's much better in one that has a native asset across the entire ecosystem, which is what monolithic chains kind of receive and modular chains don't. And the aG layer kind of brings it together. So you do get that in kind of the modular state.
Speaker B: Can you just walk through kind of what this will look like when there are more different apps on the AG layer or more chains? You know, what's an example of something that somebody could do in that environment that isn't quite possible today?
Speaker A: Yeah, sure. So I think one thing to sort of point out is the AG layer really works in a complementary way to what we call emergent coordination infrastructure. So we can think about the AG layer as being this foundational piece of infrastructure that's providing a cryptographic safety guarantee and also enabling chains to share a bridge. Then on top of that, there are coordination mechanisms, like shared sequencers or relays or builders that are building blocks across chains. One thing that you could think about is if multiple chains in the polygon ecosystem are using a shared sequencer, then at any particular slot or block, they're able to share a proposer. And that proposer is able to delegate the ability to build blocks to a single builder. By builder, we just mean some other party that's not necessarily a validator of any particular chain, but is able to operate in a much more sophisticated and professionalized way than a validator with low hardware requirements. What we can think about is, at a particular slot, this builder is actually simultaneously building a bunch of different blocks across a bunch of different chains. And maybe a user is interacting at the same time across a bunch of different chains. We can think about. Maybe I keep my funds on polygon ZKVM, and I want to mint an NFT and then swap it and take out a flash flow or some very convoluted and sophisticated defi interaction that's happening across a bunch of different chains. Then that builder is executing all those transactions simultaneously across a bunch of different chains. And so it can determine if any of those transactions fail, and if they all succeed, it can package those transactions together as a bundle and include those in a block that's executed across all different chains that are sort of within the zone of this shared sequencer. So I think that's like an example of chains that are being unified and really feel like a single consolidated block space versus the current mode of interaction. In this example would be, okay, I have funds on one chain I need to bridge via third party bridge, or via native bridge to the NFT chain where this mint is happening, and then I need to bridge back to the defi chain. It just ends up being, first of all, a very complex process, because the user has to reason about how to get to all of these apps that are actually located on different chains. It also ends up being an expensive one, because at every step I need to either accept a huge amount of latency, which could impact my ability to execute my transactions, or I need to pay third party bridges. I think this example illustrates that users will be able to reason about applications and the applications that they want to use. They won't need to reason about where those applications are located in blockspace across this ecosystem. I think that that's analogous to how the Internet works. I don't think about if I want to access a website in Europe, I don't think about bridging my packets across the Atlantic Ocean to get to this website that's hosted in Europe. I think about using the Internet as a unified environment for information, and I think of the AG layer as being a similar concept, where it's a unified environment for state and liquidity.
Speaker C: When you think about this from a really practical perspective, if you take an example, you can imagine an exchange like OKX, they are joining the AG layer, which is actually happening. That's not an example from that exchange. What a user would do is they would send assets over to the OKex chain and they might be holding, they might want to hold DAi, for example, they could exchange some eth to Dai on OKxchain. Then in the same user interaction, go ahead and transfer that Dai to another chain to Astar, and purchase an NFT with that Dai on Astar. And in that same user interaction, kind of bring it back to their wallet on that OKX chain. And so that ends up being kind of, again, the same feel that you would have in a monolithic chain where you just have one user interaction and you can execute multiple transactions. It's doing the exact same thing, but on chains that have different attributes. And the question is, why does this matter? The answer is simply that, that NFT that you purchased sitting on that specific chain, probably for most users, can create a much better user experience given that they control the entire environment on that chain. They also don't have to worry about things like cost spiking because they control their own block space. You end up getting those benefits of a great user experience, but without giving up the interaction of just one click to execute these transactions, as you would be able to do in a monolithic chain.
Speaker B: Just to understand about the Okex chain, which I guess is called X one, is that similar to Coinbase's base where they have, you know, their own.
Speaker C: Yeah, that's right.
Speaker B: So something else that I was curious about, from what I read, it seems even Solana virtual machine chains can use the aG layer, is that correct?
Speaker A: Yes.
Speaker B: Yeah. Can you, can you talk about how that works? Like, I don't know if it's literally just what you described before about the, you know, the block producer. What, I don't know what you called it, where they're simultaneously doing this on multiple chains, is that. Just talk a little bit about how that works.
Speaker A: Yeah. So I think that's a really good question and a good thing to point out. The AG layer fundamentally is not. It tries to not be opinionated whatsoever. So a chain can join and they can use their own token for staking and for gas. But one of the things that we thought was really important was enabling them to use their own execution environment. So for certain applications, the SVM might be a much better choice than the EVM or a custom execution environment that's written in rust and maybe compiled with a general purpose CKBM so they get approver for free. Might be a very good trade off for a certain type of application. For us, there's no requirement to use the EVM or ZKVM, and we've architected the system such that each prover or each execution environment that joins the AG layer doesn't create an additional security risk for any of the other chains that are using the shared deposit contract and shared bridge of the AG ler. So I think that's a very nice property because it means that developers are completely free, not only to customize how their application works, but actually to customize the environment that it runs in. And to Mark's point, I think that this will be, like, very powerful thing for chance.
Speaker B: So we've talked about this proof for a few different times, and I don't remember if we've defined it, but it's for the zero knowledge proofs. It's like creating those proofs, right?
Speaker A: Yeah.
Speaker B: And so they're doing it for all the applications or all the chains, no matter what kind of chain it is.
Speaker A: Yeah. So the chain would create or would generate its own validity proof. But I think you're touching on a very sort of subtle thing that's actually really important. And that is, let's say that we didn't have the aglaya, and all that we had was a shared bridge or a shared deposit contract, and we wanted to support a heterogeneous set of execution environments that would each have their own prover. The problem is that if we want to make this permissionless, then for every prover that we admit into this system, there's some probability that there's an undetected soundness error or some sort of bug that could be exploited by an attacker. As we admit more and more chains into the ecosystem, that probability goes up. And so it actually ends up being a very bad thing, because by the time we get to prover number 1000, there's actually a fairly high probability that at least one of those provers has a soundness bug that can be exploited by an attacker. What the AG layer enables is it basically quarantines each chain, and it says, okay, even if this chain has an unsound prover, the security assumption reduces to the case that it's the only chain in the ecosystem. An attacker might be able to rug users by exploiting a soundness bug in that prover, which is the exact same case as though the chain were running on its own without the shared deposit contract. But it fundamentally can't threaten funds that are locked in other chains on the AG layer. This is, I think, a really important and maybe underappreciated aspect of the design of the AG layer, is being able to support two things that are intention. First, having a shared deposit contract, which we think is really important for asset fungibility and for user experience, but not imposing security risk or security cost on other chains that are in this ecosystem.
Speaker B: Okay. Yeah, I mean, that sounds like a really smart architecture, because obviously we've seen in crypto that there's a lot of crazy stuff that can happen when it comes to security. So you've also launched, so you know, I saw you've launched this type one prover. I saw also mentioned a type two proverbs. Can you talk a little bit about the distinction between those, Mark, I'm sorry.
Speaker A: That I'm like talking more.
Speaker C: It's okay. While we're in the technical stuff, it's all you.
Speaker A: So the types of ZKV improvers are, it's actually a framework that was proposed by Vitalik. And so it's this spectrum where we go from having complete EVM compatibility. So a type one prover is completely EVM compatible, even to the point of being able to generate proofs for existing EVM chains. So you can take this type one prover and you can generate validity proofs for the Ethereum L one for the polygon Pos chain, for the avalanche EVM chain. But there's sort of a trade off because a lot of the way that the EVM is designed is not really friendly to zero knowledge proofs. So it uses cryptographic primitives like catch act or like the encoding for the Merkle tri, which are a little bit more expensive to generate proofs for. And so there's this trade off wherever you have, on the one hand, a type one prover where you have complete EVM compatibility, and then it goes to a type four prover where you don't really have that much EVM compatibility. But maybe your system is built in such a way that the proofs are a little bit more efficient to generate. Up until this point, Polygon's been really focused on this side of the spectrum, where we have a type one prover and a type two prover. A type two prover basically gives you an identical developer and user experience to the ethereum version of the EBM. It's just, it can't be used to generate proofs for existing EBM chains. And so with the release of the type one prover, we were really excited about this, because with the proving system improvements that polygon has made, we were actually able to generate type one ZKVM proofs at a much lower cost even than some like type four ZKBMs that are supposedly optimized for proof generation, but theyre running on less efficient proving systems. And so when you actually look at the tangible cost, it ends up being very, very cheap. So for us right now, its between one and two tenths of a cent to generate a proof for an average EVM transaction or an average Ethereum transaction. But we expect this to, to decrease really dramatically in the coming months.
Speaker B: And so there's a chain called Kanto, which is on Cosmos and had previously announced plans to become an Ethereum based chain using your CDK or chain development kit. And they ultimately decided to stay on Cosmos, but instead use this type one prover to plug into the ag layer. So from working with them, how would you advise teams that are in a similar position? You know, how should they think about their options? Like, when would it make sense for them to shift to Ethereum? When would it make sense for them to stay as is and use the prover to just plug into the ag layer? What are your thoughts on that?
Speaker C: Yeah, I mean, I'll jump in here. And my view that I always start with is like, what are you trying to achieve for your users? And does your environment currently allow you to do that? And this is like the kind of non technical version of it. But ultimately, my view is that we don't need to push somebody towards the EVM or SVM or any type of environment specifically. The question is just what do you want to build for your users, and where can you build it best? And we ultimately do not care where a chain decides to build that. All we care about is are we able to give you the ability to interact in a seamless way with chains that are also building a very specific environment for their sets of users? As long as they're both able to do that, we want them to be able to interact in a seamless, low cost and low latency way. That's what we're ultimately allowing to do with the AG layer. Now, the type one prover ends up also allowing them to do that in an EVM compatible environment or an EVM equivalent environment, whereas the AG layer obviously doesn't care. And so our view is kind of like the type one proverbs there. For those who want to use it, it adds a layer of security that you might not have otherwise, but it doesn't mean that you have to use it. And we don't really care, frankly, whether you use it, because ultimately you get the benefit of these cross chain interactions through the AG layer in a secure way due to the specific mechanisms that Brendan was touching on earlier.
Speaker B: All right, and then last bit that I just want to ask about, you also announced a new proving system called plonky three. I didn't have enough time or just couldn't understand. Exactly. So this is, I guess, one of the systems for the zero knowledge proofs, but I didn't know how that fit into everything else that we've discussed.
Speaker A: Yeah, sure. So I think a good way to look at it is like when we talk about the prover for a ZKBM, that's like you can think of that as like we are running the EVM inside a zero knowledge proof. Some proving system is generating proofs that show that the execution of that EVM or whatever vm was valid given a certain output and a certain set of transactions. Plaquey three is the proving system. That's the underlying technology that allows that to happen. Yeah, we previously released Plonky two and I think it might be like the plonky branding might be some of the worst branding in the entire industry. I'm not sure that we're successful in kind of naming those proofing systems, but we're really excited about Pocky three because Pocky two when it was released, was I think between a ten and 100 x improvement on all other proving systems that were compatible with the EVM that can be verified on ethereum. Plotkey three is between five and ten x faster than planky two. This ends up just meaning that we can generate proofs faster and at lower cost for users. Given that proving cost is passed on to users, it just reduces the cost of using systems that use your knowledge proofs.
Speaker B: All right, so in a moment we're going to talk about how they get a critical mass of applications on the ag layer. But first, a quick word from the sponsors who make the show possible. Polkadot is the original and largest layer zero blockchain with over 2000 plus developers and the anticipated Polkadot 2.0 upgrade will be a massive accelerator for the ecosystem. Upgrading the infrastructure with eight times higher transaction throughput and twice as fast block times. Perfectly tailored core time for the needs of every protocol, trustless bridges internally and into ethereum, cosmos, nier, binance, smart chain and revised tokenomics, and the implementation of a token burn to reduce inflation. Perfect for Gamefi and Defi to build, grow and scale with one of the most active crypto communities in the space. Polkadot recently announced a partnership with mythical Games, bringing top games like NFT rivals with over 650,000 players. 43 million transactions to pave the way for Gamefi and the Polkadot ecosystem. Get your web three ideas to market fast with economics that work for you. Think big. Build bigger with Polkadot. Join the community@polkadot.net. work ecosystem community back to my conversation with Mark and Brendan. So as we discussed, you guys have this AG layer, but it's not quite up and running because there's just a few applications on it. So how will you get a critical mass of applications on it? What are your plans for generating a network effect on AG layer?
Speaker C: That's a really good question. I'd say that the one important thing about the AG layer is that it is very neutral. And what we mean by that is that this is not a thing that is about Polygon, it's about bringing all of these modular chains together. And that really resonates with developers because ultimately every developer who is building a chain knows and recognizes that there are downsides to sitting alone in their own environment. This is like, this is, there's actually, this is true even for like a monolithic chain that can become part of the aglayer. But it's even more true once you start getting much more specific to an app chain. And when you think about that app chain, what they need to do is they need to recreate everything that exists in another blockchain that their users need. That is a big burden that if you're talking about thousands of chains existing and we're really not that far away from that at all, you're talking about a lot of repeated lift. And these developers, they don't want to do that. They don't need to do that. When you present them with the aglay and you say, here's a solution that is going to allow you to not have to build an entire defi ecosystem when all you care about is being able to create an amazing NFT experience for users, that is something that is very desirable. And it's not really honestly about selling it, it's about showing them why it is, why it is that they can actually do that in a safe way with low latency. That creates a good user experience. And when you show them those three things, they're like, oh great, I don't need to go build this all from scratch. And it becomes kind of a very easy decision. And that's why there is a lot of interest in the ag layer for that reason. And so that's what we call the really app chain specific discussions. I think on the other end of that you could take things like l one's themselves and that's one of the things that's interesting about the ag layer. Right. So we do work on layer twos, but the AG layer isn't just welcoming to layer two s. It can really welcome anyone into the environment. When you think of an l one, ultimately what you have is a chain that has a lot of advantages for their users from what's called a unification perspective. But ultimately there's things that those users probably want, that exists in another environment, and they have to go through an even more painful process than the one brendan's really been kind of trying to describe within the Ethereum world. When you're going from Solana to Ethereum, it's much more difficult. But when you start thinking about l one s and you say, okay, you can actually share users with Ethereum and liquidity with Ethereum, then it starts becoming a lot more desirable. And so do I think that the biggest l one s will become part of the ag layer, like right up front? Like, of course not. Do I eventually think that you get to a point where more and more l one s, and we are having discussions with multiple l one s who are very interested in joining the aG layer. The reason for it is simply because like smaller l one s realize that there are a lot like the advantage ethereums always had is a lot of users and a lot of liquidity. And so why wouldn't they allow their users to participate in that? And when you think of the ag layer, we talk about it in the sense of like unifying liquidity, but you're actually unifying users as well. So if you think about it, you're saying, hey, I have a lot of liquidity on my chain and I need to attract users there. Well, one way to do that is directly attract users to your chain. Another way to do that is say, hey, I'm going to create the deepest liquidity. So that when somebody wants to do a defi transaction on chain, you know, twelve on the ag layer, then that transaction is going to get routed through that pool. And by default, when you join the AG layer, you're acquiring a set of users that you wouldn't have otherwise, who are going to interact with your chain via the AG layer. And so ultimately you think of like that end of the spectrum of l one s, they're like, okay, I can maybe get my users to get deeper liquidity somewhere else, or a different kind of application that they can't access, or I may actually acquire more users where it is that I have an advantage within that ecosystem. Like that is desirable. And so ultimately I kind of view it as like the actions makes the most sense, and then with time it kind of like moves along to where you start looking at more monolithic chains that say, hey, I want to take part in this environment as well.
Speaker B: That's so interesting that you're also talking to layer ones. But are you noticing? Because I would imagine that there's certain types of chains or certain types of applications that kind of are more interested. Are you noticing any trends in that regard?
Speaker C: I. I mean, very interestingly, I don't know that there is a trend. I would say that when we look across general purpose chains, again, the biggest hesitation for an l one is to say, I don't look like an l one anymore. If I joined the ag layer. Except it's not really true. You can still have your own consensus. You can still have your own execution environment, that's not really true. And so when the initial reaction that you get from most of them is like, hey, not interested. And then you dig into it with them and they start realizing, oh, I can actually truly remain an l one. Okay, this is something I actually want to talk about. And so l one s are the most difficult conversations. But once you start having them, they realize there's a benefit. But then when you start looking across, whether it's defi naturally comes to folks, right? So, like, everyone in defi knows that, like, hey, if you can get access to more liquidity elsewhere, I want access to it. Those are very, very easy conversations to have, but they also exist in the gaming environment. So people are creating specific chains for games. They understand that if there can be deeper liquidity in nfts, or their users can receive deeper liquidity on another chain, that's better. Or if they can attract users from other chains, that's better. Those have been pretty easy conversations. They kind of deepen. Similarly, I really don't know that there's an area that I look at beyond l one's, where it's more difficult conversations. But again, even the l one s, once you start digging into why it is that they can remain independent, and maybe that's a point to touch on really quickly. That's important. Which is like, one of the things that we really have been focused on with the AG layer is allowing every chain to remain completely sovereignty. This is something that was really important in the cosmos ecosystem, and they did it really, really well. Maintaining that sovereignty for a chain, the way that I look at it and approach it, is it's one of those things where you should be allowed to stay your own independent chain with your own independent community, and benefit from the other chains that are in the ecosystem. Alternatively, you also have the option of becoming part of this bigger ecosystem where you can opt in socially, not just from a technical perspective, but like socially. You could say, hey, we want to actually build together on top of the ag layer. And when we actually talk to each other, one chain talks to another chain, we can actually build even more interesting things for all of our users, but that's not something that anyone's forced into. They can remain completely sovereign and build their own thing.
Speaker B: This is just so fascinating because it's kind of interesting. It's like you have ethereum as a base layer, and then theres this layer two on top, but then its like you have a cosmos on top of that. A cosmos ish type thing on top anyway. But its a ZK anyway. Its just fascinating. But one thing that I wanted to ask was, I guess I didnt either think of this before or didnt come across this, but are the transactions private or are they public?
Speaker A: They're public. You could have. One interesting thing, is that with this sovereignty over execution environments, you can have chains that are private, that can interoperate really seamlessly with public chains. And so Polygon maiden is one of these that's really looking at client side proving and privacy. And I think it's really powerful to be able to have a highly scalable private zone that you can access from anywhere in the ecosystem.
Speaker B: And out of curiosity, if this ag layer is based on ZK technology, why haven't more people thought about having private chains?
Speaker A: Well, I think that a lot of the way that chains are architected and a lot of the tooling that currently exists for EVM chains and SVM chains really depends on transactions and application state being public. I think it's just a lower lift for developers to work in the paradigms that they are familiar with and already have tooling for. I think that drives the move toward more public chains.
Speaker B: As far as I understand, Polygon's intention to build using ZK technology started back in early 2023. Why did you decide to do that at that point? What was it that you thought that this technology would enable? That was the future of the crypto space?
Speaker A: Yeah. So I think the move internally with Polygon towards ZK started actually in mid 2021, because I think there was the realization internally that the Polygon PoS chain was seeing a lot of traction, and it was fulfilling a short term need that Ethereum had, which was to scale and to escape the limitations of the l one. But I think that the founders of Polygon deserve a ton of credit for recognizing that this was not something that was going to persist. The technology that the Polygon PoS chain is built on was not going to be the technology that Ethereum would be operating on in 2030. They made a really big bet on acquiring three ZK teams, of which my project was won we were working in the background for 2022, and then 2023 launched the ZkVM, which was a type like 2.5 ZkVM. And yeah, we've been really focused on both pushing the underlying technology through R and D. So that's where planky three comes in. I think that it's fair to say that Polygon has really been setting the standard for ZK performance and efficiency in the industry. But also I think it's about building on top of those core technologies and so enabling things like the AG layer and ZKVMs and Polygon Maiden. I think those are kind of the two prongs of the Polygon. ZK strategy is like very much deep tech, R and D of underlying proving systems, and then taking that technology and figuring out what we can build to actually serve users.
Speaker C: And one of the things that we don't actually like, that people don't really think about is you kind of had two options at the time when it came to scaling. It was like the optimistic roll up kind of approach, or it was the ZK roll up approach. And when you analyze that at the time and kind of took a deep dive, it was actually pretty unclear which way would be better to most people. But I think that with time, and one of the things that we've realized is that the costs and performance of proving have increased so drastically, or the cost has decreased and the performance has increased so much, that I think a lot of those concerns that existed at the time, like very quickly the team realized, actually we need to double down on the ZK technology because it really actually is the path forward. And I think as Brennan was touching on earlier with the type one prover, it's really coming to fruition in that we are even internally, and I think we have the best ZK researchers maybe in the world. I don't know, Brendan, maybe correct me, but I think that most people view it that way. We are even outperforming what we expect internally. It's just become very evident with time that the ZK tech will get to the performance levels that you need to be able to build purely using ZK tech, without the downsides that you get with the optimistic roll ups.
Speaker A: Theres this really interesting narrative that exists where people look superficially at the costs for users. In optimistic and ZK systems, they say, okay, well, in ZK systems, maybe you can save a little bit on the margins and call data because you can do compression and state diffs and you don't need to post signatures on chain. But I think the proponents of optimistic rollups would say that costs will be strictly worse for ZK rollups because you need to pay for proving, you need to pay for the verification of proofs on ethereum. And in aggregate, this adds up to be strictly greater cost than just posting transaction data. I think that's a very superficial view because it misses the aggregate costs that are borne by users. The biggest cost that's specific to an optimistic roll up is this imposition of a delay in withdrawing from the chain on most optimistic rollups. You need to wait for seven days in order to withdraw funds via the native bridge, and users obviously are unwilling to lock funds and so they use third party bridges to avoid this delay, to avoid the requirement to lock up funds. But if you look at what the aggregate costs are that are borne by users to use third party bridges, like on arbitrum, for instance, there has been like a total of like between 35 and $40 million depending on what the price of ethos that has been borne by users to avoid the withdrawal period. And if you compare that with the cost that it would take to just prove every transaction on the arbitrary mainnet using the Polygon type one prover, it's like 100 x cheaper to just prove all of those transactions and get rid of the withdrawal period. If you look at aggregate costs, then ZK rollups are actually much, much cheaper than optimistic rollups, and they will continue to get cheaper as we improve the underlying technology and reduce proven costs further.
Speaker B: Yeah, I think some of the groups maybe that went down the other path, I don't remember. I feel like there was a very long ago blog post by Vitalik where he kind of like talked about the time skills on which he expected different things to happen. And I sort of feel like this is. Yeah, it's just playing out a lot faster than maybe people expected. I did want to ask about the ZKE EVM roll up, which it is in beta, but there was an incident on March 23 where it stopped processing blocks. Can you explain what happened there?
Speaker A: Mark, do you want to take this or should I?
Speaker C: I'll let you take it, given the technical description that you'll.
Speaker A: I think when you look at the deployment of l two s, there's usually a period of like. And this is true for optimistic rollups as well. There's like a period of a year where teams realize that the configuration that they launched their roll up in, specifically with regard to the client, is maybe not the best choice for like the long term maturity and sustainability of that roll up. And I think just being like totally candid, there were decisions that were made in the Polygon ZKVM rollup, specifically around the client and using a custom client that have led to technical debt that we're currently addressing. And so the, the specific outage was just an instance where there was a bug in the client that affected the prover and we just needed to mitigate that. But I think that as the technology progresses, and we've already seen this with the type one prover, it allows us to use existing ethereum clients. And this is a move that I think arbitrary and optimism both went through where they launched with custom clients and then they moved to existing Ethereum clients. And we're sort of mirroring that process. It's just a year or two later because we launched a year or two later. But I'm really, really optimistic and I feel really positively about the long term health of the ZKVM roll up as we make those changes and move to existing clients, move to more mature proving technology. And so I think from our perspective, we are really, really happy to continue to bet on the exponential development and growth of ZK technology. And I think that that will just play out on like longer timeframes than, you know, the first year after launch.
Speaker B: So the week that we're recording, Mark, you set up a conversation about layer threes with a tweet that said, quote, l three s exist only to take value away from Ethereum and onto the l two s on which the l three s are built. You do not need l three s to scale. Explain what you meant by that.
Speaker C: Yeah. So one of the things we talk about inside Polygon is why have l three s when you can have l two s? And the thought process here is why do we need l two s in the first place? And the answer as to why this was contemplated by Vitalik and others in the Ethereum community was because the l one, the way it is right now, is great for decentralization and great for security, and it's not great for scalability. We needed something to scale, and that's where the l two s come in. When you look at how the l two s are doing from a scalability perspective, what you see is they're actually doing pretty darn well and they're continuing to improve all the time. And so when we fast forward and we look at like a year from now, we say, are we going to be at a point where we can scale ethereum to its limits through l two s at a cost and user experience that is unique to the apps and the users as they need it. The answer is yes, you will 100% be able to do that. People talk about l three s for native gas tokens. Polygon Cdk, which allows for l two rollups, allows for a native gas token. Your own execution environment. You can create your own execution environment in l two, you can go through the reasons for having an l three, and l two satisfies them like every time. And so the simple point is that when you're trying, when somebody says I want l three s to build on my l two, what they're saying is they want to capture more value on their l two. And I have no problem, by the way, with l three s. L three s will launch on Polygon Pos, they'll launch on Polygon Cdk chains like they'll launch on top of Polygons EqM. L three s can launch as much as they want if somebody finds it desirable. It's a permissionless system and it allows people to test. So I have no problem with that. The statement was really about l two s who are trying to capture broader, more of the market. And if you think about l two s doing that at scale, in my opinion, it becomes problematic for Ethereum when everything gets built as an l three on top of one specific l two rather than across l two s, all of which are connecting to Ethereum.
Speaker B: Yeah, well, I did want to ask about when you said that you felt the layer twos could scale quite well. You probably saw how after Dunkun transactions did increase on base, but also the fees went up quite a bit. I just quickly pulled up an article saying that they went to about $0.09. So as you know, there's this layer three Degen on base. And I was curious like, you know, do you really think base isn't necessary or, sorry, Degen isn't necessary for scaling or on that chain? Or is it more just about the fragmentation issue or what's your assessment of what's happening there?
Speaker A: So the issue with fees on base was related to congestion prices. So their instance of the EVM can only process a certain number of transactions per second. And so if there's sufficient demand then they will see fees increase. And those fees are being driven by congestion on the EVM. Theyre not being driven by the cost of data availability on ethereum or obviously not proving cost. I think to Marks point, if you think about the topology of the ideal scaling setup for ethereum, actually l three s have have a real downside, which is that they are not like each l three is connected to the specific l two that it settles to. It can't directly access the state or liquidity on an l three that's not also settling to that l two. When we think about the ag layer, fundamentally, the premise is that we can add block space in an l two configuration, so every l two on the aglaya or l two supported by the ag layer will settle directly to ethereum. And those l two s can all share state and liquidity seamlessly between themselves. They don't need to route their transactions or messages or requests via l two s to access a particular l three. And so I would say that the ag layer is not like, it's not an l two, it's a layer, in the same way that a da layer or an execution layer is a layer. I think the term layer is sort of overloaded, but it enables the same level of connectedness and composability that an l three would have with an l two or with other l three s. That settled to that same l two. But instead we can just make everything an l two that settles to ethereum. I think it just reduces complexity, because you don't need to have an l three that settles to an l two before it settles to ethereum if you want to withdraw funds to l one. And we can preserve this universal connectedness property that we don't get necessarily with all three s. And if you take.
Speaker C: This to degen specifically, you could look at that and you could say dgen could have just launched with all of the same features and extremely low costs as an l two. As Brennan said, the issue with base is actually representative of why l two s make a lot of sense, which is that I don't think you're going to see most l two s be general purpose l two s. You're seeing that now because it's just frankly where we're at in the ecosystem. But you're going to see most l two s that are going to be specific to something, just like Degen, right? Like it's going to be specific to something. And when that's the case, the amount of time transactions that users, that it takes to actually drive a chain, to actually be bottlenecked in any way whatsoever, takes much, much, much longer. And you don't get into the issues that you got in with base, for example, which is a throughput issue as a result of lots of different use cases actually being on one. And so Zgen can just sit on its own for its own specific purpose. As an l two without the congestion problems that you get from a general purpose chain.
Speaker B: Okay, well, now let's talk about how there's two big developments on Ethereum that are happening or will happen. One is as just came up, there's now this modularity with data availability layers that are coming out. Obviously, Celestia, there's avail. All of these allow layer twos to scale more easily. Another one is Eigen layer that will allow restaking of Ethereum to provide security to other chains. So I wondered how these developments mesh with the AG layer, if at all.
Speaker A: The nice thing about the AG layer is that it's designed to provide chains the flexibility and the choice to use whatever data availability solution they want to. So if a chain wants to retain maximum Ethereum security, it would use Ethereum as its data availability layer. If it wants to optimize for cost or for some other property, it could use Celestia, or it could use like a data availability committee in the validium mode. And so we're fundamentally agnostic with respect to the choice of data availability layer. I think Eigen layer is a really interesting case. I think actually that the crypto economic sort of trust guarantees that are provided by Eigen layer are actually not as good a fit for rollups, which can offer a greater level of trust via validity proofs or fraud proofs. But yeah, I think that validity proofs are going to offer a better trust guarantee than the crypto economic security that comes from restaking. So that's where I think, I think Eigen layer has really, really interesting applications in DA and in other instances where it's difficult to have validity proofs or fraud proofs, but I think for roll ups they will still be dominated by validity proofs specifically.
Speaker B: All right, so obviously we spent most of the episode talking about the AG layer, but are there any specific things you're focused on with your proof of stake chain?
Speaker A: I think the upgrade to ZK, actually, it's the end of an era for Polygon, because when I joined Polygon, it was in the middle of this war over l two definitions, and specifically whether or not the POS chain was a side chain or an l two. It's going to be strange now because the POS chain will become an l two, because we will be eventually generating validity proofs that show the validity of every transaction. I'm not sure that we've decided whether it will be a validium or roll up given the reduction in DA costs, Ethereum, but yeah, I think that'll be a really exciting step for the POS chain.
Speaker B: And when do you think you might have a decision on that.
Speaker A: Mark? Do you have a sense of the. I think it's ultimately up to the community and to the users of that chain when the upgrade happens, but I think we would love to see it sometime this year.
Speaker C: Yeah, and one of the other things that we're focused on for polygon Pos is bringing it to the aG layer as well. One of the things that the AG layer is going to benefit from, I was touching on a little bit earlier, is a very mature chain that has so many connections into different centralized exchanges who can ultimately make it very easy to onboard to all of the chains into the ecosystem. In a few months, we'll be connecting Polygon POS to the AG layer, which will allow for that to happen. The ZK upgrade will happen after that. But the general idea then is you'll have all the liquidity, all of those applications that are available in Polygon POS and have been building up for years will be available to all the other users using other chains in the AG layer.
Speaker B: All right. EIP 4844. Dankun hasn't gone live on polygon yet. I guess you're in the midst of some other upgrades. So when do you expect it will go live? And what else is on the short term roadmap for Polygon?
Speaker A: Yeah, so I think in the next four or so weeks, we've been really focused on the aG layer and sort of building around that versus like the Polygon ZKVM chain has been in beta and there have been identified issues that we've been kind of focused on there. I think in the short term, upgrading the proving tech, moving to 48, 44. I think that we will see between a ten and 20 x reduction in improving cost as we move away from the existing proving tech. I think that's something that's really exciting. Getting to the point where we have sufficient liability and it's ready for greater adoption, I think will be a really exciting step for that chain.
Speaker C: Yeah, and I tend to agree. We've talked about the ag layer a lot, so I won't dig into that more. But once you get past that, I think bringing polygon Pos to the AG layer is going to be very important for other chains in the ag layer. And then beyond that it is Zkvm. And we've spent a year, a lot of people probably have recognized this as we haven't pushed it very hard. And it's because of some of the things that Brendan's been talking about, and in the next couple months is when we are actually addressing all of those. And so having a system that will have been kind of will have matured over kind of the last year, and then bringing kind of new tech to it will bring us to a point where I think we'll see a lot more excitement around polygons, EKVM. I think we'll be pushing it. I'm confident we'll be pushing it in a very different way than we have been. And I expect that to bring a lot more use than what we've been trying to do, which, frankly, has been, let's just let the system mature. And so once we get past that, I think then the entire focus is going to be on continuing to grow the AG layer with CKVM and Pos alongside of it.
Speaker A: Oh, I was just gonna say, I think the onboarding of POS to the aG layer is sort of a symbolic step, because if you think about, there's sort of this structural flaw that I think exists for l two s, which is like Mark alluded to this earlier, but you have to not only launch a chain or do the thing that you have a comparative advantage in, but you also have to play these games around tokens and airdrops and liquidity mining that will, like, bootstrap, enough interest and liquidity and momentum for your chain to actually succeed. And I think that this is like a structural flaw for ethereum. Like, it's a process that's really, really inefficient. And it's something where like, I think it's an impediment to builders being able to just focus on building their defi primitive or their game or their NFT collection. They also have to focus on these token games and bootstrapping liquidity and building a financial ecosystem in addition to a product and polygon. PoS was the original, I think, in many ways. Sandeep wrote the playbook for using tokens to bootstrap liquidity and interest on a chain. I think it will be symbolic to see that original instance of that playbook being run, joining the AG layer, which is basically supposed to solve this problem. It's supposed to create this unified pool of liquidity and state that can just be plugged into by developers and they won't have to worry about bootstrapping liquidity and doing liquidity, mining incentives, doing things that I think are arguably very, very inefficient from the perspective of the ecosystem. And so I would just call that out as something that I'm personally excited about. And I think we'll be positive for this space.
Speaker B: Yeah. And not even inefficient, but even bad. Yeah. Creating sort of the. What do they call it? The set of mercenaries as opposed to missionaries, and just creating a lot of speculation and not quite pump and dumps necessarily, but, yeah, it's just a lot of activity that's more speculative rather than, like, building something substantial. So. All right, you guys. Well, this has been super fascinating. Where can people learn more about each of you and your work?
Speaker A: So I'm on Twitter befarmer. I don't know if it's appropriate to, like.
Speaker B: Yeah, no, that's why. Yeah. To get your handles.
Speaker A: I'm not a very prolific tweeter, but, yeah, maybe Mark's a better follower than I am.
Speaker C: Not sure about that. But I'm on. I'm on Twitter or xeroxmark B.
Speaker B: Perfect. Well, it's been a pleasure having you both on unchained.
Speaker C: Thanks for having us, Laura. Thank you.
Speaker B: Thanks so much for joining us today. To learn more about Mark, Brendan and Polygon, check out the show notes for this episode. Unchained is produced by me, Laura, without from Nelson Wong, Matt Pilchard, Wanda Vanovitch, Megan Kavis Shashank and market Kuria. Thanks for listening. Unchained is now a part of the Coindesk podcast network. For the latest in digital assets. Check out markets daily five days a week with host Noel Atchison. Follow the Coindesk podcast network for some of the best shows in crypto.
