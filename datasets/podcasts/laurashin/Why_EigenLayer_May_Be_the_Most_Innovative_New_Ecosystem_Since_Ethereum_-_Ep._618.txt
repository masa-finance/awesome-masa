Speaker A: You know, I'm not sure I've seen a dev ecosystem with so many new ideas come up so quickly since Ethereum itself, where with Ethereum itself, you know, this, it unlocked this whole universe of kind of a multi asset blockchain. Smart contracts.
Speaker B: Hi everyone, welcome to Unchained, your no hype resource for all things crypto. I'm your host, Laura Shin, author of the Cryptopians. I started covering crypto eight years ago and as a senior editor at Forbes, was the first mainstream media reporters to cover cryptocurrency full time. This is the March 12, 2024 episode of Unchained. Polkadot is the original and leading layer zero blockchain with over 2000 plus developers. And the Polkadot 2.0 upgrade will be a massive accelerator for the ecosystem, making it faster, more secure and adaptable. Perfect for Gamefi and defi to build, grow and scale. Join the community@polkadot.net work ecosystem community hey unchained listeners. As you know, its hard keeping up with a fast paced world of crypto, so weve got just the thing for you. Subscribe to our free unchained daily newsletter at Unchained crypto dot substack.com dot. Youll get the latest crypto news and original articles from our reporters, as well as summaries of other happenings and bullet points. Plus our meme of the day, all curated and written by our amazing team. Its still your no hype resource for all things crypto, just in newsletter form. Sign up at unchainedcrypto dot substack.com. again, the URL is unchainedcrypto dot substack.com dot. Today's topic is what eigenlayer and restaking mean for Ethereum. Here to discuss are Sriramkhanan, founder of Eigenlayer, and Olaf Carlson, we founder and CIO of Polychain Capital. Welcome. Sri Raman.
Speaker A: Olaf hey, thanks for having me, Lara hi Olaf.
Speaker C: I'm really excited to do this chat predicting.
Speaker B: Olaf yeah, yeah. Olaf and I were chatting a little bit before the show and he was saying he feels like he hasn't seen an equal ecosystem sprout up. Eigen layer is one of the fastest ecosystems he's seen that sprout up since Ethereum, which is in my opinion, quite a high compliment. So sriram, let's start with you, and let's just make sure the audience is all up to speed on what Eigen layer is and what problems it solves for Ethereum.
Speaker C: Yeah, absolutely. Eigen layer is a mechanism that allows any developer to tap into the trust network underlying Ethereum. What is the trust network underlying Ethereum? People lock up a portion of stake ETH into a contract and then promise that they're validating the Ethereum blockchain correctly. And this is the root of trust of any proof of stake chain. And the reason this is trustworthy when people lock up a bunch of stake is that they are potentially liable to lose their stake if they misbehave. So even if all the stakers on Ethereum come together and try to attack the system, they know that they have a concrete pattern in the form of laws of stake, which basically makes the system trustworthy. What Eigen layer does is to let developers who want to access both the stake and the operator network underlying Ethereum to be used for arbitrary new systems. Anybody wants to build a new decentralized protocol which requires full flexibility in configuring and programming what the nodes in the network does, you can do it through Eigen layer. Eigen layer is a system of smart contracts on Ethereum which basically allows people to build new kinds of services. We call avss. Actively validated services. Actively validated service is any service that requires a decentralized group of nodes to actually do active work and validation to verify that they're doing the service is working correctly. So you can think of this as in the cloud world. You have the idea of APIs, application programming interfaces where you can query an API and access the data. Think of how do I build APIs, but with a trust, and with decentralized trust stacking them. That's what an EBS is, that's the ecosystem that we're enabling. Is people coming and building new sets of actively valid services on top of the Ethereum trust network by Eigen layer.
Speaker B: Olaf polychain was a seed investor in Eigen layer. Why are you so excited about it? And what do you think the launch of restaking on Ethereum will mean?
Speaker A: Yeah, so I remember this goes back to the first time I met Shuram. I think we had about 45 minutes calendered off to talk, and we ended up speaking for, I think about 3 hours about the implications of Eigen layer. It instantly made a lot of sense to me. Where these node operators that are running staking nodes in proof of stake systems, they have these fixed costs they've already taken on, right? So if you're an Ethereum staker, you already have high connectivity, high bandwidth, you already have capital at stake, and you're running this node incrementally staking for services built in Eigen layer basically just means clicking a button and saying, yeah, I'll stake in this AV's service, in addition to staking the main ethereum chain. And so the incremental cost for existing Ethereum stakers to add an Eigen layer service is extremely low. So you can think of Eigen layer in a sense as a two sided marketplace between the application developers that need security and the Ethereum stakers, or stakers really in any proof of stake system that validate those services. So it felt like that side of the marketplace stood to benefit. Like it's just quite rational for them to stake and accrue additional value from what they're already earning. Staking Ethereum then when I think about, okay, what could you build if you could have the economic securities of ethereum without having to spin up Ethereum two, right? Which today, in order to get equivalent security of Ethereum, you literally need to launch a crypto tokenization and have it become worth half a trillion dollars and get that whole staking network online just for your one application. And it's actually something that I've helped lots of people do or attempt to do, and it's very hard. The idea that you could plug into a set of tools and sort of just jump onto the existing economic security of Ethereum to add that level of extremely robust security to arbitrary application logic, that side of the marketplace. It also felt like it makes a ton of sense that application developers are going to want to build secure oracles, more flexible oracles, like a whole suite of middleware services that require extremely high security guarantees and unlock new types of application logic without having to spin up their own blockchain. I've since realized it's even further than that in that Eigen layer. Even if you want to spin up your own economic security system with your own native token, Eigen layer also allows you to do that, but initially bootstrap on Ethereum stakers while your token grows in value. So it's pretty remarkable what Eigenlayer unlocks. It's only expanded in my head as I've watched a real ecosystem emerge here around this. As we were talking right before the show, Laura, I said, I'm not sure I've seen a dev ecosystem with so many new ideas come up so quickly since Ethereum itself, where with Ethereum itself, it unlocked this whole universe of a multi asset blockchain, smart contracts. And in those early days, we had things like crypto collateralized stable coins, non crypto collateralized offline collateral stablecoins, early defi contracts, early nfts all those things were emerging around the Ethereum blockchain. I'm seeing very similar, early, extremely promising ecosystem built around Eigen layer today. Since the system has gone live, it's really gotten a ton of traction very quickly. I think that there's over $10 billion now at stake in the Eigen layer system. I tweeted something like a year ago, if eigen layer works, it should absorb most Ethereum in existence. What I really meant more precisely was most Ethereum that's being staked in the staking system. I think we're well on track for that to happen because it's quite logical for both sides of this marketplace to participate.
Speaker B: Yeah, my understanding is only a fraction of both of yours. The more that I've been learning about it, the more I just keep picturing Eigen layer as enabling Ethereum to become this giant stack of decentralized software, as service providers that all share the same security. And so it's just kind of like if you take the web, two Internet that we have now, and there's all these different services that you can sign up for, it would just be like that, but decentralized in on ethereum or something. So why don't we talk a little bit more about these AVss, just because they are so central here. And I was curious to hear your thoughts around, you know, how they should be designed, or even like, which one should be created or who decides? Or is it just, you know, let a thousand flowers bloom or what are your thoughts on that?
Speaker C: I'll give an analogy, I think just playing off the cloud analogy to start with. The first thing is we're calling these actively validated services rather than chains. When people thought about restaking, they were thinking, we're going to allow many people to build chains. But chain is one kind of an object which is its own ordered ledger. But really a service is much more general. I think the right kind of analogy is like the software as a service on top of the cloud. So one way to tell this discussion, if you go back to 1995 and if you're a web application developer, what you need to do is you have to build your own server stack, you have to build your own identity, you have to build your own payments, you have to build your own database, and then build whatever thing that you are building. Imagine your Amazon building bookstore. You have to do all these other things. And in 2024, that's absolutely not how you design a web application. You'd go to aws it, go plug into one of the identity service like Oauth, you'd go plug into a database service like MongoDB, you'd go plug into a payment service like stripe and then build like whatever website you want on top. Even that maybe you do something like Shopify. So you just have like layers and layers of hyper specialized software services that are built which, which is reminiscent of our own team open innovation or permissionless innovation in the crypto space. The idea that anybody can come and build a hyper specialized focused service and then end user phasing applications concatenate a set of these services to provide the functionality they want to their users. So that's the evolution of the web application development stack. And we see something analogous, empowered by something like Agalar is once you have a framework on top of which people can build very specialized services, these ABS's now end user applications can concatenate a bunch of these services and then provide really interesting user experiences. So when people talk about in the modular blockchain paradigm, they say maybe there are like three modules, like execution, consensus, data availability. Our view is no, there are going to be thousands of modules hyper specialized to do like very, very specific things. And when you have hundreds of modules, and another thing people ask in the modular ecosystem, is really an application going to pay a little bit of drip across all these modules? Yeah, exactly. That's how cloud works. Each web application uses 20 SaaS services in the backend and pays a drip of C's. And we have hundreds of unicorns in the SaaS economy, maybe thousands. I'm not doing the math here, but it is one of the most profitable venture investment sectors over the last 20 years. The reason it is very profitable is because it's much easier to find who is the best person to build a NoSQL database for AI than it is to find which end user application is going to win. It's much, much harder to take a bet on that because this is hyper specialized. The best database guys building this service to offer to everybody else. To compare this vision with other visions of application development in the blockchain space. One other vision of application development is the app chain thesis, which is just integrate everything into the app chain. The things that I see happen there is for example, a project like Osmosis, which is a highly technical project which is innovated all across the stack. For example, figured out how to do encrypted mempools as a part of its whole osmosis development. Because they said we don't want me, we want an encrypted map pool. But actually encrypted mempool is an awesome service for lots of applications, not just osmosis. Encrypted mempool should be a service running on Eigen layer. Now I can just plug and play across all these different services to get the end user experience that I want. Applications emerge as a consequence of interlocking across various services. That's really what we're super excited about is actively validated services being these hyper specialized focused services that you can concatenate to form rich end user applications. So that's kind of the broad vision. I can also talk a little bit about what categories of these AVss are we seeing, what are approximate, what are in the long term. So that'd be interesting.
Speaker B: Yeah, why don't you go ahead and do that actually, because I was going to ask you for more examples. I love that example about the encrypted mempool. That's amazing. But yeah, would be curious to hear about the universe of ABS's.
Speaker C: Absolutely. So we can put them into like four or five meta categories, and inside each meta category there's a bunch of categories. One meta category is roll up services. So Ethereum has a roll up scaling roadmap where roll ups are going to be a lot. What are the services that require decentralization that can augment the roll up scaling roadmap? So some people are confused whether Eigen layer is in collision course with the roll up roadmap or not. In fact, it is actually turbocharging, I think, the roll up roadmap. So one of the things we see is roll up services that can be built on. Again, the first service we are building ourselves is eigenvalue. When you are a roll up, you want to write data to Ethereum to make sure that, you know, anybody has access to the inputs and outputs of your computation so that they can challenge it or continue the computation on if you go away. So that's a data availability layer. So Eigen DA is a data availability layer. We built on top of Eigen layer eigenvalue is launching with a throughput of ten megabytes per second. That is compared to Ethereum's upcoming upgrade called 4844 or Dankun is up 30 kb/second so it's a couple of hours of magnitude out there. So that is an example. And then how eigend was built is basically take the Ethereum roadmap dance starting KZG encoding and all that stuff that is already there, then just build it, because there is much more energy and interest in private markets to fund things, fund these kinds of services, than Ethereum itself can necessarily do all the development in house. So that's an example ide. But lots of other services that we see spring up, like another example of a roll up service is bridging. I want to bridge from one. Fragmentation of roll ups is something we all talk about. I want to bridge from one l two to another l two. How should I bridge? If somebody has locked up enough capital and promises that, hey, I have this $10 billion of stake, and I promise that this is the roll up state of this optimism, or optimum or Zksync, then I can use that to lock the state and move around information across these at a much, much lower latency than even Ethereum bridging using crypto economic security. The third one is mev. How do I handle mev on these roll ups? Do I need a decentralized sequencer? Do I need some threshold mempool? What kinds of, like, modulation can I do on mev? And this is very application specific. There's no universal way to do mev management. Sometimes you want first and first out. Sometimes you want auction. Sometimes you want threshold encryption. Sometimes you want even driven actions like limit orders should be automatically executed. All of these are services coming up on Eigen. So this is something we're very excited for. Then. Another category is watchtowers. So you have these execution environments which can have faults, somebody can, but you need a group of nodes which are watching and can trigger these fault alerts. So they need to be incentivized, they need to be staked. That can happen on Eigler. Watchtower is a category coming up fast. Finality. Right now, a roll up sequencer makes a kind of pinky promise that, hey, it's final, it will get finalized on Ethereum, but I promise that I'm going to post this to Ethereum. But instead you could have a committee, which is cake, which makes that promise with an economic credibility rather than just reputational credibility, which is what sequences are doing today. So, examples of categories on just roll up services, and I would say we didn't think of more than just the first one eigenva ourselves. The rest is permissionless innovations just coming out of, like, the market, these kinds of ideas. So this is a meta category, roll up services. Another category that we're seeing is what I call a coprocessor. A core processor is different from a roll up in that it's not a chain. Like, there is nowhere. You go to a block explorer and see the order of transactions off that, that system. A core processor is something that is like a service that is not, you know, I'm on Ethereum. I call a service, you know, let's say I want to run AI, like inference protocol. The inference protocol runs somewhere, and then the answer comes back with a certain amount of trust. And you can get trust from two kinds of things. One is zero knowledge proofs that, yes, this is executed correctly. Or it could come from crypto economic assertion that, hey, I have like $500 million backing this claim. If this claim was wrong, you will be able to, whoever is receiving this claim will be able to slash and redistribute $500 million. That becomes very powerful. Ideal layer helps. The second category of co processors, we call them crypto economic co processors. A co processor that comes with a certain amount of economic integrity attached to them. And this could be, I want to do AI inference, I want to do SQL databases, I want to run arbitrary Linux programs. I want to verify ZK proofs not on chain, but off chain. So these are the categories of things we're seeing in co process. I think this is one of the most interesting and exciting, like, sectors, because for the first time, we call these smart contracts. I think Olaf was alluding to this a little bit before when we started. And if you think about how smart are they, they're rigid contracts, really. When you enter into a smart contract, they're very rigid. They're not that smart. This simple curve that uniswap started with XY equals k, it's a very simple equation. Huge amount of intelligence. It doesn't have access to the history of all the data. It's not using some complex machine learning to adjust prices. But that's how contracts have to evolve, is they have to actually bring smartness. Whether that is databases, that is machine learning, AI protocols, whether that is running general purpose computation like a general Linux box on top of it. All of these are really needed for us to go to systems that are actually smarteen and rigid means if they say something that is actually right. And one way we bring rigidity to the system is that each of these claims are packed with an amount of economic security. So you know that either the claim is right, or you will be able to redistribute a certain amount of money. And that's, that gives you very strong rigidity. So as long as you don't move more economic value than what the stake is, you actually have like unconditional correctness. So that's a really powerful rigid mechanics by which you can now, bring an arbitrary amount of computational intelligence to bear on these smart contracts. So that's a second category I'll stop before going to the other one.
Speaker A: And let me also jump on to add a very specific example that I think is pretty compelling of how this could work. So with this, you can have smart contracts execute based on these secure output of a machine learning model. And when I say secure, I mean like Ethereum level security, right, where you run, you know, a consensus algorithm to decide, you know, the output of this AI model. You know, we all agree that it's, it's correct, and it can be used to trigger financial logic. You can't just connect to private APIs and say, okay, you know, this smart contract is going to manage a billion dollars based on how this private API, you know, tells it to. You really need this sort of economic security, where if you're managing a billion dollars in that smart contract, you need the output of a machine learning system to be validated, and you need it to be secure in the way that Ethereum is secure. So a very specific way you could use this. We all know that the interface layer to, you know, talk to blockchains and smart contracts is not very good. We've all used metamask and fumbled around trying to figure out how to interact with a smart contract. Why did it fail? This gas market, it's all quite complex for your average person. Imagine instead if the end user could interact with their wallet using natural language, like a large language model. So let's say you have a bunch of assets on the optimism roll up, and you want to buy an NFT on the arbitrum roll up. You could say, hey, I want to buy this asset at this price. Get it done. Basically, eigenlayer, the security afforded by eigenlayer, allows a service to interpret that language that the user wants and actually put together a series of smart contract signatures that gets that done. You could imagine the interface layer that we've all been using for years to interact with blockchain systems could, in theory, be reduced to natural language models. So the way you would interact with smart contracts is the way you interact with chat GPT, where you just, in normal language, layman's terms, say, I'm interested in buying this asset or making this trade. And in the background, there might be five or six different signatures that need to happen, or attestations, and we can get very, very high economic security that those things are happening in the correct way that the user intended. So the potential here is, this is just one example, but I think just this example is a really big deal. It's sort of replacing the entire way that users interact with these systems with natural language models. And this, to me, would be a huge breakthrough in usability for your average person who understands what it means to type of, you know, hey, I want to buy this asset and I want to use the bitcoin in my wallet. And the asset might be on Ethereum, and you might have to then bridge your bitcoin to Ethereum, sell it for ether, you know, go it onto a roll up, and then go buy the asset. Right. And the large language model, you know, can interpret your instructions and come up with a series of smart contract signatures needed to execute that and have it all validated by one of these Eigen layer AV's services. So that's just one example that I think would be like a seismic shift in the way people interact with blockchains and genuinely be like an order of magnitude improvement for the whole ecosystem.
Speaker B: Yeah. And what I like about that is that it addresses the issue of, like, a lot of people. If they're not in crypto, they're not even going to know the difference between arbitrum and optimism. They're not going to understand why can't you use your bitcoin to buy that? Nft, that's on optimism. Like, they won't even. So, hopefully in that future, people won't even know where things are located or whatnot. But I did want to ask, because there's a lot of talk about how now with all these different roll ups, there's also a lot of fragmentation. So do you guys see this as a way to address that problem?
Speaker C: Yeah, I think I was mentioning earlier that bridging across these roll ups would be done by some kind of economically secured bridges. And we're seeing many, many of those bridges come up. And what I think Olaf's adding here is those bridges could be triggered from natural language, right? When somebody says, hey, do this for me, and then those instructions are interpreted in a rigid way that you're not just trusting some third party, but actually having consensus on the output relationships and then actually have that happen. So I think in some sense, the way I think about it is these will be different services. And then there is a kind of like a service that triggers other services, a UX service, which could be this NLP models, which will then, like, go and trigger these. Now trigger the bridging, trigger this action. You know, that'd be like, super fun.
Speaker B: To see NLP as in, like, natural language processing or something.
Speaker C: Yeah. And LP is not exactly.
Speaker B: And then you, I think you had like a, another category you were going to talk about.
Speaker C: Yeah, we have three more categories to talk about, like, okay, cryptography, go ahead, category, you want to bring like more advanced cryptography into blockchains. And why? Like, because right now blockchains are either verifiable and transparent, or, you know, but don't allow for privacy. So you want rich privacy expressions in blockchain. So you, and while also preserving verifiability, you need to have more complex cryptography, like secure multi party computation, where you're distributing a secret across many, many nodes, and they all do computation, portions of the computation, so, and arrive at the answer without no one actually having access to the, to the actual private information. You can have trusted execution environments, which are special hardware zones which are built with cryptographic integrity by intel arm and so on. And you can actually require that validators or node operators actually have these networks of trusted execution environments, which augments other kinds of trust. You can have fully homomorphic encryption where you encrypt the data and then all the computation is done inside the encrypted space. So again, like amazing breakthrough technology, these kinds of things, we are seeing many, many projects building on idle air with all of these types of fundamental technologies. And they used to exist in some way or the other in other chains. For example, Oasis was a trusted execution environment chain on cosmos. The power of these services becomes much, much larger when they become composable across all the other kinds of things we are talking about. So people have gone, the evolution of the agular ecosystem has been really interesting to watch, where we are already seeing people build an AV's that will serve other avss, or are only possible because other avss will start interacting with them. And of course, all of these are simultaneously are co emergent. But it's exciting to see that, because now, oh, I can build a service that will help these other services. And now if I put these services together, it's a whole, it is something that will actually ten x the infra landscape of crypto. This is another category, cryptography, I would say another category is proofs, you know, broadly laid out. Normally we think of proofs as zero knowledge proofs or computation proofs, but there are other kinds of proofs you need to assertions that are verified. Right? For example, right now I'm doing this interview from the University of Washington in Seattle, and how do I prove to a blockchain that I'm actually in Seattle? This is called a proof of location. Until now, we talk a lot about blockchains being decentralized. And geography is a core aspect of decentralization, whether it's regulation or other reasons. But how do we measure decentralization from a blockchain? And it's shocking to know there is no locus of decentralized measurement of decentralization today. And the first server is measuring decentralizations coming up on eigenvector called Witness chain, which is a proof of location protocol, which basically what it does is it has a decentralized network already, the large decentralized network underpinning Ethereum. They send network pings back and forth to verify that you're close enough to like other nodes which are also in the same zone, and concordance among the latencies from all the different measurement zones to actually establish. Okay, yes, you know, Sriram is right now in Seattle and this is a really powerful service. So it's an example of a proof. Another example of a proof would be a proof of mission hood. Like I want to prove to you that I have a unique iPhone device. It's not the same as any other device. And these devices come up with this root certification. These can be verified on chain and then presented to other services. This is another example of proof of mesh and hood. A third example would be I have some private data which is locked across many servers. Imagine Olaf's building a decentralized marketplace which wants to compete with Amazon and he wants to offer a discount for anybody who has used at least spent $1,000 a month over the last twelve months on Amazon. That's the customer that he wants to attract. How do I verify to him that that's actually the case that I have made these transactions with Amazon. I can log into the Amazon web server and I have an encrypted link with Amazon using this thing called TL's to do that. But how do I certify this to a decentralized network? There are these really brilliant schemes called tierless northwest coming up where when I opened my encrypted connection to Amazon, some node sits in the middle that I give authority to that verifies the encrypted traffic. And then later I can do a proof against the traffic that actually I had thousand dollar of balance last month on Amazon. Another example of a really cool service coming up on inlet. This is another category, proofs of. Finally a category which is very special and unique to restaking an Eigen layer is all these other services that I mentioned could have been built by anybody staking enough money and having enough node operators. The last category is what we call ethereal inclusion guarantees, which is it's about particularly the Ethereum l one blockchain. I want to modulate when you're validating the Ethereum l one blockchain, you you have to make sure that the transactions you put in are all valid. But there is nothing that decides what transactions you put in and what transactions you don't. And there's no way for you to make assertions or promises about specifically what transactions you do put in. For example, if Ethereum had this ability to, a validator could say, hey, my blocks coming up in three slots. And if you give me an additional fee, I'll promise you that your l two blob or whatever thing will absolutely be included three blocks down the line. I'll make sure of that. And I can make a promise like that about an Ethereum l one block. So this is what we call l one inclusion guarantees, or Ethereum inclusion guarantees. Lots of interesting applications. Imagine I want to build a stablecoin liquidation protocol. I have to make sure that liquidations clear as fast as possible. But if the block proposals are already promised to take liquidations and enforce it, otherwise they'll get slashed. You will have a high rigid guarantee that actually those things will happen. So lots of interesting applications to modulate Ethereum L one itself, that's another kind of meta category. So these are all the types of things we are seeing. It's a kind of explosion of ideas we're super excited about.
Speaker B: Yeah, it's super exciting. There's just, yeah, as we've discussed, a lot of innovation that's happening. So in a moment, we're going to talk about potential risks, but first, a quick word from the sponsors who make this show possible. Polkadot is the original and largest layer zero blockchain with over 2000 plus developers and the anticipated Polkadot 2.0 upgrade will be a massive accelerator for the ecosystem, upgrading the infrastructure with eight times higher transaction throughput and twice as fast block times. Perfectly tailored core time for the needs of every protocol. Trustless bridges internally and into Ethereum cosmos, Nier binance smart chain and revised tokenomics, and the implementation of a token burn to reduce inflation. Perfect for gamefi and Defi to build, grow and scale with one of the most active crypto communities in the space. Polkadot recently announced a partnership with mythical Games, bringing top games like NFT rivals with over 650,000 players and 43 million transactions to pave the way for Gamefi and the Polkadot ecosystem. Get your web three ideas to market fast with economics that work for you. Think big, build bigger with Polkadot. Join the community@polkadot.net. work ecosystem community back to my conversation with Sri Ramanolof. So, you know, this has come up time and again, but one of the potential risks that people commonly call out is possibly rehypothecation more. I guess it really relates to the security and then these cascading risks that can happen due to the restaking. But interestingly, I even see people saying things like this. Crypto analyst Miles Deutscher said, quote, I see restaking as the next version of the Defi Ponzis. So I'd be interested from both of you what your response is to these concerns.
Speaker A: I'm sort of assuming that the underlying criticism there is that basically node operators now are staking in potentially hundreds of different proof of stake services simultaneously. And in theory, there could be slashing risk across all 100 services. Again, in theory, because each service decides its own rules. It's an open marketplace. As a staker, you get to decide what to opt in to likely. I think the end state of this is that staking nodes select the types of risks they're willing to take on for AVss and sort of define those risks very precisely and sort of auto join every AV's that lands within that risk profile. That I think is, is the kind of the end state of this marketplace. You know, I do think there is risk every time, you know, a staking node is staking on a new system. I also though think that there's a lot of work being done on preventing slashing from operator error. So slashing in proof of stake systems is meant to deter bad behavior. And when I say bad, I mean somebody actively trying to hurt the system and sort of attack the consensus protocol and come up with an output that's different than what it should be. Now that interestingly though, the vast majority of slashing, and I would say like 99.9% of cases where somebody gets slashed are in fact the intentionality is not really attacking the system, it's operator error of some kind, right? They go offline or their software is misconfigured or something like that. I do think there's a kind of big push that it's going to be accelerated because of Eigen layer to get software in the hands of node operators that can basically detect if they are going to run the software as intended and can better tell the sort of, quote, intentionality behind those staking nodes. I do think this is going to be a very important part of the growth of the Eigen layer ecosystem is staking nodes feeling confident that they can stake on arbitrary avss that they've never heard of without risk, that there's going to be an attack in that system where all of a sudden they're out of consensus and therefore slashed, even though they had no intention of hurting that mechanism or being out of consensus in that mechanism. I really think it's quite different than just endless rehypothecation. I'm going to stake ether, receive a bunch of tokens that represent that staked ethereum, and then stake that, receive a bunch of tokens that represent that. Then you're at a derivative of a derivative, and then I'm going to stake that. It's not really how this works. You're not sort of re hypothecating a derivative over and over and over and over. You're sort of just re hypothecating that core collateral over and over. It's sort of like I put up my house for a mortgage and take a loan. It's not like I then take that loan to buy a new house with a new mortgage, and I take that loan to buy a new house with a new mortgage, and I'm basically leveraged now. So I think that in that particular quote, you said, laura, it's more about rehypothecation leading to leverage. Right. And that being at systemic risk, I don't view this as that at all. I think that there is risk, like I described, with staking nodes, like staking in 100 different systems. But I think the way to mitigate it, it's just a very different type of risk than a derivative of a derivative of a derivative. Adding leverage to a financial contract.
Speaker B: One question before, and maybe Sri Rami, you can answer this, is just in that scenario, if you do get slashed, it sort of feels like, well, then I don't really know what happens. Let's say you are staked in ten different things, and you get slashed. Then how does it affect the security of the other systems that you're staked in? And then the other thing that I asked, I wanted to know about that is, if it's like every AVSS is setting its own standards, then is the security of the overall system determined by, like, kind of the lowest common denominator? Av's like, yeah, these are questions that came up for me.
Speaker C: Yeah, no, those useful questions. One way to start from where Olaf left off, which is, I think, the type of risk, why is it fundamentally different? Is slashing risk is endogenous you control it as a validator. If you are not bad, you will not get slashed as long as the system holds that property, that if you're not malicious, you will never get slashed. That's a technical problem to solve. Like how do you build a system which is actually the case? This is the same problem for Ethereum itself or any proof of stake system. How do you make sure that you're not getting slash for just running the get node? Like this was a big thing that the whole community was discussing a few months back. This is a very important problem, but it's a technical problem, so fundamentally it's a different nature. And the way to think about it is slashing is an endogenous risk. You control that you will get slashed or not because you're opting into these objective smart contracts on these various avss, which specify what the slashing condition is. So you control whether you will get slashed or nothing, as opposed to if you actually take 100 x leverage instead of opting into 100 services, you're taking exogenous risk, something you don't control, like the market price moves 1% and then you will get liquidated. That's absolutely not the case on Eigen layer. Eigen layer is a validation platform, and it's only meant to slash you if you're actively malicious. And why is there non zero yield in this system? It's due to an information asymmetry. If I'm a validator, I know I'm honest, so I'm not going to get slashed. You don't know that I'm honest, so you're paying for this, covering these asymmetry of this information. But I know I'm honest, so I'm actually willing to back it with a portion of capital. Now, going back to Laura's question, how do Avss know, like how much security or whatever they're getting? I think the right way to think about it. Restaking became a popular word, and like, it's a very, it invokes a certain degen like, defi type feeling to it, and it sounds like rehypothecation. So people like, conflate that. But really, Eigen layer is shared security, and shared security is one way to think about it is imagine there are 100 protocols, and each of which can afford, for the fee that they're paying $100 million of stake. Okay, 100 protocols, each of which can offer $100 million of stake. This is one world where they're all separate. They have separate pools of stake they're maintaining, and then they have these protocols, the second world is all of them pools. They're staked together into a $10 billion pool, which is stake across 100 services. Now, the power restaked across 100 services. I argue that the second world is actually really much better when the reason is to attack any one protocol. Now, it's the same fee across the two worlds, because, you know, the same amount of stake and same amount of fees. But in the second world, to attack any one service, you need $10 billion of capital, and you know that you will lose $5 billion of it. So there is a kind of like hardening of security at scale. Like as you get more and more security bundled together, you just like much stronger. The same reason why nation states have armies, not city states. And even across nation states, you pull together to form alliances. So that's the spirit. But one thing you lose when you're aggregating security like that is you're losing attributability. Because maybe in the first world, one of them is Coinbase, and Coinbase says, hey, these other services maybe have 100 million stake, but I have 3 billion stake because I'm Coinbase and I'm running bits. Now, this was missing in what we call the first simple model of security, which is pool security, where everybody is just pooling together. There's no attribution, so we solve this by doing more complex accounting. On Eigen layer, we have something called attributable security. Attributable security is you have this common pool, everybody's validating everything together. But among this pool, I'm going to give a given service an attribution which says that no matter how many other services triggers slashing simultaneously, you will be able to this, you as an AV's will be able to slash and redistribute this much amount of money. There's an auction which actually sells these rights. So this means in the second world, as Coinbase, you could still take out $3 billion of economic security while attributable economic security, while still getting $10 billion of pooled security. So you simultaneously getting the benefit of attribution while also getting the benefit of pooling. So this solves this problem kind of completely, because now it doesn't matter what's happening to these other services, and it's their problem to deal with. I have the Eigen layer like interface to an AV's is you tell me how much attributable security you want, and I'll give you that. And Eigen layer has to now maintain, do much more work to maintain this what we call proof of solvency. It has to be solvent if all the AVss simultaneously draw their claims. But that is what we maintain. Eigen layer remains solvent even if all of the attributions are simultaneously requested. You're now completely buffered across all these services requesting attribution, SAP. So this is a breakthrough that we made in the last six months on how actually Eigen layer attributions work. The other thing that builds on top of this is now imagine you're building a bridge. You're bringing a bridge. A bridge may require anywhere between, let's say, $10 million vt volume to $1 billion vt volume. You know, it's crypto. There is 100 x variance between a bull market and a bad market and a good week and a pat week. So, you know, say you may have like a variance which is from 10 million to 1 billion. So if you were just on your own, you would have to provision for the most demand that you would experience or miss out on the demand. Like if you only provision security for 100 million, like any week where there is volume more than 100 million, you're up. Like, you don't have security to deal with it. In Eigen layer in the pool security model, you can buy attribution on the go. Like you'd say, man, I'm getting more, more demand, so I'm going to actually go and pull more from the pool security, buy more attribution claims. I can actually now have 1 billion for this week and only 10 million for the next week. So this is, we call this elastic scaling of security, which is exactly what for compute, Amazon did is elastic scaling of compute, which is what the EC two is elastic cloud computing. And this is the elastic scaling of security. You get as much security as you want. And by having a common pool, you can kind of smooth out all the random variations. Because when BTC is going up, Sol is not, and when ETH is going up, something else is not. But having a common pool means you can average out across all these fluctuations. Each one buys the amount of security that they want. That's a really, really interesting dynamic. There are other further more complex dynamics when imagine there is an application like farcaster using, let's say five avss. It uses an oracle, it uses a data availability, it uses like AI co processor and forecaster is doing some kind of weekly volume of $1 billion. If it had these as separate pools of security, it has to go and buy this attributable security of 1 billion separately from each of these services. Because any one of those services, the Oracle, the data availability, the AI, any one of those services screw up, their money could be lost. Now in eigen layer, because there is a lot of shared staking, the same group of nodes are sticking across many services. You only need to buy one x of that attributable security and say that any one of these services fails. As long as you can assure me that any one of those secure services fails, you will pay me 1 billion. Now you're suddenly saving your insurance or attribution fee by five x as you're bundling many, many services. So there's an economy of scale, of attribution, that is like elastic scaling of security, that is the power of pool security. All of these are like foundational features that we've thought about when you're building the first universal crypto economic system of security. So instead of. I think a lot of people have this view that it's some kind of like a Degen platform. It's the exact opposite of being a pure DGen platform. It is the sharpest system of economic security available today.
Speaker B: Well, that's going to be disappointing news to the Degens, who are following your points program and probably eagerly anticipating an airdrop. So I got to ask this, like, will Eigen layer do a token? You guys have a points program. So just curious to hear what your thoughts were when you went to design it. Like, why do you have a points program? Why did you design it the way you did? And what are you thinking when it comes to a token? And either of you can answer, because I know Olaf, you're an investor. I'm sure you guys discuss these things.
Speaker C: I'll take this and let Olaf add something on top. The points represent the amount of eat you have staked and how many hours or how many weeks. So it's basically eat hours is the kind of fundamental unit of. It's just a measure of participation in the Eigen layer system. And unlike many other point systems, which are arbitrary, it's just a very simple, clear measurement of how much staking contribution you've done into the protocol. So that's the eigen layer points. There's nothing mysterious about how these points are created or allotted. It's a simple measurement. And why do we have the points program? As we build this protocol? When services come in and they want to demand stakers and operators, they want people who have committed or committed deeply into the ecosystem. So that's a simple measurement. For example, Avssenhe, if there's over provisioning of security, they might choose only those who were around in the system for long enough. There's a simple measurement for it. And as we aim to decentralize the protocol in the longer run, obviously there is governance rights that is accorded to people who have actually participated in the protocol. And staking is one element of participation in the protocol. So obviously there will be stakeholders in the governance of the protocol as we go on. But we have not thought far enough to actually talk about.
Speaker B: Oh, really interesting, Olaf, to you? Yeah, because, well, actually, let's throw this in there as well. I'm sure, you know, kelp Dao recently began allowing people to take their Eigen layer points and get liquidity out of them. So in a way, that is kind of right now the closest thing that we have to an Eigen layer token. So I was kind of curious for your thoughts on that.
Speaker C: And, yeah, one of the things I like to actively discourage is people taking leverage on top of these lrTs. You know, I said there's no leverage or, you know, in the Eiger layer protocol, but you actually can take leverage outside of the Eigen layer protocol, go and like, you know, deposit your LRT into like a lending pool and then borrow against it, and then like, loop.
Speaker B: This several times, assuming liquid restaking token. For people who are wondering what that stands for. LRT restaking token.
Speaker C: And there are many protocols like Calp and Etherfi and Buffer and Renzo and Rio, which are building these kinds of protocols, which are very, very useful, actually, because they outsource management of how do I choose operators? How do I choose AVss? How do I do risk management, like, you know, have a consistent, decentralized governance protocol. Each of them have their own thing, so you should do your own research before engaging with any of them. They're external to eigen layer, but it is a useful service. But while doing it, people are now, the DGEn version of it is to actually go and take leverage on top of it, which is you take a liquid restaking token, which is potentially represents your eth deposited in the protocol, and then go and borrow against it ETH, and then go and do this again and again. And if e to the liquid restaking token moves from one to one to they'll get liquidated and lose their capital. Lara, you were asking me earlier, what do you think about people doing expecting this to be new defi yield? And is there any Ponzi nature to it? Eigen layer is designed to have zero principal risk. If you actually staked and the operators ran correctly, you don't take any principle risk. But the way our space is, somebody figures out how to take principles, we're actually going and taking leverage outside the protocol and going and doing this thing. This can be very dangerous. This is a very, very risky thing to do because there's all sorts of reasons why a liquid restrict eth will not be worth one eth in the short term, even if it becomes equal to one eth in the long term. There is liquidity crunches. Lets say theres a hack somewhere in their code which they can even have an upgrade console and upgrade but who knows, that takes a week. And within that the thing will depict all kinds of reasons why a liquid restaking token will not be equal to one eth in the short term. So doing anything with this leverage on this is extremely risky. And when im saying eigenvector doesnt have leverage, it's in the aggregate protocol. When you stake across multiple services, it's the only risk. Is the operator risk that Olaf already alluded to. But people are doing this stuff. This is very risky. We advise people not to do it in terms of just a representation for rewards. The points, it is not something we are encouraging, but by itself it doesn't create any leverage or anything crazy in the protocol. Of course these points may be worth nothing. That is the risk that you're taking when you're going and buying a point. But thats a different kind of risk than actually taking these kinds of leverage risk which youre thinking that youre not taking any risk and actually make it lose all your capital. Yeah.
Speaker A: The only other thing ill add is I know that sometimes in our space people get confused about product market fit and sort of economic incentives. Let's say you're building a social platform and it's post to earn, right? Like you might see a lot of people posting and you might think you have product market fit, right? But in reality you've added an economic incentive that is perhaps not sustainable or like short term oriented where you in fact don't have product market fit. And I think it's very important to understand that a lot of these incentive models can increase the rate of growth of something that already has product market fit. This is like Uber saying get your first ride free, right? Uber has product market fit. That first free ride is not the only ride people take on Uber after that, they continue to use Uber because they like the service. I do think sometimes in crypto I see first ride free and then it's confusing to people whether anyone wants the second ride or not. You can subsidize through these incentive systems the growth of things that already have product market fit. However, these subsidies do not replace product market fit. And the one thing I'll say about Eigen layer is it's very, very clear that there's product market fit without any sort of subsidy system. Whereas like in a post to earn social protocol, like, you might see the behavior that's more like, say an axie infinity, right? Where it's like, wait a second, are we subsidizing the growth of players in an awesome video game community, or are people just kind of grabbing that first free ride and the moment their free ride is over, like, they leave the system? Right. So it's very clear to me that Eigen layer is not reliant on any sort of incentive outside of just the usefulness of the service.
Speaker B: So one quick question that I did want to ask is, you kind of hinted, I feel like it was stream rom. Maybe you sort of hinted that potentially the points at some point would translate into being used for governance. Did you say that?
Speaker C: Yeah, it could definitely be used by AVss to choose, like what stakers to opt in for a given service. It could be used as a mesh of participation in governance. We have, we don't have like a concrete roadmap on exactly all the ways that this could be used.
Speaker B: So I don't know if you remember back in the day, people got concerned that Lido's dominance could lead the LDO token to kind of be quasi governance token for Ethereum. So do you think a similar situation could happen for an Eigen layer token?
Speaker C: Yeah. So I think there is a fundamental difference between something like Lido and something like either Lido decides who the operator set is that actually can run Ethereum nodes. So imagine 100% of Ethereum stake went through lido. Then the lido operator set is identical to the Ethereum operator set. Eigen layer has no such subjective judgment. Eigen layer just says, anybody who wants to stake and be an operator can be an operator. This is a very conscious decision that we took that the Eigen layer protocol only makes objective judgments and nothing subjective assignments to operators. So that is a fundamental difference. So even if 100% of each stake went through Eigen layer, you don't have the same problem that Lido has, which is that they are deciding the operator set, whereas Eigen is not designing the operator set. An operator set decision is critical to decentralization, cellistribute resistance, other properties of the Ethereum protocol. And it is one of those things where the Ethereum protocol is not yet complete, and I say it's not yet complete because the protocol needs censorship resistance, but has no way of enforcing it. Inside the protocol. It is reliant on us shouting on Twitter and just canvassing people to actually get these properties. But I would say off that kind of a spectrum, Ethereum is the most sophisticated in the sense that it is actually not reliant on the operator set for safety at all, because if you violate safety, you will get slashed. It is written in protocol. It's as rigid as it gets. So we're not trusting the operator set to maintain safety, which means don't double sign a block or don't sign an invalid block, you don't have to go. And nobody comes on Twitter and requests this because you know if you do it, you're going to lose your money, so nobody's going to do it. But there are some aspects which have not been internalized yet, because this is a new emerging area, and we have a new proposal for this called Rivier, which is a mechanism to penalize for censorship in Ethereum itself. But these are the things that we are actually, we don't like this social herding that is necessary to actually make the Ethereum protocol work. Because is it a protocol or is it some, you know, philosophy that we all have to buy to actually, you know, even though I'm like, very aligned on the philosophy, but we don't build a system assuming the participants are going to subscribe to a philosophy like, that's not a rigid system. A rigid system enforces its roles internally and self consistently, that, I don't need to know this. So the way we think about it is, first, is like, Igelair does not impose any objective, subjective considerations on the operators and, you know, operator side, which means, like, we don't have the same problems that something like lido has, but also recognizing that Ethereum itself can actually internalize its constraints much better. And like, actually proposing. I gave a talk on this protocol called Riviera in the censorship WTF workshop. But, you know, how do we make Ethereum better? So that's our two answers to that question. In fact, one of our stated goals is the internalization of the Eigen layer protocol into Ethereum.
Speaker B: Yes. Okay.
Speaker C: Eventually, Eiger layer should be like a core part of the Ethereum protocol. It doesn't need to be something external. Of course, there's a lot of details that we are figuring out, like all the things I mentioned today about attributable security. How do you account for this correctly? All of these need to be ironed out in the open kind of market, where it's at like an arm's length from Ethereum, where these experiments are on. And eventually, as the results are of solid and rigid. This is one of the reasons why many Eigen layers like, oh, it's not really restaking or anything like I think Olaf mentioned briefly before. The reason is you need a single place which has the full accounting. That's the right architecture. And right now, Eigen layer has full accounting. Eigen layer has minimal information interfaces to Ethereum, because Ethereum is not built with the idea that something like Eigen layer will exist. And now we're trying to get Ethereum to kind of co evolve or maybe even just internalize it, because that can be like a single place of accounting for everything. So that's our long term stated goal, that something like Eigen layer becomes something like enshrined into Ethereum itself.
Speaker B: I'll be super interested to follow that. So with our time remaining, I definitely want to talk about AI, because I noticed polychain has invested in things that are working with Eigen layer. And there's a number of different announcements of Eigen layer working with different AI projects such as alt layer ritual. You guys can kind of talk about any of this that you want. But yeah, we have limited time. But I did also just want to throw in there, you know, Sriram, you have such a background in AI as well, so would be curious to hear kind of on any of the topics that I just mentioned from either of you.
Speaker A: Yeah, yeah, I'll quickly jump in to just say, you know, we have invested in, I think, about a dozen projects building on Eigen layer already. I also anticipate that many of the projects in our portfolio that did not have a stated goal when we made the investment to build on Eigen layer, will realize that Eigen layer is probably the best home for the service or technology that they're building. We are also, I'm looking for more projects building here because, as I said, I really think the ecosystem growing here is a sort of step function improvement on what exists today. It's real, like a real expansion on the types of things you can build, you know, just on Ethereum today. And, you know, when it comes to, more specifically, Laura, your question of this kind of AI, you know, call it AI coprocessor, more specifically category, I talked a little bit about this replacing like a wallet interface with a language model interface, so that it just abstracts away all of this complexity that the end user has. To deal with today. We've talked about this abstraction for years in crypto, like, for over ten years. Everyone knows these wallets are kind of a disaster. We finally have, I think, a possible path, right? And I'm not saying it's easy or it doesn't come with its own gotchas, but a possible path to a sort of natural language model where like, if you call your banker, you can say, I want to buy five shares of this company. And they say, okay, great, it's done, right? It's like a language model thing in the background. There's all sorts of mechanisms that allow that to be possible. But the end user doesn't have to know what a market maker is or what exchange they bought it on or where the custodian is. They don't have to ever interact with any of the low level systems. So I do think that AI for abstraction is a big one. I think another one, though, is this fully homomorphic encryption. So, like, you can imagine encrypting data and then sending it to an AI model to sort of, you know, compute on it and send you back, you know, the output. And interestingly, the AI model never knows what you were trying to figure out. It can sort of do math, for example, on encrypted data without you revealing the data to the model. So there's, you know, there's lots of interesting applications here. Like, let's say you have a huge data set that you think will be predictive of prices, but you obviously don't want to reveal the data set because it's sort of your edge or alpha in the market. You can encrypt the data set, send it to, you know, third party to execute predictive computation on it and send it back to you. You pay them for the service, but also they don't get the data, so they can't, like, take advantage of you sharing it in some way. I think that smart contracts just triggering based on the outputs of AI algorithms is going to be a huge category. And this language wallet, you know, language model wallet, I think, is like the first thing I sort of thought of, but I think people will come up with many, many more things would be built here. And it's sort of like a feeling I had with early Ethereum, which is I couldn't articulate in 2014 when I read the Ethereum whitepaper. Okay, well, it's going to lead to Defi, Daos, NFTs in this order and on these dates. Right? But I did know that it enabled developers to express themselves in ways that weren't possible before. I do think that that intuition or sort of spidey sense is tingling for sure. And we're seeing like, these early application use cases. But I really think a lot will get invented over the next year that I have not thought of.
Speaker C: One of the quotes from Olaf from way back, I think maybe 2017 was like, I think he said something like the world's best investor in like 20 years, maybe an old Tesla, you know, sitting and like, you know, doing trades. Taking a view on that. Actually, one of the things I'm super excited about is sovereign AI, which is sovereign means, like self sovereign. It's an AI that is like, not running on behalf of anybody, but it's running on behalf of itself, becomes possible on blockchains in a way that's simply not possible elsewhere. For these things to exist by themselves and to actually undergo darwinian evolution, they need a few things. They need a fitness function. What's a fitness function on a blockchain is how much wallet balance do you have to be able to use the wallet balance to actually, say, trigger an AI computation or protocol that actually goes, does a trade, does an mev action, does, like, some other thing, and then like, increase the wallet balance? So we're absolutely going to see, I think, sovereign digital AI, this is not robots like, you know, self constructing like progeny, but it's just like a wallet and an agent attached to a wallet that is sovereign by itself. This has to happen. And my prediction is, this is two years up. In two years, we'll see this. And some of this is going to run absolute rampage by, like, going and attacking wallets and like extracting money and doing all kinds of crazy things. So we're going to need to do AI sanctions before we know it. So sovereign digital AI is going to be a category. They're going to just be these agents that sit on blockchains and go. Some of them will provide useful services, others will try to, like, hack to had to make money. And the thing is, we will lose control really quickly because these blockchains have all the properties needed for these AI's to actually to live and like, not because you have sensitive resistance, you have immutability, like all these properties that make it possible for this to arise as its own category. But, you know, more, more fun, more practical, more nice things also will arise. And one of my favorite things is how do we trigger more open innovation in AI? This big fight that's going on at the center of AI is open source versus closed source. There are big problems on both sides. Open source, of course, there's one dimension of safety which I think actually being open means you are more safe in a very fundamental sense. Imagine you go out to battle with this opponent, but you know, the opponent's entire, like, battle plan. Why? Because it's open source. Like, you know, if an open source AI goes rogue, you know everything about it. And like, you know, can you defend against something about which you know everything? Yeah, I think it's much, much easier. So. But the fundamental problems I see are basically, open source has no fundamental incentives. Closed source has incentives. Open source has permissionless innovation packing it. Closed source has no permissionless innovation. Can we create a new model which has both permissionless innovation and value accrual to the creator? Because, you know, my kind of like, highest principle in all of my kind of principles is the system. Have a balanced karma. Like, you do something, you contribute to the Commons, is the Commons giving something proportional back to you, then you will have an incentive to go and contribute more and more to the comments. But if I pollute the comments, am I getting slashed? Am I actually getting the negative feedback back from the comments? That's the two things that are needed to actually make this work. One mechanism we have thought about a lot is a new licensing mechanism which is called PDL, permissionless derivatives license. The idea is unlike any other license, which usually talks about the creator's rights and what rights are afforded to the Commons. The permissionless derivatives license attaches itself to a platform, particularly a blockchain platform, says, hey, I'm creating this new software, but I'm putting it up on a PDL on Eigen layer, or on a system on top of Eigen layer, for example. And what it means is now anybody else can come and create. So the PDL says anybody can come and create derivatives permissionlessly, but they also have to deploy the derivative on the same platform and attribute it. Okay, it's not enforced by, like, you know, in protocol. Eventually, maybe with Fhe and other things, you can actually enforce it in protocol, but initially it can just be enforced by law. That this is what the license is, that anybody can create derivatives and post it on the chain. Now what happens is the original, so the value. Now the blockchain monitors how much usage of these models and value accrued, and it distributes it among both the source and the derivative. And is it static, is it dynamic? Can we do complex value allocation across chains? All of these are interesting problems. We have some ideas for the core thing is this license, the PDL license, the permissionless derivatives license allows, for example, AI creators to come and deploy models and other people to create derivative models on top and have value accrual. Like pass through this sequence of derivations rather than only to one guy or so it's basically ties the two things correctly. You have value accrual and you have permissionless derivations. So it's a new model I'm very excited about, and we are going to popularize this a lot. But this is an idea that could be really powerful for AI, where I think permissionless innovation and value accrual to creators is going to completely outrun closed source. Like, oh, we have this brain trusted open AI. Yeah, it's true. But always the amount of innovation and ideas outside your organization far exceeds the amount of ideas you can capture inside any. So that's, that's one, one direction I'm like super excited about is, you know, can we actually turbocharge permissionless innovation for AI on platforms like Agamemnon?
Speaker B: Wait, and just to understand, like when you talked about that, I saw so many different ways that that could work. Like, it could work like for, yeah, just open source, I guess. Yeah, AI is like developers working on things. But then, you know, you were talking about creators, so I didn't know if you also meant there like literally like artists or whatever. So it could really be the full range of all these different types of.
Speaker C: Creative, specifically talking about like the AI model creators, right? People who create and train these models, they are deploying it, but exactly the same ideas hold for all kinds of other things like media and even inputs into the models. So if you think about a model, a model is like the algorithm plus data and plus compute, right? Compute can be hosted on blockchain and accounted for. And we see many platforms starting to do it. Data should be hosted on blockchains and accounted for and attributed and create a mechanism for them to be owners of the models. And then model algorithms need to come together so we can coordinate all these sites on a common legible substrate, which is of course blockchains. And then you see the system seems to have a lot of power.
Speaker A: Yeah, get ready, you guys, for tokens that allow you to own a part of these sovereign AI's. Instead of owning a share of a company that controls an algorithm, you have much clearer sense of exactly what you own. If you own a token that represents a share, so to speak, of one of these sovereign AI's, and in fact could potentially govern the AI as well. So it's another. I totally agree, Shiram. This is going to be, I think, a very big category over the next couple of years and something that lots of very unusual, I think, outcomes will occur.
Speaker B: Oh, my gosh, you guys, I'm so excited. I feel like we barely scratched the surface. I had to ditch a whole bunch of questions. But you guys, this was so fun. Where can people learn more about each of you and your work?
Speaker A: You know, I think that polychain website, you know, has all the information you could ever want to know about, you know, our organization. So you can just check that out.
Speaker C: From our end, you know, at igenlair on Twitter is the Eigen LA yer is the right place. And also my name sreet on Twitter, Ray Ramkanan. Both of these are good places to follow and track of what we're doing.
Speaker B: Awesome. Well, it's been a pleasure having you both on Unchained.
Speaker C: Thank you so much, Laura. Thank you.
Speaker B: Thanks so much for joining us today. To learn more about Sriram Olaf and restaking in Eigen Lair, check out the show notes for this episode. Unchained is produced by me, Laura Shin, with help from Nelson Wong, Matt Pilchard, Juan Aranovich, Megan Davis Shashank, and market curry. Thanks for listening. Unchained is now a part of the Coindesk podcast network. For the latest in digital assets. Check out markets daily five days a week with host Noel Atchison. Follow the Coindesk podcast network for some of the best shows in crypto.
